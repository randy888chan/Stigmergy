Directory structure:
└── hkuds-deepcode/
    ├── README.md
    ├── __init__.py
    ├── deepcode.py
    ├── LICENSE
    ├── MANIFEST.in
    ├── mcp_agent.config.yaml
    ├── mcp_agent.secrets.yaml
    ├── requirements.txt
    ├── setup.py
    ├── .pre-commit-config.yaml
    ├── cli/
    │   ├── __init__.py
    │   ├── cli_app.py
    │   ├── cli_interface.py
    │   ├── cli_launcher.py
    │   ├── main_cli.py
    │   └── workflows/
    │       ├── __init__.py
    │       └── cli_workflow_adapter.py
    ├── config/
    │   ├── mcp_tool_definitions.py
    │   └── mcp_tool_definitions_index.py
    ├── schema/
    │   └── mcp-agent.config.schema.json
    ├── tools/
    │   ├── __init__.py
    │   ├── bocha_search_server.py
    │   ├── code_reference_indexer.py
    │   ├── command_executor.py
    │   ├── git_command.py
    │   ├── indexer_config.yaml
    │   ├── pdf_converter.py
    │   ├── pdf_downloader.py
    │   └── pdf_utils.py
    ├── ui/
    │   ├── __init__.py
    │   ├── app.py
    │   ├── handlers.py
    │   ├── layout.py
    │   └── streamlit_app.py
    ├── utils/
    │   ├── __init__.py
    │   ├── cli_interface.py
    │   ├── dialogue_logger.py
    │   ├── file_processor.py
    │   ├── llm_utils.py
    │   └── simple_llm_logger.py
    ├── workflows/
    │   ├── __init__.py
    │   ├── code_implementation_workflow.py
    │   ├── code_implementation_workflow_index.py
    │   ├── codebase_index_workflow.py
    │   └── agents/
    │       ├── __init__.py
    │       ├── code_implementation_agent.py
    │       └── document_segmentation_agent.py
    └── .github/
        ├── dependabot.yml
        ├── pull_request_template.md
        ├── ISSUE_TEMPLATE/
        │   ├── bug_report.yml
        │   ├── config.yml
        │   ├── feature_request.yml
        │   └── question.yml
        └── workflows/
            ├── linting.yaml
            └── pypi-publish.yml

================================================
FILE: README.md
================================================
[Binary file]


================================================
FILE: __init__.py
================================================
"""
DeepCode - AI Research Engine

🧬 Next-Generation AI Research Automation Platform
⚡ Transform research papers into working code automatically
"""

__version__ = "1.0.4"
__author__ = "DeepCode Team"
__url__ = "https://github.com/HKUDS/DeepCode"

# Import main components for easy access
from utils import FileProcessor, DialogueLogger

__all__ = [
    "FileProcessor",
    "DialogueLogger",
    "__version__",
    "__author__",
    "__url__",
]



================================================
FILE: deepcode.py
================================================
#!/usr/bin/env python3
"""
DeepCode - AI Research Engine Launcher

🧬 Next-Generation AI Research Automation Platform
⚡ Transform research papers into working code automatically
"""

import os
import sys
import subprocess
from pathlib import Path


def check_dependencies():
    """Check if necessary dependencies are installed"""
    import importlib.util

    print("🔍 Checking dependencies...")

    missing_deps = []
    missing_system_deps = []

    # Check Streamlit availability
    if importlib.util.find_spec("streamlit") is not None:
        print("✅ Streamlit is installed")
    else:
        missing_deps.append("streamlit>=1.28.0")

    # Check PyYAML availability
    if importlib.util.find_spec("yaml") is not None:
        print("✅ PyYAML is installed")
    else:
        missing_deps.append("pyyaml")

    # Check asyncio availability
    if importlib.util.find_spec("asyncio") is not None:
        print("✅ Asyncio is available")
    else:
        missing_deps.append("asyncio")

    # Check PDF conversion dependencies
    if importlib.util.find_spec("reportlab") is not None:
        print("✅ ReportLab is installed (for text-to-PDF conversion)")
    else:
        missing_deps.append("reportlab")
        print("⚠️  ReportLab not found (text files won't convert to PDF)")

    # Check LibreOffice for Office document conversion
    try:
        import subprocess
        import platform

        subprocess_kwargs = {
            "capture_output": True,
            "text": True,
            "timeout": 5,
        }

        if platform.system() == "Windows":
            subprocess_kwargs["creationflags"] = 0x08000000  # Hide console window

        # Try different LibreOffice commands
        libreoffice_found = False
        for cmd in ["libreoffice", "soffice"]:
            try:
                result = subprocess.run([cmd, "--version"], **subprocess_kwargs)
                if result.returncode == 0:
                    print(
                        "✅ LibreOffice is installed (for Office document conversion)"
                    )
                    libreoffice_found = True
                    break
            except (
                subprocess.CalledProcessError,
                FileNotFoundError,
                subprocess.TimeoutExpired,
            ):
                continue

        if not libreoffice_found:
            missing_system_deps.append("LibreOffice")
            print("⚠️  LibreOffice not found (Office documents won't convert to PDF)")

    except Exception:
        missing_system_deps.append("LibreOffice")
        print("⚠️  Could not check LibreOffice installation")

    # Display missing dependencies
    if missing_deps or missing_system_deps:
        print("\n📋 Dependency Status:")

        if missing_deps:
            print("❌ Missing Python dependencies:")
            for dep in missing_deps:
                print(f"   - {dep}")
            print(f"\nInstall with: pip install {' '.join(missing_deps)}")

        if missing_system_deps:
            print("\n⚠️  Missing system dependencies (optional for full functionality):")
            for dep in missing_system_deps:
                print(f"   - {dep}")
            print("\nInstall LibreOffice:")
            print("   - Windows: Download from https://www.libreoffice.org/")
            print("   - macOS: brew install --cask libreoffice")
            print("   - Ubuntu/Debian: sudo apt-get install libreoffice")

        # Only fail if critical Python dependencies are missing
        if missing_deps:
            return False
        else:
            print("\n✅ Core dependencies satisfied (optional dependencies missing)")
    else:
        print("✅ All dependencies satisfied")

    return True


def cleanup_cache():
    """Clean up Python cache files"""
    try:
        print("🧹 Cleaning up cache files...")
        # Clean up __pycache__ directories
        os.system('find . -type d -name "__pycache__" -exec rm -r {} + 2>/dev/null')
        # Clean up .pyc files
        os.system('find . -name "*.pyc" -delete 2>/dev/null')
        print("✅ Cache cleanup completed")
    except Exception as e:
        print(f"⚠️  Cache cleanup failed: {e}")


def print_banner():
    """Display startup banner"""
    banner = """
╔══════════════════════════════════════════════════════════════╗
║                                                              ║
║    🧬 DeepCode - AI Research Engine                          ║
║                                                              ║
║    ⚡ NEURAL • AUTONOMOUS • REVOLUTIONARY ⚡                ║
║                                                              ║
║    Transform research papers into working code               ║
║    Next-generation AI automation platform                   ║
║                                                              ║
╚══════════════════════════════════════════════════════════════╝
"""
    print(banner)


def main():
    """Main function"""
    print_banner()

    # Check dependencies
    if not check_dependencies():
        print("\n🚨 Please install missing dependencies and try again.")
        sys.exit(1)

    # Get current script directory
    current_dir = Path(__file__).parent
    streamlit_app_path = current_dir / "ui" / "streamlit_app.py"

    # Check if streamlit_app.py exists
    if not streamlit_app_path.exists():
        print(f"❌ UI application file not found: {streamlit_app_path}")
        print("Please ensure the ui/streamlit_app.py file exists.")
        sys.exit(1)

    print(f"\n📁 UI App location: {streamlit_app_path}")
    print("🌐 Starting DeepCode web interface...")
    print("🚀 Launching on http://localhost:8501")
    print("=" * 70)
    print("💡 Tip: Keep this terminal open while using the application")
    print("🛑 Press Ctrl+C to stop the server")
    print("=" * 70)

    # Launch Streamlit application
    try:
        cmd = [
            sys.executable,
            "-m",
            "streamlit",
            "run",
            str(streamlit_app_path),
            "--server.port",
            "8501",
            "--server.address",
            "localhost",
            "--browser.gatherUsageStats",
            "false",
            "--theme.base",
            "dark",
            "--theme.primaryColor",
            "#3b82f6",
            "--theme.backgroundColor",
            "#0f1419",
            "--theme.secondaryBackgroundColor",
            "#1e293b",
        ]

        subprocess.run(cmd, check=True)

    except subprocess.CalledProcessError as e:
        print(f"\n❌ Failed to start DeepCode: {e}")
        print("Please check if Streamlit is properly installed.")
        sys.exit(1)
    except KeyboardInterrupt:
        print("\n\n🛑 DeepCode server stopped by user")
        print("Thank you for using DeepCode! 🧬")
    except Exception as e:
        print(f"\n❌ Unexpected error: {e}")
        print("Please check your Python environment and try again.")
        sys.exit(1)
    finally:
        # Clean up cache files
        cleanup_cache()


if __name__ == "__main__":
    main()



================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2025 ✨Data Intelligence Lab@HKU✨

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: MANIFEST.in
================================================
include README.md
include LICENSE
include requirements.txt
include __init__.py
include *.png
include *.yaml
recursive-include config *.yaml
recursive-include prompts *
recursive-include schema *
recursive-include ui *.py
recursive-include cli *.py
recursive-include utils *.py
recursive-include tools *.py
recursive-include workflows *.py
global-exclude *.pyc
global-exclude .git*
global-exclude .history*
global-exclude .ruff_cache*
global-exclude __pycache__*



================================================
FILE: mcp_agent.config.yaml
================================================
$schema: ./schema/mcp-agent.config.schema.json

# Default search server configuration
# Options: "brave" or "bocha-mcp"
default_search_server: "brave"

# Planning mode configuration
# Options: "segmented" or "traditional"
# segmented: Breaks down large tasks to avoid token truncation (recommended)
# traditional: Uses parallel agents but may hit token limits
planning_mode: "traditional"

# Document segmentation configuration
document_segmentation:
  enabled: false  # Whether to use intelligent document segmentation
  size_threshold_chars: 50000  # Document size threshold (in characters) to trigger segmentation
  # If document size > threshold and enabled=true, use segmentation workflow
  # If document size <= threshold or enabled=false, use traditional full-document reading

execution_engine: asyncio
logger:
  transports: [console, file]
  level: info
  progress_display: true
  path_settings:
    path_pattern: "logs/mcp-agent-{unique_id}.jsonl"
    unique_id: "timestamp" # Options: "timestamp" or "session_id"
    timestamp_format: "%Y%m%d_%H%M%S"



mcp:
  servers:
    brave:
      # On windows replace the command and args line to use `node` and the absolute path to the server.
      # Use `npm i -g @modelcontextprotocol/server-brave-search` to install the server globally.
      # Use `npm -g root` to find the global node_modules path.`
      # command: "node"
      # args: ["c:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-brave-search/dist/index.js"]
      command: "npx"
      args: ["-y", "@modelcontextprotocol/server-brave-search"]
      env:
        # You can also place your BRAVE_API_KEY in the fastagent.secrets.yaml file.
        BRAVE_API_KEY: ""
    filesystem:
      # On windows update the command and arguments to use `node` and the absolute path to the server.
      # Use `npm i -g @modelcontextprotocol/server-filesystem` to install the server globally.
      # Use `npm -g root` to find the global node_modules path.`
      # command: "node"
      # args: ["c:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-filesystem/dist/index.js","."]
      command: "npx"
      args: ["-y", "@modelcontextprotocol/server-filesystem"]
    fetch:
      command: "uvx"
      args: ["mcp-server-fetch"]
    github-downloader:
      command: "python"
      args: ["tools/git_command.py"]
      env:
        PYTHONPATH: "."
    file-downloader:
      command: "python"
      args: ["tools/pdf_downloader.py"]
      env:
        PYTHONPATH: "."
    command-executor:
      command: "python"
      args: ["tools/command_executor.py"]
      env:
        PYTHONPATH: "."
    code-implementation:
      command: "python"
      args: ["tools/code_implementation_server.py"]
      env:
        PYTHONPATH: "."
      description: "Paper code reproduction tool server - provides file operations, code execution, search and other functions"
    code-reference-indexer:
      command: "python"
      args: ["tools/code_reference_indexer.py"]
      env:
        PYTHONPATH: "."
      description: "Code reference indexer server - Provides intelligent code reference search from indexed repositories"
    bocha-mcp:
      command: "python3"
      args: ["tools/bocha_search_server.py"]
      env:
        PYTHONPATH: "."
        BOCHA_API_KEY: ""
    document-segmentation:
      command: "python"
      args: ["tools/document_segmentation_server.py"]
      env:
        PYTHONPATH: "."
      description: "Document segmentation server - Provides intelligent document analysis and segmented reading to optimize token usage"

openai:
  # Secrets (API keys, etc.) are stored in an mcp_agent.secrets.yaml file which can be gitignored
  #  default_model: "o3-mini"
  default_model: "anthropic/claude-3.5-sonnet"


anthropic:



================================================
FILE: mcp_agent.secrets.yaml
================================================
openai:
  api_key: ""
  base_url: ""


anthropic:
  api_key: ""



================================================
FILE: requirements.txt
================================================
aiofiles>=0.8.0
aiohttp>=3.8.0
anthropic
asyncio-mqtt
docling
mcp-agent
mcp-server-git
nest_asyncio
pathlib2
PyPDF2>=2.0.0
reportlab>=3.5.0
streamlit



================================================
FILE: setup.py
================================================
import setuptools
from pathlib import Path
import os


# Reading the long description from README.md
def read_long_description():
    try:
        return Path("README.md").read_text(encoding="utf-8")
    except FileNotFoundError:
        return "DeepCode: Open Agentic Coding (Paper2Code & Text2Web & Text2Backend)"


# Retrieving metadata from __init__.py
def retrieve_metadata():
    vars2find = ["__author__", "__version__", "__url__"]
    vars2readme = {}

    # Use definitive path relative to setup.py location
    init_file_path = os.path.join(os.path.dirname(__file__), "__init__.py")

    with open(init_file_path, encoding="utf-8") as f:
        for line in f.readlines():
            for v in vars2find:
                if line.startswith(v):
                    line = (
                        line.replace(" ", "").replace('"', "").replace("'", "").strip()
                    )
                    vars2readme[v] = line.split("=")[1]

    # Checking if all required variables are found
    missing_vars = [v for v in vars2find if v not in vars2readme]
    if missing_vars:
        raise ValueError(
            f"Missing required metadata variables in __init__.py: {missing_vars}"
        )

    return vars2readme


# Reading dependencies from requirements.txt
def read_requirements():
    deps = []
    try:
        with open("./requirements.txt", encoding="utf-8") as f:
            deps = [
                line.strip() for line in f if line.strip() and not line.startswith("#")
            ]
    except FileNotFoundError:
        print(
            "Warning: 'requirements.txt' not found. No dependencies will be installed."
        )
    return deps


metadata = retrieve_metadata()
long_description = read_long_description()
requirements = read_requirements()

setuptools.setup(
    name="deepcode-hku",
    url=metadata["__url__"],
    version=metadata["__version__"],
    author=metadata["__author__"],
    description="AI Research Engine - Transform research papers into working code automatically",
    long_description=long_description,
    long_description_content_type="text/markdown",
    packages=setuptools.find_packages(
        exclude=("tests*", "docs*", ".history*", ".git*", ".ruff_cache*")
    ),
    py_modules=["deepcode"],
    classifiers=[
        "Development Status :: 4 - Beta",
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
        "Intended Audience :: Developers",
        "Intended Audience :: Science/Research",
        "Topic :: Software Development :: Libraries :: Python Modules",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
        "Topic :: Text Processing :: Linguistic",
    ],
    python_requires=">=3.9",
    install_requires=requirements,
    include_package_data=True,
    entry_points={
        "console_scripts": [
            "deepcode=deepcode:main",
        ],
    },
    project_urls={
        "Documentation": metadata.get("__url__", ""),
        "Source": metadata.get("__url__", ""),
        "Tracker": f"{metadata.get('__url__', '')}/issues"
        if metadata.get("__url__")
        else "",
    },
)



================================================
FILE: .pre-commit-config.yaml
================================================
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v5.0.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: requirements-txt-fixer


  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.6.4
    hooks:
      - id: ruff-format
      - id: ruff
        args: [--fix, --ignore=E402]

  - repo: https://github.com/mgedmin/check-manifest
    rev: "0.49"
    hooks:
      - id: check-manifest
        stages: [manual]



================================================
FILE: cli/__init__.py
================================================
"""
CLI Module for DeepCode Agent
DeepCode智能体CLI模块

包含以下组件 / Contains the following components:
- cli_app: CLI应用主程序 / CLI application main program
- cli_interface: CLI界面组件 / CLI interface components
- cli_launcher: CLI启动器 / CLI launcher
"""

__version__ = "1.0.0"
__author__ = "DeepCode Team - Data Intelligence Lab @ HKU"

from .cli_app import main as cli_main
from .cli_interface import CLIInterface
from .cli_launcher import main as launcher_main

__all__ = ["cli_main", "CLIInterface", "launcher_main"]



================================================
FILE: cli/cli_app.py
================================================
#!/usr/bin/env python3
"""
DeepCode - CLI Application Main Program
深度代码 - CLI应用主程序

🧬 Open-Source Code Agent by Data Intelligence Lab @ HKU
⚡ Revolutionizing research reproducibility through collaborative AI
"""

import os
import sys
import asyncio
import time
import json

# 禁止生成.pyc文件
os.environ["PYTHONDONTWRITEBYTECODE"] = "1"

# 添加项目根目录到路径
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

# 导入MCP应用和工作流

from cli.workflows import CLIWorkflowAdapter
from cli.cli_interface import CLIInterface, Colors


class CLIApp:
    """CLI应用主类 - 升级版智能体编排引擎"""

    def __init__(self):
        self.cli = CLIInterface()
        self.workflow_adapter = CLIWorkflowAdapter(cli_interface=self.cli)
        self.app = None  # Will be initialized by workflow adapter
        self.logger = None
        self.context = None
        # Document segmentation configuration
        self.segmentation_config = {"enabled": True, "size_threshold_chars": 50000}

    async def initialize_mcp_app(self):
        """初始化MCP应用 - 使用工作流适配器"""
        # Workflow adapter will handle MCP initialization
        return await self.workflow_adapter.initialize_mcp_app()

    async def cleanup_mcp_app(self):
        """清理MCP应用 - 使用工作流适配器"""
        await self.workflow_adapter.cleanup_mcp_app()

    def update_segmentation_config(self):
        """Update document segmentation configuration in mcp_agent.config.yaml"""
        import yaml
        import os

        config_path = os.path.join(
            os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
            "mcp_agent.config.yaml",
        )

        try:
            # Read current config
            with open(config_path, "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)

            # Update document segmentation settings
            if "document_segmentation" not in config:
                config["document_segmentation"] = {}

            config["document_segmentation"]["enabled"] = self.segmentation_config[
                "enabled"
            ]
            config["document_segmentation"]["size_threshold_chars"] = (
                self.segmentation_config["size_threshold_chars"]
            )

            # Write updated config
            with open(config_path, "w", encoding="utf-8") as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True)

            self.cli.print_status(
                "📄 Document segmentation configuration updated", "success"
            )

        except Exception as e:
            self.cli.print_status(
                f"⚠️ Failed to update segmentation config: {str(e)}", "warning"
            )

    async def process_input(self, input_source: str, input_type: str):
        """处理输入源（URL或文件）- 使用升级版智能体编排引擎"""
        try:
            # Update segmentation configuration before processing
            self.update_segmentation_config()

            self.cli.print_separator()
            self.cli.print_status(
                "🚀 Starting intelligent agent orchestration...", "processing"
            )

            # 显示处理阶段（根据配置决定）
            self.cli.display_processing_stages(0, self.cli.enable_indexing)

            # 使用工作流适配器进行处理
            result = await self.workflow_adapter.process_input_with_orchestration(
                input_source=input_source,
                input_type=input_type,
                enable_indexing=self.cli.enable_indexing,
            )

            if result["status"] == "success":
                # 显示完成状态
                final_stage = 8 if self.cli.enable_indexing else 5
                self.cli.display_processing_stages(
                    final_stage, self.cli.enable_indexing
                )
                self.cli.print_status(
                    "🎉 Agent orchestration completed successfully!", "complete"
                )

                # 显示结果
                self.display_results(
                    result.get("analysis_result", ""),
                    result.get("download_result", ""),
                    result.get("repo_result", ""),
                    result.get("pipeline_mode", "comprehensive"),
                )
            else:
                self.cli.print_status(
                    f"❌ Processing failed: {result.get('error', 'Unknown error')}",
                    "error",
                )

            # 添加到历史记录
            self.cli.add_to_history(input_source, result)

            return result

        except Exception as e:
            error_msg = str(e)
            self.cli.print_error_box("Agent Orchestration Error", error_msg)
            self.cli.print_status(f"Error during orchestration: {error_msg}", "error")

            # 添加错误到历史记录
            error_result = {"status": "error", "error": error_msg}
            self.cli.add_to_history(input_source, error_result)

            return error_result

    def display_results(
        self,
        analysis_result: str,
        download_result: str,
        repo_result: str,
        pipeline_mode: str = "comprehensive",
    ):
        """显示处理结果"""
        self.cli.print_results_header()

        # 显示流水线模式
        if pipeline_mode == "chat":
            mode_display = "💬 Chat Planning Mode"
        elif pipeline_mode == "comprehensive":
            mode_display = "🧠 Comprehensive Mode"
        else:
            mode_display = "⚡ Optimized Mode"
        print(
            f"{Colors.BOLD}{Colors.PURPLE}🤖 PIPELINE MODE: {mode_display}{Colors.ENDC}"
        )
        self.cli.print_separator("─", 79, Colors.PURPLE)

        print(f"{Colors.BOLD}{Colors.OKCYAN}📊 ANALYSIS PHASE RESULTS:{Colors.ENDC}")
        self.cli.print_separator("─", 79, Colors.CYAN)

        # 尝试解析并格式化分析结果
        try:
            if analysis_result.strip().startswith("{"):
                parsed_analysis = json.loads(analysis_result)
                print(json.dumps(parsed_analysis, indent=2, ensure_ascii=False))
            else:
                print(
                    analysis_result[:1000] + "..."
                    if len(analysis_result) > 1000
                    else analysis_result
                )
        except Exception:
            print(
                analysis_result[:1000] + "..."
                if len(analysis_result) > 1000
                else analysis_result
            )

        print(f"\n{Colors.BOLD}{Colors.PURPLE}📥 DOWNLOAD PHASE RESULTS:{Colors.ENDC}")
        self.cli.print_separator("─", 79, Colors.PURPLE)
        print(
            download_result[:1000] + "..."
            if len(download_result) > 1000
            else download_result
        )

        print(
            f"\n{Colors.BOLD}{Colors.GREEN}⚙️  IMPLEMENTATION PHASE RESULTS:{Colors.ENDC}"
        )
        self.cli.print_separator("─", 79, Colors.GREEN)
        print(repo_result[:1000] + "..." if len(repo_result) > 1000 else repo_result)

        # 尝试提取生成的代码目录信息
        if "Code generated in:" in repo_result:
            code_dir = (
                repo_result.split("Code generated in:")[-1].strip().split("\n")[0]
            )
            print(
                f"\n{Colors.BOLD}{Colors.YELLOW}📁 Generated Code Directory: {Colors.ENDC}{code_dir}"
            )

        # 显示处理完成的工作流阶段
        print(
            f"\n{Colors.BOLD}{Colors.OKCYAN}🔄 COMPLETED WORKFLOW STAGES:{Colors.ENDC}"
        )

        if pipeline_mode == "chat":
            stages = [
                "🚀 Engine Initialization",
                "💬 Requirements Analysis",
                "🏗️ Workspace Setup",
                "📝 Implementation Plan Generation",
                "⚙️ Code Implementation",
            ]
        else:
            stages = [
                "📄 Document Processing",
                "🔍 Reference Analysis",
                "📋 Plan Generation",
                "📦 Repository Download",
                "🗂️ Codebase Indexing",
                "⚙️ Code Implementation",
            ]

        for stage in stages:
            print(f"  ✅ {stage}")

        self.cli.print_separator()

    async def run_interactive_session(self):
        """运行交互式会话"""
        # 清屏并显示启动界面
        self.cli.clear_screen()
        self.cli.print_logo()
        self.cli.print_welcome_banner()

        # 初始化MCP应用
        await self.initialize_mcp_app()

        try:
            # 主交互循环
            while self.cli.is_running:
                self.cli.create_menu()
                choice = self.cli.get_user_input()

                if choice in ["q", "quit", "exit"]:
                    self.cli.print_goodbye()
                    break

                elif choice in ["u", "url"]:
                    url = self.cli.get_url_input()
                    if url:
                        await self.process_input(url, "url")

                elif choice in ["f", "file"]:
                    file_path = self.cli.upload_file_gui()
                    if file_path:
                        await self.process_input(f"file://{file_path}", "file")

                elif choice in ["t", "chat", "text"]:
                    chat_input = self.cli.get_chat_input()
                    if chat_input:
                        await self.process_input(chat_input, "chat")

                elif choice in ["h", "history"]:
                    self.cli.show_history()

                elif choice in ["c", "config", "configure"]:
                    # Sync current segmentation config from CLI interface
                    self.segmentation_config["enabled"] = self.cli.segmentation_enabled
                    self.segmentation_config["size_threshold_chars"] = (
                        self.cli.segmentation_threshold
                    )

                    self.cli.show_configuration_menu()

                    # Sync back from CLI interface after configuration changes
                    self.segmentation_config["enabled"] = self.cli.segmentation_enabled
                    self.segmentation_config["size_threshold_chars"] = (
                        self.cli.segmentation_threshold
                    )

                else:
                    self.cli.print_status(
                        "Invalid choice. Please select U, F, T, C, H, or Q.", "warning"
                    )

                # 询问是否继续
                if self.cli.is_running and choice in ["u", "f", "t", "chat", "text"]:
                    if not self.cli.ask_continue():
                        self.cli.is_running = False
                        self.cli.print_status("Session ended by user", "info")

        except KeyboardInterrupt:
            print(f"\n{Colors.WARNING}⚠️  Process interrupted by user{Colors.ENDC}")
        except Exception as e:
            print(f"\n{Colors.FAIL}❌ Unexpected error: {str(e)}{Colors.ENDC}")
        finally:
            # 清理资源
            await self.cleanup_mcp_app()


async def main():
    """主函数"""
    start_time = time.time()

    try:
        # 创建并运行CLI应用
        app = CLIApp()
        await app.run_interactive_session()

    except KeyboardInterrupt:
        print(f"\n{Colors.WARNING}⚠️  Application interrupted by user{Colors.ENDC}")
    except Exception as e:
        print(f"\n{Colors.FAIL}❌ Application error: {str(e)}{Colors.ENDC}")
    finally:
        end_time = time.time()
        print(
            f"\n{Colors.BOLD}{Colors.CYAN}⏱️  Total runtime: {end_time - start_time:.2f} seconds{Colors.ENDC}"
        )

        # 清理缓存文件
        print(f"{Colors.YELLOW}🧹 Cleaning up cache files...{Colors.ENDC}")
        if os.name == "nt":  # Windows
            os.system(
                "powershell -Command \"Get-ChildItem -Path . -Filter '__pycache__' -Recurse -Directory | Remove-Item -Recurse -Force\" 2>nul"
            )
        else:  # Unix/Linux/macOS
            os.system('find . -type d -name "__pycache__" -exec rm -r {} + 2>/dev/null')

        print(
            f"{Colors.OKGREEN}✨ Goodbye! Thanks for using DeepCode CLI! ✨{Colors.ENDC}"
        )


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: cli/cli_interface.py
================================================
#!/usr/bin/env python3
"""
Enhanced CLI Interface Module for DeepCode
增强版CLI界面模块 - 专为DeepCode设计
"""

import os
import time
import platform
from typing import Optional


class Colors:
    """ANSI color codes for terminal styling"""

    HEADER = "\033[95m"
    OKBLUE = "\033[94m"
    OKCYAN = "\033[96m"
    OKGREEN = "\033[92m"
    WARNING = "\033[93m"
    FAIL = "\033[91m"
    ENDC = "\033[0m"
    BOLD = "\033[1m"
    UNDERLINE = "\033[4m"

    # Gradient colors
    PURPLE = "\033[35m"
    MAGENTA = "\033[95m"
    BLUE = "\033[34m"
    CYAN = "\033[36m"
    GREEN = "\033[32m"
    YELLOW = "\033[33m"


class CLIInterface:
    """Enhanced CLI interface with modern styling for DeepCode"""

    def __init__(self):
        self.uploaded_file = None
        self.is_running = True
        self.processing_history = []
        self.enable_indexing = True  # Default configuration
        self.segmentation_enabled = True  # Default to smart segmentation
        self.segmentation_threshold = 50000  # Default threshold

        # Check tkinter availability for file dialogs
        self.tkinter_available = True
        try:
            import tkinter as tk

            # Test if tkinter can create a window
            test_root = tk.Tk()
            test_root.withdraw()
            test_root.destroy()
        except Exception:
            self.tkinter_available = False

    def clear_screen(self):
        """Clear terminal screen"""
        os.system("cls" if os.name == "nt" else "clear")

    def print_logo(self):
        """Print enhanced ASCII logo for DeepCode CLI"""
        logo = f"""
{Colors.CYAN}╔═══════════════════════════════════════════════════════════════════════════════╗
║                                                                               ║
║  {Colors.BOLD}{Colors.MAGENTA}██████╗ ███████╗███████╗██████╗  ██████╗ ██████╗ ██████╗ ███████╗{Colors.CYAN}               ║
║  {Colors.BOLD}{Colors.PURPLE}██╔══██╗██╔════╝██╔════╝██╔══██╗██╔════╝██╔═══██╗██╔══██╗██╔════╝{Colors.CYAN}               ║
║  {Colors.BOLD}{Colors.BLUE}██║  ██║█████╗  █████╗  ██████╔╝██║     ██║   ██║██║  ██║█████╗  {Colors.CYAN}               ║
║  {Colors.BOLD}{Colors.OKBLUE}██║  ██║██╔══╝  ██╔══╝  ██╔═══╝ ██║     ██║   ██║██║  ██║██╔══╝  {Colors.CYAN}               ║
║  {Colors.BOLD}{Colors.OKCYAN}██████╔╝███████╗███████╗██║     ╚██████╗╚██████╔╝██████╔╝███████╗{Colors.CYAN}               ║
║  {Colors.BOLD}{Colors.GREEN}╚═════╝ ╚══════╝╚══════╝╚═╝      ╚═════╝ ╚═════╝ ╚═════╝ ╚══════╝{Colors.CYAN}               ║
║                                                                               ║
║  {Colors.BOLD}{Colors.GREEN}🧬 OPEN-SOURCE CODE AGENT • DATA INTELLIGENCE LAB @ HKU 🚀           {Colors.CYAN}║
║  {Colors.BOLD}{Colors.GREEN}⚡ REVOLUTIONIZING RESEARCH REPRODUCIBILITY ⚡                      {Colors.CYAN}║
║                                                                               ║
╚═══════════════════════════════════════════════════════════════════════════════╝{Colors.ENDC}
"""
        print(logo)

    def print_welcome_banner(self):
        """Print enhanced welcome banner"""
        banner = f"""
{Colors.BOLD}{Colors.CYAN}╔═══════════════════════════════════════════════════════════════════════════════╗
║                             WELCOME TO DEEPCODE CLI                          ║
╠═══════════════════════════════════════════════════════════════════════════════╣
║  {Colors.YELLOW}Open-Source Code Agent | Data Intelligence Lab @ HKU | MIT License        {Colors.CYAN}║
║  {Colors.GREEN}Status: Ready | Engine: Multi-Agent Architecture Initialized               {Colors.CYAN}║
║  {Colors.PURPLE}Mission: Revolutionizing Research Reproducibility                         {Colors.CYAN}║
║                                                                               ║
║  {Colors.BOLD}{Colors.OKCYAN}💎 CORE CAPABILITIES:{Colors.ENDC}                                                      {Colors.CYAN}║
║    {Colors.BOLD}{Colors.OKCYAN}▶ Automated Paper-to-Code Reproduction                                {Colors.CYAN}║
║    {Colors.BOLD}{Colors.OKCYAN}▶ Collaborative Multi-Agent Architecture                             {Colors.CYAN}║
║    {Colors.BOLD}{Colors.OKCYAN}▶ Intelligent Code Implementation & Validation                       {Colors.CYAN}║
║    {Colors.BOLD}{Colors.OKCYAN}▶ Future Vision: One Sentence → Complete Codebase                   {Colors.CYAN}║
╚═══════════════════════════════════════════════════════════════════════════════╝{Colors.ENDC}
"""
        print(banner)

    def print_separator(self, char="═", length=79, color=Colors.CYAN):
        """Print a styled separator line"""
        print(f"{color}{char * length}{Colors.ENDC}")

    def print_status(self, message: str, status_type: str = "info"):
        """Print status message with appropriate styling"""
        status_styles = {
            "success": f"{Colors.OKGREEN}✅",
            "error": f"{Colors.FAIL}❌",
            "warning": f"{Colors.WARNING}⚠️ ",
            "info": f"{Colors.OKBLUE}ℹ️ ",
            "processing": f"{Colors.YELLOW}⏳",
            "upload": f"{Colors.PURPLE}📁",
            "download": f"{Colors.CYAN}📥",
            "analysis": f"{Colors.MAGENTA}🔍",
            "implementation": f"{Colors.GREEN}⚙️ ",
            "complete": f"{Colors.OKGREEN}🎉",
        }

        icon = status_styles.get(status_type, status_styles["info"])
        timestamp = time.strftime("%H:%M:%S")
        print(
            f"[{Colors.BOLD}{timestamp}{Colors.ENDC}] {icon} {Colors.BOLD}{message}{Colors.ENDC}"
        )

    def create_menu(self):
        """Create enhanced interactive menu"""
        # Display current configuration
        pipeline_mode = "🧠 COMPREHENSIVE" if self.enable_indexing else "⚡ OPTIMIZED"
        index_status = "✅ Enabled" if self.enable_indexing else "🔶 Disabled"
        segmentation_mode = (
            "📄 SMART" if self.segmentation_enabled else "📋 TRADITIONAL"
        )

        menu = f"""
{Colors.BOLD}{Colors.CYAN}╔═══════════════════════════════════════════════════════════════════════════════╗
║                                MAIN MENU                                      ║
╠═══════════════════════════════════════════════════════════════════════════════╣
║  {Colors.OKGREEN}🌐 [U] Process URL       {Colors.CYAN}│  {Colors.PURPLE}📁 [F] Upload File    {Colors.CYAN}│  {Colors.MAGENTA}💬 [T] Chat Input{Colors.CYAN}    ║
║  {Colors.OKCYAN}⚙️  [C] Configure        {Colors.CYAN}│  {Colors.YELLOW}📊 [H] History        {Colors.CYAN}│  {Colors.FAIL}❌ [Q] Quit{Colors.CYAN}         ║
║                                                                               ║
║  {Colors.BOLD}🤖 Current Pipeline Mode: {pipeline_mode}{Colors.CYAN}                          ║
║  {Colors.BOLD}🗂️  Codebase Indexing: {index_status}{Colors.CYAN}                                    ║
║  {Colors.BOLD}📄 Document Processing: {segmentation_mode}{Colors.CYAN}                               ║
║                                                                               ║
║  {Colors.YELLOW}📝 URL Processing:{Colors.CYAN}                                                         ║
║  {Colors.YELLOW}   ▶ Enter research paper URL (arXiv, IEEE, ACM, etc.)                    {Colors.CYAN}║
║  {Colors.YELLOW}   ▶ Supports direct PDF links and academic paper pages                   {Colors.CYAN}║
║                                                                               ║
║  {Colors.PURPLE}📁 File Processing:{Colors.CYAN}                                                        ║
║  {Colors.PURPLE}   ▶ Upload PDF, DOCX, PPTX, HTML, or TXT files                          {Colors.CYAN}║
║  {Colors.PURPLE}   ▶ Intelligent file format detection and processing                     {Colors.CYAN}║
║                                                                               ║
║  {Colors.MAGENTA}💬 Chat Input:{Colors.CYAN}                                                           ║
║  {Colors.MAGENTA}   ▶ Describe your coding requirements in natural language                {Colors.CYAN}║
║  {Colors.MAGENTA}   ▶ AI generates implementation plan and code automatically             {Colors.CYAN}║
║                                                                               ║
║  {Colors.OKCYAN}🔄 Processing Pipeline:{Colors.CYAN}                                                    ║
║  {Colors.OKCYAN}   ▶ Intelligent agent orchestration → Code synthesis                     {Colors.CYAN}║
║  {Colors.OKCYAN}   ▶ Multi-agent coordination with progress tracking                     {Colors.CYAN}║
╚═══════════════════════════════════════════════════════════════════════════════╝{Colors.ENDC}
"""
        print(menu)

    def get_user_input(self):
        """Get user input with styled prompt"""
        print(f"\n{Colors.BOLD}{Colors.OKCYAN}➤ Your choice: {Colors.ENDC}", end="")
        return input().strip().lower()

    def upload_file_gui(self) -> Optional[str]:
        """Enhanced file upload interface with better error handling"""
        if not self.tkinter_available:
            self.print_status(
                "GUI file dialog not available - using manual input", "warning"
            )
            return self._get_manual_file_path()

        def select_file():
            try:
                import tkinter as tk
                from tkinter import filedialog

                root = tk.Tk()
                root.withdraw()
                root.attributes("-topmost", True)

                file_types = [
                    ("Research Papers", "*.pdf;*.docx;*.doc"),
                    ("PDF Files", "*.pdf"),
                    ("Word Documents", "*.docx;*.doc"),
                    ("PowerPoint Files", "*.pptx;*.ppt"),
                    ("HTML Files", "*.html;*.htm"),
                    ("Text Files", "*.txt;*.md"),
                    ("All Files", "*.*"),
                ]

                if platform.system() == "Darwin":
                    file_types = [
                        ("Research Papers", ".pdf .docx .doc"),
                        ("PDF Files", ".pdf"),
                        ("Word Documents", ".docx .doc"),
                        ("PowerPoint Files", ".pptx .ppt"),
                        ("HTML Files", ".html .htm"),
                        ("Text Files", ".txt .md"),
                        ("All Files", ".*"),
                    ]

                file_path = filedialog.askopenfilename(
                    title="Select Research File - DeepCode CLI",
                    filetypes=file_types,
                    initialdir=os.getcwd(),
                )

                root.destroy()
                return file_path

            except Exception as e:
                self.print_status(f"File dialog error: {str(e)}", "error")
                return self._get_manual_file_path()

        self.print_status("Opening file browser dialog...", "upload")
        file_path = select_file()

        if file_path:
            self.print_status(
                f"File selected: {os.path.basename(file_path)}", "success"
            )
            return file_path
        else:
            self.print_status("No file selected", "warning")
            return None

    def _get_manual_file_path(self) -> Optional[str]:
        """Get file path through manual input with validation"""
        self.print_separator("─", 79, Colors.YELLOW)
        print(f"{Colors.BOLD}{Colors.YELLOW}📁 Manual File Path Input{Colors.ENDC}")
        print(
            f"{Colors.CYAN}Please enter the full path to your research paper file:{Colors.ENDC}"
        )
        print(
            f"{Colors.CYAN}Supported formats: PDF, DOCX, PPTX, HTML, TXT, MD{Colors.ENDC}"
        )
        self.print_separator("─", 79, Colors.YELLOW)

        while True:
            print(f"\n{Colors.BOLD}{Colors.OKCYAN}📂 File path: {Colors.ENDC}", end="")
            file_path = input().strip()

            if not file_path:
                self.print_status(
                    "Empty path entered. Please try again or press Ctrl+C to cancel.",
                    "warning",
                )
                continue

            file_path = os.path.expanduser(file_path)
            file_path = os.path.abspath(file_path)

            if not os.path.exists(file_path):
                self.print_status(f"File not found: {file_path}", "error")
                retry = (
                    input(f"{Colors.YELLOW}Try again? (y/n): {Colors.ENDC}")
                    .strip()
                    .lower()
                )
                if retry != "y":
                    return None
                continue

            if not os.path.isfile(file_path):
                self.print_status(f"Path is not a file: {file_path}", "error")
                continue

            supported_extensions = {
                ".pdf",
                ".docx",
                ".doc",
                ".pptx",
                ".ppt",
                ".html",
                ".htm",
                ".txt",
                ".md",
            }
            file_ext = os.path.splitext(file_path)[1].lower()

            if file_ext not in supported_extensions:
                self.print_status(f"Unsupported file format: {file_ext}", "warning")
                proceed = (
                    input(f"{Colors.YELLOW}Process anyway? (y/n): {Colors.ENDC}")
                    .strip()
                    .lower()
                )
                if proceed != "y":
                    continue

            self.print_status(
                f"File validated: {os.path.basename(file_path)}", "success"
            )
            return file_path

    def get_url_input(self) -> str:
        """Enhanced URL input with validation"""
        self.print_separator("─", 79, Colors.GREEN)
        print(f"{Colors.BOLD}{Colors.GREEN}🌐 URL Input Interface{Colors.ENDC}")
        print(
            f"{Colors.CYAN}Enter a research paper URL from supported platforms:{Colors.ENDC}"
        )
        print(
            f"{Colors.CYAN}• arXiv (arxiv.org)        • IEEE Xplore (ieeexplore.ieee.org){Colors.ENDC}"
        )
        print(
            f"{Colors.CYAN}• ACM Digital Library      • SpringerLink • Nature • Science{Colors.ENDC}"
        )
        print(
            f"{Colors.CYAN}• Direct PDF links         • Academic publisher websites{Colors.ENDC}"
        )
        self.print_separator("─", 79, Colors.GREEN)

        while True:
            print(f"\n{Colors.BOLD}{Colors.OKCYAN}🔗 URL: {Colors.ENDC}", end="")
            url = input().strip()

            if not url:
                self.print_status(
                    "Empty URL entered. Please try again or press Ctrl+C to cancel.",
                    "warning",
                )
                continue

            if not url.startswith(("http://", "https://")):
                self.print_status("URL must start with http:// or https://", "error")
                retry = (
                    input(f"{Colors.YELLOW}Try again? (y/n): {Colors.ENDC}")
                    .strip()
                    .lower()
                )
                if retry != "y":
                    return ""
                continue

            academic_domains = [
                "arxiv.org",
                "ieeexplore.ieee.org",
                "dl.acm.org",
                "link.springer.com",
                "nature.com",
                "science.org",
                "scholar.google.com",
                "researchgate.net",
                "semanticscholar.org",
            ]

            is_academic = any(domain in url.lower() for domain in academic_domains)
            if not is_academic and not url.lower().endswith(".pdf"):
                self.print_status(
                    "URL doesn't appear to be from a known academic platform", "warning"
                )
                proceed = (
                    input(f"{Colors.YELLOW}Process anyway? (y/n): {Colors.ENDC}")
                    .strip()
                    .lower()
                )
                if proceed != "y":
                    continue

            self.print_status(f"URL validated: {url}", "success")
            return url

    def get_chat_input(self) -> str:
        """Enhanced chat input interface for coding requirements"""
        self.print_separator("─", 79, Colors.PURPLE)
        print(f"{Colors.BOLD}{Colors.PURPLE}💬 Chat Input Interface{Colors.ENDC}")
        print(
            f"{Colors.CYAN}Describe your coding requirements in natural language.{Colors.ENDC}"
        )
        print(
            f"{Colors.CYAN}Our AI will analyze your needs and generate a comprehensive implementation plan.{Colors.ENDC}"
        )
        self.print_separator("─", 79, Colors.PURPLE)

        # Display examples to help users
        print(f"\n{Colors.BOLD}{Colors.YELLOW}💡 Examples:{Colors.ENDC}")
        print(f"{Colors.CYAN}Academic Research:{Colors.ENDC}")
        print(
            "  • 'I need to implement a reinforcement learning algorithm for robotic control'"
        )
        print(
            "  • 'Create a neural network for image classification with attention mechanisms'"
        )
        print(f"{Colors.CYAN}Engineering Projects:{Colors.ENDC}")
        print(
            "  • 'Develop a web application for project management with user authentication'"
        )
        print("  • 'Create a data visualization dashboard for sales analytics'")
        print(f"{Colors.CYAN}Mixed Projects:{Colors.ENDC}")
        print(
            "  • 'Implement a machine learning model with a web interface for real-time predictions'"
        )

        self.print_separator("─", 79, Colors.PURPLE)

        print(
            f"\n{Colors.BOLD}{Colors.OKCYAN}✏️  Enter your coding requirements below:{Colors.ENDC}"
        )
        print(
            f"{Colors.YELLOW}(Type your description, press Enter twice when finished, or Ctrl+C to cancel){Colors.ENDC}"
        )

        lines = []
        empty_line_count = 0

        while True:
            try:
                if len(lines) == 0:
                    print(f"{Colors.BOLD}> {Colors.ENDC}", end="")
                else:
                    print(f"{Colors.BOLD}  {Colors.ENDC}", end="")

                line = input()

                if line.strip() == "":
                    empty_line_count += 1
                    if empty_line_count >= 2:
                        # Two consecutive empty lines means user finished input
                        break
                    lines.append("")  # Keep empty line for formatting
                else:
                    empty_line_count = 0
                    lines.append(line)

            except KeyboardInterrupt:
                print(f"\n{Colors.WARNING}Input cancelled by user{Colors.ENDC}")
                return ""

        # Join all lines and clean up
        user_input = "\n".join(lines).strip()

        if not user_input:
            self.print_status("No input provided", "warning")
            return ""

        if len(user_input) < 20:
            self.print_status(
                "Input too short. Please provide more detailed requirements (at least 20 characters)",
                "warning",
            )
            retry = (
                input(f"{Colors.YELLOW}Try again? (y/n): {Colors.ENDC}").strip().lower()
            )
            if retry == "y":
                return self.get_chat_input()  # Recursive call for retry
            return ""

        # Display input summary
        word_count = len(user_input.split())
        char_count = len(user_input)

        print(f"\n{Colors.BOLD}{Colors.GREEN}📋 Input Summary:{Colors.ENDC}")
        print(f"  • {Colors.CYAN}Word count: {word_count}{Colors.ENDC}")
        print(f"  • {Colors.CYAN}Character count: {char_count}{Colors.ENDC}")

        # Show preview
        preview = user_input[:200] + "..." if len(user_input) > 200 else user_input
        print(f"\n{Colors.BOLD}{Colors.CYAN}📄 Preview:{Colors.ENDC}")
        print(f"{Colors.YELLOW}{preview}{Colors.ENDC}")

        # Confirm with user
        confirm = (
            input(
                f"\n{Colors.BOLD}{Colors.OKCYAN}Proceed with this input? (y/n): {Colors.ENDC}"
            )
            .strip()
            .lower()
        )
        if confirm != "y":
            retry = (
                input(f"{Colors.YELLOW}Edit input? (y/n): {Colors.ENDC}")
                .strip()
                .lower()
            )
            if retry == "y":
                return self.get_chat_input()  # Recursive call for retry
            return ""

        self.print_status(
            f"Chat input captured: {word_count} words, {char_count} characters",
            "success",
        )
        return user_input

    def show_progress_bar(self, message: str, duration: float = 2.0):
        """Show animated progress bar"""
        print(f"\n{Colors.BOLD}{Colors.CYAN}{message}{Colors.ENDC}")

        bar_length = 50
        for i in range(bar_length + 1):
            percent = (i / bar_length) * 100
            filled = "█" * i
            empty = "░" * (bar_length - i)

            print(
                f"\r{Colors.OKGREEN}[{filled}{empty}] {percent:3.0f}%{Colors.ENDC}",
                end="",
                flush=True,
            )
            time.sleep(duration / bar_length)

        print(f"\n{Colors.OKGREEN}✓ {message} completed{Colors.ENDC}")

    def show_spinner(self, message: str, duration: float = 1.0):
        """Show spinner animation"""
        spinner_chars = "⠋⠙⠹⠸⠼⠴⠦⠧⠇⠏"
        end_time = time.time() + duration

        print(
            f"{Colors.BOLD}{Colors.CYAN}{message}... {Colors.ENDC}", end="", flush=True
        )

        i = 0
        while time.time() < end_time:
            print(
                f"\r{Colors.BOLD}{Colors.CYAN}{message}... {Colors.YELLOW}{spinner_chars[i % len(spinner_chars)]}{Colors.ENDC}",
                end="",
                flush=True,
            )
            time.sleep(0.1)
            i += 1

        print(
            f"\r{Colors.BOLD}{Colors.CYAN}{message}... {Colors.OKGREEN}✓{Colors.ENDC}"
        )

    def display_processing_stages(
        self,
        current_stage: int = 0,
        enable_indexing: bool = True,
        chat_mode: bool = False,
    ):
        """Display processing pipeline stages with current progress"""
        if chat_mode:
            # Chat mode - simplified workflow for user requirements
            stages = [
                ("🚀", "Initialize", "Setting up chat engine"),
                ("💬", "Planning", "Analyzing requirements"),
                ("🏗️", "Setup", "Creating workspace"),
                ("📝", "Save Plan", "Saving implementation plan"),
                ("⚙️", "Implement", "Generating code"),
            ]
            pipeline_mode = "CHAT PLANNING"
        elif enable_indexing:
            # Full pipeline with all stages
            stages = [
                ("🚀", "Initialize", "Setting up AI engine"),
                ("📊", "Analyze", "Analyzing research content"),
                ("📥", "Download", "Processing document"),
                ("📋", "Plan", "Generating code architecture"),
                ("🔍", "References", "Analyzing references"),
                ("📦", "Repos", "Downloading repositories"),
                ("🗂️", "Index", "Building code index"),
                ("⚙️", "Implement", "Implementing code"),
            ]
            pipeline_mode = "COMPREHENSIVE"
        else:
            # Fast mode - skip indexing related stages
            stages = [
                ("🚀", "Initialize", "Setting up AI engine"),
                ("📊", "Analyze", "Analyzing research content"),
                ("📥", "Download", "Processing document"),
                ("📋", "Plan", "Generating code architecture"),
                ("⚙️", "Implement", "Implementing code"),
            ]
            pipeline_mode = "OPTIMIZED"

        print(
            f"\n{Colors.BOLD}{Colors.CYAN}📋 {pipeline_mode} PIPELINE STATUS{Colors.ENDC}"
        )
        self.print_separator("─", 79, Colors.CYAN)

        for i, (icon, name, desc) in enumerate(stages):
            if i < current_stage:
                status = f"{Colors.OKGREEN}✓ COMPLETED{Colors.ENDC}"
            elif i == current_stage:
                status = f"{Colors.YELLOW}⏳ IN PROGRESS{Colors.ENDC}"
            else:
                status = f"{Colors.CYAN}⏸️  PENDING{Colors.ENDC}"

            print(
                f"{icon} {Colors.BOLD}{name:<12}{Colors.ENDC} │ {desc:<25} │ {status}"
            )

        self.print_separator("─", 79, Colors.CYAN)

    def print_results_header(self):
        """Print results section header"""
        header = f"""
{Colors.BOLD}{Colors.OKGREEN}╔═══════════════════════════════════════════════════════════════════════════════╗
║                              PROCESSING RESULTS                              ║
╚═══════════════════════════════════════════════════════════════════════════════╝{Colors.ENDC}
"""
        print(header)

    def print_error_box(self, title: str, error_msg: str):
        """Print formatted error box"""
        print(
            f"\n{Colors.FAIL}╔══════════════════════════════════════════════════════════════╗"
        )
        print(f"║ {Colors.BOLD}ERROR: {title:<50}{Colors.FAIL} ║")
        print("╠══════════════════════════════════════════════════════════════╣")

        words = error_msg.split()
        lines = []
        current_line = ""

        for word in words:
            if len(current_line + word) <= 54:
                current_line += word + " "
            else:
                lines.append(current_line.strip())
                current_line = word + " "
        if current_line:
            lines.append(current_line.strip())

        for line in lines:
            print(f"║ {line:<56} ║")

        print(
            f"╚══════════════════════════════════════════════════════════════╝{Colors.ENDC}"
        )

    def cleanup_cache(self):
        """清理Python缓存文件 / Clean up Python cache files"""
        try:
            self.print_status("Cleaning up cache files...", "info")
            # 清理__pycache__目录
            os.system('find . -type d -name "__pycache__" -exec rm -r {} + 2>/dev/null')
            # 清理.pyc文件
            os.system('find . -name "*.pyc" -delete 2>/dev/null')
            self.print_status("Cache cleanup completed", "success")
        except Exception as e:
            self.print_status(f"Cache cleanup failed: {e}", "warning")

    def print_goodbye(self):
        """Print goodbye message"""
        # 清理缓存文件
        self.cleanup_cache()

        goodbye = f"""
{Colors.BOLD}{Colors.CYAN}╔═══════════════════════════════════════════════════════════════════════════════╗
║                                GOODBYE                                        ║
╠═══════════════════════════════════════════════════════════════════════════════╣
║  {Colors.OKGREEN}🎉 Thank you for using DeepCode CLI!                                     {Colors.CYAN}║
║                                                                               ║
║  {Colors.YELLOW}🧬 Join our community in revolutionizing research reproducibility         {Colors.CYAN}║
║  {Colors.PURPLE}⚡ Together, we're building the future of automated code generation       {Colors.CYAN}║
║                                                                               ║
║  {Colors.OKCYAN}💡 Questions? Contribute to our open-source mission at GitHub             {Colors.CYAN}║
║  {Colors.GREEN}🧹 Cache files cleaned up for optimal performance                         {Colors.CYAN}║
║                                                                               ║
╚═══════════════════════════════════════════════════════════════════════════════╝{Colors.ENDC}
"""
        print(goodbye)

    def ask_continue(self) -> bool:
        """Ask if user wants to continue with another paper"""
        self.print_separator("─", 79, Colors.YELLOW)
        print(f"\n{Colors.BOLD}{Colors.YELLOW}🔄 Process another paper?{Colors.ENDC}")
        choice = input(f"{Colors.OKCYAN}Continue? (y/n): {Colors.ENDC}").strip().lower()
        return choice in ["y", "yes", "1", "true"]

    def add_to_history(self, input_source: str, result: dict):
        """Add processing result to history"""
        entry = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "input_source": input_source,
            "status": result.get("status", "unknown"),
            "result": result,
        }
        self.processing_history.append(entry)

    def show_history(self):
        """Display processing history"""
        if not self.processing_history:
            self.print_status("No processing history available", "info")
            return

        print(f"\n{Colors.BOLD}{Colors.CYAN}📚 PROCESSING HISTORY{Colors.ENDC}")
        self.print_separator("─", 79, Colors.CYAN)

        for i, entry in enumerate(self.processing_history, 1):
            status_icon = "✅" if entry["status"] == "success" else "❌"
            source = entry["input_source"]
            if len(source) > 50:
                source = source[:47] + "..."

            print(f"{i}. {status_icon} {entry['timestamp']} | {source}")

        self.print_separator("─", 79, Colors.CYAN)

    def show_configuration_menu(self):
        """Show configuration options menu"""
        self.clear_screen()

        # Get segmentation config status
        segmentation_enabled = getattr(self, "segmentation_enabled", True)
        segmentation_threshold = getattr(self, "segmentation_threshold", 50000)

        print(f"""
{Colors.BOLD}{Colors.CYAN}╔═══════════════════════════════════════════════════════════════════════════════╗
║                           CONFIGURATION MENU                                  ║
╠═══════════════════════════════════════════════════════════════════════════════╣
║                                                                               ║
║  {Colors.BOLD}🤖 Agent Orchestration Engine Configuration{Colors.CYAN}                             ║
║                                                                               ║
║  {Colors.OKCYAN}[1] Pipeline Mode:{Colors.CYAN}                                                        ║
║      {Colors.BOLD}🧠 Comprehensive Mode{Colors.CYAN} - Full intelligence analysis (Default)         ║
║         ✓ Research Analysis + Resource Processing                            ║
║         ✓ Reference Intelligence Discovery                                   ║
║         ✓ Automated Repository Acquisition                                   ║
║         ✓ Codebase Intelligence Orchestration                               ║
║         ✓ Intelligent Code Implementation Synthesis                         ║
║                                                                               ║
║      {Colors.BOLD}⚡ Optimized Mode{Colors.CYAN} - Fast processing (Skip indexing)                    ║
║         ✓ Research Analysis + Resource Processing                            ║
║         ✓ Code Architecture Synthesis                                        ║
║         ✓ Intelligent Code Implementation Synthesis                         ║
║         ✗ Reference Intelligence Discovery (Skipped)                        ║
║         ✗ Repository Acquisition (Skipped)                                   ║
║         ✗ Codebase Intelligence Orchestration (Skipped)                     ║
║                                                                               ║
║  {Colors.OKCYAN}[2] Document Processing:{Colors.CYAN}                                                   ║
║      {Colors.BOLD}📄 Smart Segmentation{Colors.CYAN} - Intelligent document analysis (Default)      ║
║         ✓ Semantic boundary detection                                        ║
║         ✓ Algorithm integrity preservation                                   ║
║         ✓ Formula chain recognition                                          ║
║         ✓ Adaptive character limits                                          ║
║                                                                               ║
║      {Colors.BOLD}📋 Traditional Processing{Colors.CYAN} - Full document reading                       ║
║         ✓ Complete document analysis                                         ║
║         ✗ Smart segmentation (Disabled)                                      ║
║                                                                               ║
║  {Colors.YELLOW}Current Settings:{Colors.CYAN}                                                         ║
║    Pipeline: {'🧠 Comprehensive Mode' if self.enable_indexing else '⚡ Optimized Mode'}                                          ║
║    Document: {'📄 Smart Segmentation' if segmentation_enabled else '📋 Traditional Processing'}                                ║
║    Threshold: {segmentation_threshold} characters                                    ║
║                                                                               ║
║  {Colors.OKGREEN}[T] Toggle Pipeline    {Colors.BLUE}[S] Toggle Segmentation    {Colors.FAIL}[B] Back{Colors.CYAN}     ║
╚═══════════════════════════════════════════════════════════════════════════════╝{Colors.ENDC}
""")

        while True:
            print(
                f"\n{Colors.BOLD}{Colors.OKCYAN}➤ Configuration choice: {Colors.ENDC}",
                end="",
            )
            choice = input().strip().lower()

            if choice in ["t", "toggle"]:
                self.enable_indexing = not self.enable_indexing
                mode = "🧠 Comprehensive" if self.enable_indexing else "⚡ Optimized"
                self.print_status(f"Pipeline mode switched to: {mode}", "success")
                time.sleep(1)
                self.show_configuration_menu()
                return

            elif choice in ["s", "segmentation"]:
                current_state = getattr(self, "segmentation_enabled", True)
                self.segmentation_enabled = not current_state
                seg_mode = (
                    "📄 Smart Segmentation"
                    if self.segmentation_enabled
                    else "📋 Traditional Processing"
                )
                self.print_status(
                    f"Document processing switched to: {seg_mode}", "success"
                )
                time.sleep(1)
                self.show_configuration_menu()
                return

            elif choice in ["b", "back"]:
                return

            else:
                self.print_status(
                    "Invalid choice. Please enter 'T', 'S', or 'B'.", "warning"
                )



================================================
FILE: cli/cli_launcher.py
================================================
#!/usr/bin/env python3
"""
DeepCode - CLI Research Engine Launcher
DeepCode - CLI研究引擎启动器

🧬 Open-Source Code Agent by Data Intelligence Lab @ HKU (CLI Edition)
⚡ Revolutionizing research reproducibility through collaborative AI via command line
"""

import sys
from pathlib import Path


def check_dependencies():
    """检查必要的依赖是否已安装 / Check if necessary dependencies are installed"""
    import importlib.util

    print("🔍 Checking CLI dependencies...")

    missing_deps = []

    # Check asyncio availability
    if importlib.util.find_spec("asyncio") is not None:
        print("✅ Asyncio is available")
    else:
        missing_deps.append("asyncio")

    # Check PyYAML availability
    if importlib.util.find_spec("yaml") is not None:
        print("✅ PyYAML is installed")
    else:
        missing_deps.append("pyyaml")

    # Check Tkinter availability
    if importlib.util.find_spec("tkinter") is not None:
        print("✅ Tkinter is available (for file dialogs)")
    else:
        print("⚠️  Tkinter not available - file dialogs will use manual input")

    # Check for MCP agent dependencies
    if importlib.util.find_spec("mcp_agent.app") is not None:
        print("✅ MCP Agent framework is available")
    else:
        missing_deps.append("mcp-agent")

    # Check for workflow dependencies
    # 添加项目根目录到路径
    current_dir = Path(__file__).parent
    project_root = current_dir.parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))

    if importlib.util.find_spec("workflows.agent_orchestration_engine") is not None:
        print("✅ Workflow modules are available")
    else:
        print("⚠️  Workflow modules may not be properly configured")

    # Check for CLI components
    if importlib.util.find_spec("cli.cli_app") is not None:
        print("✅ CLI application components are available")
    else:
        print("❌ CLI application components missing")
        missing_deps.append("cli-components")

    if missing_deps:
        print("\n❌ Missing dependencies:")
        for dep in missing_deps:
            print(f"   - {dep}")
        print("\nPlease install missing dependencies using:")
        print(
            f"pip install {' '.join([d for d in missing_deps if d != 'cli-components'])}"
        )
        if "cli-components" in missing_deps:
            print(
                "CLI components appear to be missing - please check the cli/ directory"
            )
        return False

    print("✅ All CLI dependencies satisfied")
    return True


def print_banner():
    """显示CLI启动横幅 / Display CLI startup banner"""
    banner = """
╔══════════════════════════════════════════════════════════════╗
║                                                              ║
║    🧬 DeepCode - Open-Source Code Agent                      ║
║                                                              ║
║    ⚡ DATA INTELLIGENCE LAB @ HKU ⚡                        ║
║                                                              ║
║                               ║
║                                                              ║
╚══════════════════════════════════════════════════════════════╝
"""
    print(banner)


def main():
    """主函数 / Main function"""
    print_banner()

    # 检查依赖 / Check dependencies
    if not check_dependencies():
        print("\n🚨 Please install missing dependencies and try again.")
        sys.exit(1)

    # 获取当前脚本目录 / Get current script directory
    current_dir = Path(__file__).parent
    project_root = current_dir.parent
    cli_app_path = current_dir / "cli_app.py"

    # 检查cli_app.py是否存在 / Check if cli_app.py exists
    if not cli_app_path.exists():
        print(f"❌ CLI application file not found: {cli_app_path}")
        print("Please ensure the cli/cli_app.py file exists.")
        sys.exit(1)

    print(f"\n📁 CLI App location: {cli_app_path}")
    print("🖥️  Starting DeepCode CLI interface...")
    print("🚀 Initializing command line application")
    print("=" * 70)
    print("💡 Tip: Follow the interactive prompts to process your research")
    print("🛑 Press Ctrl+C to exit at any time")
    print("=" * 70)

    # 启动CLI应用 / Launch CLI application
    try:
        # 导入并运行CLI应用
        if str(project_root) not in sys.path:
            sys.path.insert(0, str(project_root))  # 添加项目根目录到路径
        from cli.cli_app import main as cli_main

        print("\n🎯 Launching CLI application...")

        # 使用asyncio运行主函数
        import asyncio

        asyncio.run(cli_main())

    except KeyboardInterrupt:
        print("\n\n🛑 DeepCode CLI stopped by user")
        print("Thank you for using DeepCode CLI! 🧬")
    except ImportError as e:
        print(f"\n❌ Failed to import CLI application: {e}")
        print("Please check if all modules are properly installed.")
        sys.exit(1)
    except Exception as e:
        print(f"\n❌ Unexpected error: {e}")
        print("Please check your Python environment and try again.")
        sys.exit(1)


if __name__ == "__main__":
    main()



================================================
FILE: cli/main_cli.py
================================================
[Binary file]


================================================
FILE: cli/workflows/__init__.py
================================================
"""
CLI-specific Workflow Adapters
CLI专用工作流适配器

This module provides CLI-optimized versions of workflow components that are
specifically adapted for command-line interface usage patterns.
"""

from .cli_workflow_adapter import CLIWorkflowAdapter

__all__ = ["CLIWorkflowAdapter"]



================================================
FILE: cli/workflows/cli_workflow_adapter.py
================================================
"""
CLI Workflow Adapter for Agent Orchestration Engine
CLI工作流适配器 - 智能体编排引擎

This adapter provides CLI-optimized interface to the latest agent orchestration engine,
with enhanced progress reporting, error handling, and CLI-specific optimizations.
"""

import os
from typing import Callable, Dict, Any
from mcp_agent.app import MCPApp


class CLIWorkflowAdapter:
    """
    CLI-optimized workflow adapter for the intelligent agent orchestration engine.

    This adapter provides:
    - Enhanced CLI progress reporting
    - Optimized error handling for CLI environments
    - Streamlined interface for command-line usage
    - Integration with the latest agent orchestration engine
    """

    def __init__(self, cli_interface=None):
        """
        Initialize CLI workflow adapter.

        Args:
            cli_interface: CLI interface instance for progress reporting
        """
        self.cli_interface = cli_interface
        self.app = None
        self.logger = None
        self.context = None

    async def initialize_mcp_app(self) -> Dict[str, Any]:
        """
        Initialize MCP application for CLI usage.

        Returns:
            dict: Initialization result
        """
        try:
            if self.cli_interface:
                self.cli_interface.show_spinner(
                    "🚀 Initializing Agent Orchestration Engine", 2.0
                )

            # Initialize MCP application
            self.app = MCPApp(name="cli_agent_orchestration")
            self.app_context = self.app.run()
            agent_app = await self.app_context.__aenter__()

            self.logger = agent_app.logger
            self.context = agent_app.context

            # Configure filesystem access
            import os

            self.context.config.mcp.servers["filesystem"].args.extend([os.getcwd()])

            if self.cli_interface:
                self.cli_interface.print_status(
                    "🧠 Agent Orchestration Engine initialized successfully", "success"
                )

            return {
                "status": "success",
                "message": "MCP application initialized successfully",
            }

        except Exception as e:
            error_msg = f"Failed to initialize MCP application: {str(e)}"
            if self.cli_interface:
                self.cli_interface.print_status(error_msg, "error")
            return {"status": "error", "message": error_msg}

    async def cleanup_mcp_app(self):
        """
        Clean up MCP application resources.
        """
        if hasattr(self, "app_context"):
            try:
                await self.app_context.__aexit__(None, None, None)
                if self.cli_interface:
                    self.cli_interface.print_status(
                        "🧹 Resources cleaned up successfully", "info"
                    )
            except Exception as e:
                if self.cli_interface:
                    self.cli_interface.print_status(
                        f"⚠️ Cleanup warning: {str(e)}", "warning"
                    )

    def create_cli_progress_callback(self) -> Callable:
        """
        Create CLI-optimized progress callback function.

        Returns:
            Callable: Progress callback function
        """

        def progress_callback(progress: int, message: str):
            if self.cli_interface:
                # Map progress to CLI stages
                if progress <= 10:
                    self.cli_interface.display_processing_stages(1)
                elif progress <= 25:
                    self.cli_interface.display_processing_stages(2)
                elif progress <= 40:
                    self.cli_interface.display_processing_stages(3)
                elif progress <= 50:
                    self.cli_interface.display_processing_stages(4)
                elif progress <= 60:
                    self.cli_interface.display_processing_stages(5)
                elif progress <= 70:
                    self.cli_interface.display_processing_stages(6)
                elif progress <= 85:
                    self.cli_interface.display_processing_stages(7)
                else:
                    self.cli_interface.display_processing_stages(8)

                # Display status message
                self.cli_interface.print_status(message, "processing")

        return progress_callback

    async def execute_full_pipeline(
        self, input_source: str, enable_indexing: bool = True
    ) -> Dict[str, Any]:
        """
        Execute the complete intelligent multi-agent research orchestration pipeline.

        Args:
            input_source: Research input source (file path, URL, or preprocessed analysis)
            enable_indexing: Whether to enable advanced intelligence analysis

        Returns:
            dict: Comprehensive pipeline execution result
        """
        try:
            # Import the latest agent orchestration engine
            from workflows.agent_orchestration_engine import (
                execute_multi_agent_research_pipeline,
            )

            # Create CLI progress callback
            progress_callback = self.create_cli_progress_callback()

            # Display pipeline start
            if self.cli_interface:
                mode = "comprehensive" if enable_indexing else "optimized"
                self.cli_interface.print_status(
                    f"🚀 Starting {mode} agent orchestration pipeline...", "processing"
                )
                self.cli_interface.display_processing_stages(0)

            # Execute the pipeline
            result = await execute_multi_agent_research_pipeline(
                input_source=input_source,
                logger=self.logger,
                progress_callback=progress_callback,
                enable_indexing=enable_indexing,
            )

            # Display completion
            if self.cli_interface:
                self.cli_interface.display_processing_stages(8)
                self.cli_interface.print_status(
                    "🎉 Agent orchestration pipeline completed successfully!",
                    "complete",
                )

            return {
                "status": "success",
                "result": result,
                "pipeline_mode": "comprehensive" if enable_indexing else "optimized",
            }

        except Exception as e:
            error_msg = f"Pipeline execution failed: {str(e)}"
            if self.cli_interface:
                self.cli_interface.print_status(error_msg, "error")

            return {
                "status": "error",
                "error": error_msg,
                "pipeline_mode": "comprehensive" if enable_indexing else "optimized",
            }

    async def execute_chat_pipeline(self, user_input: str) -> Dict[str, Any]:
        """
        Execute the chat-based planning and implementation pipeline.

        Args:
            user_input: User's coding requirements and description

        Returns:
            dict: Chat pipeline execution result
        """
        try:
            # Import the chat-based pipeline
            from workflows.agent_orchestration_engine import (
                execute_chat_based_planning_pipeline,
            )

            # Create CLI progress callback for chat mode
            def chat_progress_callback(progress: int, message: str):
                if self.cli_interface:
                    # Map progress to CLI stages for chat mode
                    if progress <= 5:
                        self.cli_interface.display_processing_stages(
                            0, chat_mode=True
                        )  # Initialize
                    elif progress <= 30:
                        self.cli_interface.display_processing_stages(
                            1, chat_mode=True
                        )  # Planning
                    elif progress <= 50:
                        self.cli_interface.display_processing_stages(
                            2, chat_mode=True
                        )  # Setup
                    elif progress <= 70:
                        self.cli_interface.display_processing_stages(
                            3, chat_mode=True
                        )  # Save Plan
                    else:
                        self.cli_interface.display_processing_stages(
                            4, chat_mode=True
                        )  # Implement

                    # Display status message
                    self.cli_interface.print_status(message, "processing")

            # Display pipeline start
            if self.cli_interface:
                self.cli_interface.print_status(
                    "🚀 Starting chat-based planning pipeline...", "processing"
                )
                self.cli_interface.display_processing_stages(0, chat_mode=True)

            # Execute the chat pipeline with indexing enabled for enhanced code understanding
            result = await execute_chat_based_planning_pipeline(
                user_input=user_input,
                logger=self.logger,
                progress_callback=chat_progress_callback,
                enable_indexing=True,  # Enable indexing for better code implementation
            )

            # Display completion
            if self.cli_interface:
                self.cli_interface.display_processing_stages(
                    4, chat_mode=True
                )  # Final stage for chat mode
                self.cli_interface.print_status(
                    "🎉 Chat-based planning pipeline completed successfully!",
                    "complete",
                )

            return {"status": "success", "result": result, "pipeline_mode": "chat"}

        except Exception as e:
            error_msg = f"Chat pipeline execution failed: {str(e)}"
            if self.cli_interface:
                self.cli_interface.print_status(error_msg, "error")

            return {"status": "error", "error": error_msg, "pipeline_mode": "chat"}

    async def process_input_with_orchestration(
        self, input_source: str, input_type: str, enable_indexing: bool = True
    ) -> Dict[str, Any]:
        """
        Process input using the intelligent agent orchestration engine.

        This is the main CLI interface to the latest agent orchestration capabilities.

        Args:
            input_source: Input source (file path or URL)
            input_type: Type of input ('file' or 'url')
            enable_indexing: Whether to enable advanced intelligence analysis

        Returns:
            dict: Processing result with status and details
        """
        pipeline_result = None

        try:
            # Initialize MCP app
            init_result = await self.initialize_mcp_app()
            if init_result["status"] != "success":
                return init_result

            # Process file:// URLs for traditional file/URL inputs
            if input_source.startswith("file://"):
                file_path = input_source[7:]
                if os.name == "nt" and file_path.startswith("/"):
                    file_path = file_path.lstrip("/")
                input_source = file_path

            # Execute appropriate pipeline based on input type
            if input_type == "chat":
                # Use chat-based planning pipeline for user requirements
                pipeline_result = await self.execute_chat_pipeline(input_source)
            else:
                # Use traditional multi-agent research pipeline for files/URLs
                pipeline_result = await self.execute_full_pipeline(
                    input_source, enable_indexing=enable_indexing
                )

            return {
                "status": pipeline_result["status"],
                "analysis_result": "Integrated into agent orchestration pipeline",
                "download_result": "Integrated into agent orchestration pipeline",
                "repo_result": pipeline_result.get("result", ""),
                "pipeline_mode": pipeline_result.get("pipeline_mode", "comprehensive"),
                "error": pipeline_result.get("error"),
            }

        except Exception as e:
            error_msg = f"Error during orchestrated processing: {str(e)}"
            if self.cli_interface:
                self.cli_interface.print_status(error_msg, "error")

            return {
                "status": "error",
                "error": error_msg,
                "analysis_result": "",
                "download_result": "",
                "repo_result": "",
                "pipeline_mode": "comprehensive" if enable_indexing else "optimized",
            }

        finally:
            # Clean up resources
            await self.cleanup_mcp_app()



================================================
FILE: config/mcp_tool_definitions.py
================================================
"""
MCP工具定义配置模块
MCP Tool Definitions Configuration Module

将工具定义从主程序逻辑中分离，提供标准化的工具定义格式
Separate tool definitions from main program logic, providing standardized tool definition format

支持的工具类型：
- 文件操作工具 (File Operations)
- 代码执行工具 (Code Execution)
- 搜索工具 (Search Tools)
- 项目结构工具 (Project Structure Tools)
"""

from typing import Dict, List, Any


class MCPToolDefinitions:
    """MCP工具定义管理器"""

    @staticmethod
    def get_code_implementation_tools() -> List[Dict[str, Any]]:
        """
        获取代码实现相关的工具定义
        Get tool definitions for code implementation
        """
        return [
            MCPToolDefinitions._get_read_file_tool(),
            MCPToolDefinitions._get_read_multiple_files_tool(),
            MCPToolDefinitions._get_read_code_mem_tool(),
            MCPToolDefinitions._get_write_file_tool(),
            MCPToolDefinitions._get_write_multiple_files_tool(),
            MCPToolDefinitions._get_execute_python_tool(),
            MCPToolDefinitions._get_execute_bash_tool(),
        ]

    @staticmethod
    def _get_read_file_tool() -> Dict[str, Any]:
        """读取文件工具定义"""
        return {
            "name": "read_file",
            "description": "Read file content, supports specifying line number range",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_path": {
                        "type": "string",
                        "description": "File path, relative to workspace",
                    },
                    "start_line": {
                        "type": "integer",
                        "description": "Start line number (starting from 1, optional)",
                    },
                    "end_line": {
                        "type": "integer",
                        "description": "End line number (starting from 1, optional)",
                    },
                },
                "required": ["file_path"],
            },
        }

    @staticmethod
    def _get_read_multiple_files_tool() -> Dict[str, Any]:
        """批量读取多个文件工具定义"""
        return {
            "name": "read_multiple_files",
            "description": "Read multiple files in a single operation (for batch reading)",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_requests": {
                        "type": "string",
                        "description": 'JSON string with file requests, e.g., \'{"file1.py": {}, "file2.py": {"start_line": 1, "end_line": 10}}\' or simple array \'["file1.py", "file2.py"]\'',
                    },
                    "max_files": {
                        "type": "integer",
                        "description": "Maximum number of files to read in one operation",
                        "default": 5,
                        "minimum": 1,
                        "maximum": 10,
                    },
                },
                "required": ["file_requests"],
            },
        }

    @staticmethod
    def _get_read_code_mem_tool() -> Dict[str, Any]:
        """Read code memory tool definition - reads from implement_code_summary.md"""
        return {
            "name": "read_code_mem",
            "description": "Check if file summaries exist in implement_code_summary.md for multiple files in a single call. Returns summaries for all requested files if available.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_paths": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "List of file paths to check for summary information in implement_code_summary.md",
                    }
                },
                "required": ["file_paths"],
            },
        }

    @staticmethod
    def _get_write_file_tool() -> Dict[str, Any]:
        """写入文件工具定义"""
        return {
            "name": "write_file",
            "description": "Write content to file",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_path": {
                        "type": "string",
                        "description": "File path, relative to workspace",
                    },
                    "content": {
                        "type": "string",
                        "description": "Content to write to file",
                    },
                    "create_dirs": {
                        "type": "boolean",
                        "description": "Whether to create directories if they don't exist",
                        "default": True,
                    },
                    "create_backup": {
                        "type": "boolean",
                        "description": "Whether to create backup file if file already exists",
                        "default": False,
                    },
                },
                "required": ["file_path", "content"],
            },
        }

    @staticmethod
    def _get_write_multiple_files_tool() -> Dict[str, Any]:
        """批量写入多个文件工具定义"""
        return {
            "name": "write_multiple_files",
            "description": "Write multiple files in a single operation (for batch implementation)",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_implementations": {
                        "type": "string",
                        "description": 'JSON string mapping file paths to content, e.g., \'{"file1.py": "content1", "file2.py": "content2"}\'',
                    },
                    "create_dirs": {
                        "type": "boolean",
                        "description": "Whether to create directories if they don't exist",
                        "default": True,
                    },
                    "create_backup": {
                        "type": "boolean",
                        "description": "Whether to create backup files if they already exist",
                        "default": False,
                    },
                    "max_files": {
                        "type": "integer",
                        "description": "Maximum number of files to write in one operation",
                        "default": 5,
                        "minimum": 1,
                        "maximum": 10,
                    },
                },
                "required": ["file_implementations"],
            },
        }

    @staticmethod
    def _get_execute_python_tool() -> Dict[str, Any]:
        """Python执行工具定义"""
        return {
            "name": "execute_python",
            "description": "Execute Python code and return output",
            "input_schema": {
                "type": "object",
                "properties": {
                    "code": {"type": "string", "description": "Python code to execute"},
                    "timeout": {
                        "type": "integer",
                        "description": "Timeout in seconds",
                        "default": 30,
                    },
                },
                "required": ["code"],
            },
        }

    @staticmethod
    def _get_execute_bash_tool() -> Dict[str, Any]:
        """Bash执行工具定义"""
        return {
            "name": "execute_bash",
            "description": "Execute bash command",
            "input_schema": {
                "type": "object",
                "properties": {
                    "command": {
                        "type": "string",
                        "description": "Bash command to execute",
                    },
                    "timeout": {
                        "type": "integer",
                        "description": "Timeout in seconds",
                        "default": 30,
                    },
                },
                "required": ["command"],
            },
        }

    @staticmethod
    def _get_file_structure_tool() -> Dict[str, Any]:
        """文件结构获取工具定义"""
        return {
            "name": "get_file_structure",
            "description": "Get directory file structure",
            "input_schema": {
                "type": "object",
                "properties": {
                    "directory": {
                        "type": "string",
                        "description": "Directory path, relative to workspace",
                        "default": ".",
                    },
                    "max_depth": {
                        "type": "integer",
                        "description": "Maximum traversal depth",
                        "default": 5,
                    },
                },
            },
        }

    @staticmethod
    def _get_search_code_references_tool() -> Dict[str, Any]:
        """统一代码参考搜索工具定义 - 合并了三个步骤为一个工具"""
        return {
            "name": "search_code_references",
            "description": "UNIFIED TOOL: Search relevant reference code from index files. Combines directory setup, index loading, and searching in a single call.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "indexes_path": {
                        "type": "string",
                        "description": "Path to the indexes directory containing JSON index files",
                    },
                    "target_file": {
                        "type": "string",
                        "description": "Target file path to be implemented",
                    },
                    "keywords": {
                        "type": "string",
                        "description": "Search keywords, comma-separated",
                        "default": "",
                    },
                    "max_results": {
                        "type": "integer",
                        "description": "Maximum number of results to return",
                        "default": 10,
                    },
                },
                "required": ["indexes_path", "target_file"],
            },
        }

    @staticmethod
    def _get_get_indexes_overview_tool() -> Dict[str, Any]:
        """获取索引概览工具定义"""
        return {
            "name": "get_indexes_overview",
            "description": "Get overview of all available reference code index information from specified directory",
            "input_schema": {
                "type": "object",
                "properties": {
                    "indexes_path": {
                        "type": "string",
                        "description": "Path to the indexes directory containing JSON index files",
                    }
                },
                "required": ["indexes_path"],
            },
        }

    @staticmethod
    def _get_set_workspace_tool() -> Dict[str, Any]:
        """Set workspace directory tool definition"""
        return {
            "name": "set_workspace",
            "description": "Set the workspace directory for file operations",
            "input_schema": {
                "type": "object",
                "properties": {
                    "workspace_path": {
                        "type": "string",
                        "description": "Directory path for the workspace",
                    }
                },
                "required": ["workspace_path"],
            },
        }

    # @staticmethod
    # def _get_set_indexes_directory_tool() -> Dict[str, Any]:
    #     """Set indexes directory tool definition - DEPRECATED: Use unified search_code_references instead"""
    #     return {
    #         "name": "set_indexes_directory",
    #         "description": "Set the directory path for code reference indexes",
    #         "input_schema": {
    #             "type": "object",
    #             "properties": {
    #                 "indexes_path": {
    #                     "type": "string",
    #                     "description": "Directory path containing index JSON files"
    #                 }
    #             },
    #             "required": ["indexes_path"]
    #         }
    #     }

    @staticmethod
    def get_available_tool_sets() -> Dict[str, str]:
        """
        获取可用的工具集合
        Get available tool sets
        """
        return {
            "code_implementation": "代码实现相关工具集 / Code implementation tool set",
            # 可以在这里添加更多工具集
            # "data_analysis": "数据分析工具集 / Data analysis tool set",
            # "web_scraping": "网页爬取工具集 / Web scraping tool set",
        }

    @staticmethod
    def get_tool_set(tool_set_name: str) -> List[Dict[str, Any]]:
        """
        根据名称获取特定的工具集
        Get specific tool set by name
        """
        tool_sets = {
            "code_implementation": MCPToolDefinitions.get_code_implementation_tools(),
        }

        return tool_sets.get(tool_set_name, [])

    @staticmethod
    def get_all_tools() -> List[Dict[str, Any]]:
        """
        获取所有可用工具
        Get all available tools
        """
        all_tools = []
        for tool_set_name in MCPToolDefinitions.get_available_tool_sets().keys():
            all_tools.extend(MCPToolDefinitions.get_tool_set(tool_set_name))
        return all_tools


# 便捷访问函数
def get_mcp_tools(tool_set: str = "code_implementation") -> List[Dict[str, Any]]:
    """
    便捷函数：获取MCP工具定义
    Convenience function: Get MCP tool definitions

    Args:
        tool_set: 工具集名称 (默认: "code_implementation")

    Returns:
        工具定义列表
    """
    return MCPToolDefinitions.get_tool_set(tool_set)



================================================
FILE: config/mcp_tool_definitions_index.py
================================================
"""
MCP工具定义配置模块
MCP Tool Definitions Configuration Module

将工具定义从主程序逻辑中分离，提供标准化的工具定义格式
Separate tool definitions from main program logic, providing standardized tool definition format

支持的工具类型：
- 文件操作工具 (File Operations)
- 代码执行工具 (Code Execution)
- 搜索工具 (Search Tools)
- 项目结构工具 (Project Structure Tools)
"""

from typing import Dict, List, Any


class MCPToolDefinitions:
    """MCP工具定义管理器"""

    @staticmethod
    def get_code_implementation_tools() -> List[Dict[str, Any]]:
        """
        获取代码实现相关的工具定义
        Get tool definitions for code implementation
        """
        return [
            MCPToolDefinitions._get_read_file_tool(),
            MCPToolDefinitions._get_read_multiple_files_tool(),
            MCPToolDefinitions._get_read_code_mem_tool(),
            MCPToolDefinitions._get_write_file_tool(),
            MCPToolDefinitions._get_write_multiple_files_tool(),
            MCPToolDefinitions._get_execute_python_tool(),
            MCPToolDefinitions._get_execute_bash_tool(),
            MCPToolDefinitions._get_search_code_references_tool(),
            MCPToolDefinitions._get_search_code_tool(),
            MCPToolDefinitions._get_file_structure_tool(),
            MCPToolDefinitions._get_set_workspace_tool(),
            MCPToolDefinitions._get_operation_history_tool(),
        ]

    @staticmethod
    def get_code_evaluation_tools() -> List[Dict[str, Any]]:
        """
        获取代码评估相关的工具定义
        Get tool definitions for code evaluation
        """
        return [
            MCPToolDefinitions._get_analyze_repo_structure_tool(),
            MCPToolDefinitions._get_detect_dependencies_tool(),
            MCPToolDefinitions._get_assess_code_quality_tool(),
            MCPToolDefinitions._get_evaluate_documentation_tool(),
            MCPToolDefinitions._get_check_reproduction_readiness_tool(),
            MCPToolDefinitions._get_generate_evaluation_summary_tool(),
            MCPToolDefinitions._get_detect_empty_files_tool(),
            MCPToolDefinitions._get_detect_missing_files_tool(),
            MCPToolDefinitions._get_generate_code_revision_report_tool(),
        ]

    @staticmethod
    def _get_read_file_tool() -> Dict[str, Any]:
        """读取文件工具定义"""
        return {
            "name": "read_file",
            "description": "Read file content, supports specifying line number range",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_path": {
                        "type": "string",
                        "description": "File path, relative to workspace",
                    },
                    "start_line": {
                        "type": "integer",
                        "description": "Start line number (starting from 1, optional)",
                    },
                    "end_line": {
                        "type": "integer",
                        "description": "End line number (starting from 1, optional)",
                    },
                },
                "required": ["file_path"],
            },
        }

    @staticmethod
    def _get_read_multiple_files_tool() -> Dict[str, Any]:
        """批量读取多个文件工具定义"""
        return {
            "name": "read_multiple_files",
            "description": "Read multiple files in a single operation (for batch reading)",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_requests": {
                        "type": "string",
                        "description": 'JSON string with file requests, e.g., \'{"file1.py": {}, "file2.py": {"start_line": 1, "end_line": 10}}\' or simple array \'["file1.py", "file2.py"]\'',
                    },
                    "max_files": {
                        "type": "integer",
                        "description": "Maximum number of files to read in one operation",
                        "default": 5,
                        "minimum": 1,
                        "maximum": 10,
                    },
                },
                "required": ["file_requests"],
            },
        }

    @staticmethod
    def _get_read_code_mem_tool() -> Dict[str, Any]:
        """Read code memory tool definition - reads from implement_code_summary.md"""
        return {
            "name": "read_code_mem",
            "description": "Check if file summaries exist in implement_code_summary.md for multiple files in a single call. Returns summaries for all requested files if available.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_paths": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "List of file paths to check for summary information in implement_code_summary.md",
                    }
                },
                "required": ["file_paths"],
            },
        }

    @staticmethod
    def _get_write_file_tool() -> Dict[str, Any]:
        """写入文件工具定义"""
        return {
            "name": "write_file",
            "description": "Write content to file",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_path": {
                        "type": "string",
                        "description": "File path, relative to workspace",
                    },
                    "content": {
                        "type": "string",
                        "description": "Content to write to file",
                    },
                    "create_dirs": {
                        "type": "boolean",
                        "description": "Whether to create directories if they don't exist",
                        "default": True,
                    },
                    "create_backup": {
                        "type": "boolean",
                        "description": "Whether to create backup file if file already exists",
                        "default": False,
                    },
                },
                "required": ["file_path", "content"],
            },
        }

    @staticmethod
    def _get_write_multiple_files_tool() -> Dict[str, Any]:
        """批量写入多个文件工具定义"""
        return {
            "name": "write_multiple_files",
            "description": "Write multiple files in a single operation (for batch implementation)",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_implementations": {
                        "type": "string",
                        "description": 'JSON string mapping file paths to content, e.g., \'{"file1.py": "content1", "file2.py": "content2"}\'',
                    },
                    "create_dirs": {
                        "type": "boolean",
                        "description": "Whether to create directories if they don't exist",
                        "default": True,
                    },
                    "create_backup": {
                        "type": "boolean",
                        "description": "Whether to create backup files if they already exist",
                        "default": False,
                    },
                    "max_files": {
                        "type": "integer",
                        "description": "Maximum number of files to write in one operation",
                        "default": 5,
                        "minimum": 1,
                        "maximum": 10,
                    },
                },
                "required": ["file_implementations"],
            },
        }

    @staticmethod
    def _get_execute_python_tool() -> Dict[str, Any]:
        """Python执行工具定义"""
        return {
            "name": "execute_python",
            "description": "Execute Python code and return output",
            "input_schema": {
                "type": "object",
                "properties": {
                    "code": {"type": "string", "description": "Python code to execute"},
                    "timeout": {
                        "type": "integer",
                        "description": "Timeout in seconds",
                        "default": 30,
                    },
                },
                "required": ["code"],
            },
        }

    @staticmethod
    def _get_execute_bash_tool() -> Dict[str, Any]:
        """Bash执行工具定义"""
        return {
            "name": "execute_bash",
            "description": "Execute bash command",
            "input_schema": {
                "type": "object",
                "properties": {
                    "command": {
                        "type": "string",
                        "description": "Bash command to execute",
                    },
                    "timeout": {
                        "type": "integer",
                        "description": "Timeout in seconds",
                        "default": 30,
                    },
                },
                "required": ["command"],
            },
        }

    @staticmethod
    def _get_file_structure_tool() -> Dict[str, Any]:
        """文件结构获取工具定义"""
        return {
            "name": "get_file_structure",
            "description": "Get directory file structure",
            "input_schema": {
                "type": "object",
                "properties": {
                    "directory": {
                        "type": "string",
                        "description": "Directory path, relative to workspace",
                        "default": ".",
                    },
                    "max_depth": {
                        "type": "integer",
                        "description": "Maximum traversal depth",
                        "default": 5,
                    },
                },
            },
        }

    @staticmethod
    def _get_search_code_references_tool() -> Dict[str, Any]:
        """统一代码参考搜索工具定义 - 合并了三个步骤为一个工具"""
        return {
            "name": "search_code_references",
            "description": "UNIFIED TOOL: Search relevant reference code from index files. Combines directory setup, index loading, and searching in a single call.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "indexes_path": {
                        "type": "string",
                        "description": "Path to the indexes directory containing JSON index files",
                    },
                    "target_file": {
                        "type": "string",
                        "description": "Target file path to be implemented",
                    },
                    "keywords": {
                        "type": "string",
                        "description": "Search keywords, comma-separated",
                        "default": "",
                    },
                    "max_results": {
                        "type": "integer",
                        "description": "Maximum number of results to return",
                        "default": 10,
                    },
                },
                "required": ["indexes_path", "target_file"],
            },
        }

    @staticmethod
    def _get_search_code_tool() -> Dict[str, Any]:
        """代码搜索工具定义 - 在当前代码库中搜索模式"""
        return {
            "name": "search_code",
            "description": "Search patterns in code files within the current repository",
            "input_schema": {
                "type": "object",
                "properties": {
                    "pattern": {
                        "type": "string",
                        "description": "Search pattern",
                    },
                    "file_pattern": {
                        "type": "string",
                        "description": "File pattern (e.g., '*.py')",
                        "default": "*.py",
                    },
                    "use_regex": {
                        "type": "boolean",
                        "description": "Whether to use regular expressions",
                        "default": False,
                    },
                    "search_directory": {
                        "type": "string",
                        "description": "Specify search directory (optional)",
                    },
                },
                "required": ["pattern"],
            },
        }

    @staticmethod
    def _get_operation_history_tool() -> Dict[str, Any]:
        """操作历史工具定义"""
        return {
            "name": "get_operation_history",
            "description": "Get operation history",
            "input_schema": {
                "type": "object",
                "properties": {
                    "last_n": {
                        "type": "integer",
                        "description": "Return the last N operations",
                        "default": 10,
                    },
                },
            },
        }

    @staticmethod
    def _get_get_indexes_overview_tool() -> Dict[str, Any]:
        """获取索引概览工具定义"""
        return {
            "name": "get_indexes_overview",
            "description": "Get overview of all available reference code index information from specified directory",
            "input_schema": {
                "type": "object",
                "properties": {
                    "indexes_path": {
                        "type": "string",
                        "description": "Path to the indexes directory containing JSON index files",
                    }
                },
                "required": ["indexes_path"],
            },
        }

    @staticmethod
    def _get_set_workspace_tool() -> Dict[str, Any]:
        """Set workspace directory tool definition"""
        return {
            "name": "set_workspace",
            "description": "Set the workspace directory for file operations",
            "input_schema": {
                "type": "object",
                "properties": {
                    "workspace_path": {
                        "type": "string",
                        "description": "Directory path for the workspace",
                    }
                },
                "required": ["workspace_path"],
            },
        }

    # @staticmethod
    # def _get_set_indexes_directory_tool() -> Dict[str, Any]:
    #     """Set indexes directory tool definition - DEPRECATED: Use unified search_code_references instead"""
    #     return {
    #         "name": "set_indexes_directory",
    #         "description": "Set the directory path for code reference indexes",
    #         "input_schema": {
    #             "type": "object",
    #             "properties": {
    #                 "indexes_path": {
    #                     "type": "string",
    #                     "description": "Directory path containing index JSON files"
    #                 }
    #             },
    #             "required": ["indexes_path"]
    #         }
    #     }

    # Code evaluation tool definitions
    @staticmethod
    def _get_analyze_repo_structure_tool() -> Dict[str, Any]:
        return {
            "name": "analyze_repo_structure",
            "description": "Perform comprehensive repository structure analysis",
            "input_schema": {
                "type": "object",
                "properties": {
                    "repo_path": {
                        "type": "string",
                        "description": "Path to the repository to analyze",
                    }
                },
                "required": ["repo_path"],
            },
        }

    @staticmethod
    def _get_detect_dependencies_tool() -> Dict[str, Any]:
        return {
            "name": "detect_dependencies",
            "description": "Detect and analyze project dependencies across multiple languages",
            "input_schema": {
                "type": "object",
                "properties": {
                    "repo_path": {
                        "type": "string",
                        "description": "Path to the repository",
                    }
                },
                "required": ["repo_path"],
            },
        }

    @staticmethod
    def _get_assess_code_quality_tool() -> Dict[str, Any]:
        return {
            "name": "assess_code_quality",
            "description": "Assess code quality metrics and identify potential issues",
            "input_schema": {
                "type": "object",
                "properties": {
                    "repo_path": {
                        "type": "string",
                        "description": "Path to the repository",
                    }
                },
                "required": ["repo_path"],
            },
        }

    @staticmethod
    def _get_evaluate_documentation_tool() -> Dict[str, Any]:
        return {
            "name": "evaluate_documentation",
            "description": "Evaluate documentation completeness and quality",
            "input_schema": {
                "type": "object",
                "properties": {
                    "repo_path": {
                        "type": "string",
                        "description": "Path to the repository",
                    },
                    "docs_path": {
                        "type": "string",
                        "description": "Optional path to external documentation",
                    },
                },
                "required": ["repo_path"],
            },
        }

    @staticmethod
    def _get_check_reproduction_readiness_tool() -> Dict[str, Any]:
        return {
            "name": "check_reproduction_readiness",
            "description": "Assess repository readiness for reproduction and validation",
            "input_schema": {
                "type": "object",
                "properties": {
                    "repo_path": {
                        "type": "string",
                        "description": "Path to the repository",
                    },
                    "docs_path": {
                        "type": "string",
                        "description": "Optional path to reproduction documentation",
                    },
                },
                "required": ["repo_path"],
            },
        }

    @staticmethod
    def _get_generate_evaluation_summary_tool() -> Dict[str, Any]:
        return {
            "name": "generate_evaluation_summary",
            "description": "Generate comprehensive evaluation summary combining all analysis results",
            "input_schema": {
                "type": "object",
                "properties": {
                    "repo_path": {
                        "type": "string",
                        "description": "Path to the repository",
                    },
                    "docs_path": {
                        "type": "string",
                        "description": "Optional path to reproduction documentation",
                    },
                },
                "required": ["repo_path"],
            },
        }

    @staticmethod
    def _get_detect_empty_files_tool() -> Dict[str, Any]:
        return {
            "name": "detect_empty_files",
            "description": "Detect empty files in the repository that may need implementation",
            "input_schema": {
                "type": "object",
                "properties": {
                    "repo_path": {
                        "type": "string",
                        "description": "Path to the repository to analyze",
                    }
                },
                "required": ["repo_path"],
            },
        }

    @staticmethod
    def _get_detect_missing_files_tool() -> Dict[str, Any]:
        return {
            "name": "detect_missing_files",
            "description": "Detect missing essential files like main programs, tests, requirements, etc.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "repo_path": {
                        "type": "string",
                        "description": "Path to the repository to analyze",
                    }
                },
                "required": ["repo_path"],
            },
        }

    @staticmethod
    def _get_generate_code_revision_report_tool() -> Dict[str, Any]:
        return {
            "name": "generate_code_revision_report",
            "description": "Generate comprehensive code revision report combining empty files, missing files, and quality analysis",
            "input_schema": {
                "type": "object",
                "properties": {
                    "repo_path": {
                        "type": "string",
                        "description": "Path to the repository to analyze",
                    },
                    "docs_path": {
                        "type": "string",
                        "description": "Optional path to documentation",
                    },
                },
                "required": ["repo_path"],
            },
        }

    @staticmethod
    def get_available_tool_sets() -> Dict[str, str]:
        """
        获取可用的工具集合
        Get available tool sets
        """
        return {
            "code_implementation": "代码实现相关工具集 / Code implementation tool set",
            "code_evaluation": "代码评估相关工具集 / Code evaluation tool set",
            # 可以在这里添加更多工具集
            # "data_analysis": "数据分析工具集 / Data analysis tool set",
            # "web_scraping": "网页爬取工具集 / Web scraping tool set",
        }

    @staticmethod
    def get_tool_set(tool_set_name: str) -> List[Dict[str, Any]]:
        """
        根据名称获取特定的工具集
        Get specific tool set by name
        """
        tool_sets = {
            "code_implementation": MCPToolDefinitions.get_code_implementation_tools(),
            "code_evaluation": MCPToolDefinitions.get_code_evaluation_tools(),
        }

        return tool_sets.get(tool_set_name, [])

    @staticmethod
    def get_all_tools() -> List[Dict[str, Any]]:
        """
        获取所有可用工具
        Get all available tools
        """
        all_tools = []
        for tool_set_name in MCPToolDefinitions.get_available_tool_sets().keys():
            all_tools.extend(MCPToolDefinitions.get_tool_set(tool_set_name))
        return all_tools


# 便捷访问函数
def get_mcp_tools(tool_set: str = "code_implementation") -> List[Dict[str, Any]]:
    """
    便捷函数：获取MCP工具定义
    Convenience function: Get MCP tool definitions

    Args:
        tool_set: 工具集名称 (默认: "code_implementation")

    Returns:
        工具定义列表
    """
    return MCPToolDefinitions.get_tool_set(tool_set)



================================================
FILE: schema/mcp-agent.config.schema.json
================================================
{
  "$defs": {
    "LogPathSettings": {
      "description": "Settings for configuring log file paths with dynamic elements like timestamps or session IDs.",
      "properties": {
        "path_pattern": {
          "default": "logs/mcp-agent-{unique_id}.jsonl",
          "title": "Path Pattern",
          "type": "string",
          "description": "Path pattern for log files with a {unique_id} placeholder"
        },
        "unique_id": {
          "default": "timestamp",
          "enum": [
            "timestamp",
            "session_id"
          ],
          "title": "Unique Id",
          "type": "string",
          "description": "Type of unique identifier to use in the log filename"
        },
        "timestamp_format": {
          "default": "%Y%m%d_%H%M%S",
          "title": "Timestamp Format",
          "type": "string",
          "description": "Format string for timestamps when unique_id is set to timestamp"
        }
      },
      "title": "LogPathSettings",
      "type": "object"
    },
    "AnthropicSettings": {
      "additionalProperties": true,
      "description": "Settings for using Anthropic models in the MCP Agent application.",
      "properties": {
        "api_key": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Api Key"
        }
      },
      "title": "AnthropicSettings",
      "type": "object"
    },
    "CohereSettings": {
      "additionalProperties": true,
      "description": "Settings for using Cohere models in the MCP Agent application.",
      "properties": {
        "api_key": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Api Key"
        }
      },
      "title": "CohereSettings",
      "type": "object"
    },
    "LoggerSettings": {
      "description": "Logger settings for the MCP Agent application.",
      "properties": {
        "type": {
          "default": "console",
          "enum": [
            "none",
            "console",
            "file",
            "http"
          ],
          "title": "Type",
          "type": "string"
        },
        "transports": {
          "default": [
            "console"
          ],
          "items": {
            "enum": [
              "none",
              "console",
              "file",
              "http"
            ],
            "type": "string"
          },
          "title": "Transports",
          "type": "array",
          "description": "List of transports to use (can enable multiple simultaneously)"
        },
        "level": {
          "default": "info",
          "enum": [
            "debug",
            "info",
            "warning",
            "error"
          ],
          "title": "Level",
          "type": "string",
          "description": "Minimum logging level"
        },
        "progress_display": {
          "default": true,
          "title": "Progress Display",
          "type": "boolean",
          "description": "Enable or disable the progress display"
        },
        "path": {
          "default": "mcp-agent.jsonl",
          "title": "Path",
          "type": "string",
          "description": "Path to log file, if logger 'type' is 'file'."
        },
        "path_settings": {
          "anyOf": [
            {
              "$ref": "#/$defs/LogPathSettings"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Path Settings",
          "description": "Advanced settings for log file paths with dynamic elements like timestamps or session IDs"
        },
        "batch_size": {
          "default": 100,
          "title": "Batch Size",
          "type": "integer",
          "description": "Number of events to accumulate before processing"
        },
        "flush_interval": {
          "default": 2.0,
          "title": "Flush Interval",
          "type": "number",
          "description": "How often to flush events in seconds"
        },
        "max_queue_size": {
          "default": 2048,
          "title": "Max Queue Size",
          "type": "integer",
          "description": "Maximum queue size for event processing"
        },
        "http_endpoint": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Http Endpoint",
          "description": "HTTP endpoint for event transport"
        },
        "http_headers": {
          "anyOf": [
            {
              "additionalProperties": {
                "type": "string"
              },
              "type": "object"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Http Headers",
          "description": "HTTP headers for event transport"
        },
        "http_timeout": {
          "default": 5.0,
          "title": "Http Timeout",
          "type": "number",
          "description": "HTTP timeout seconds for event transport"
        }
      },
      "title": "LoggerSettings",
      "type": "object"
    },
    "MCPRootSettings": {
      "additionalProperties": true,
      "description": "Represents a root directory configuration for an MCP server.",
      "properties": {
        "uri": {
          "title": "Uri",
          "type": "string",
          "description": "The URI identifying the root. Must start with file://"
        },
        "name": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Name",
          "description": "Optional name for the root."
        },
        "server_uri_alias": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Server Uri Alias",
          "description": "Optional URI alias for presentation to the server"
        }
      },
      "required": [
        "uri"
      ],
      "title": "MCPRootSettings",
      "type": "object"
    },
    "MCPServerAuthSettings": {
      "additionalProperties": true,
      "description": "Represents authentication configuration for a server.",
      "properties": {
        "api_key": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Api Key"
        }
      },
      "title": "MCPServerAuthSettings",
      "type": "object"
    },
    "MCPServerSettings": {
      "description": "Represents the configuration for an individual server.",
      "properties": {
        "name": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Name",
          "description": "The name of the server."
        },
        "description": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Description",
          "description": "The description of the server."
        },
        "transport": {
          "default": "stdio",
          "enum": [
            "stdio",
            "sse"
          ],
          "title": "Transport",
          "type": "string",
          "description": "The transport mechanism."
        },
        "command": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Command",
          "description": "The command to execute the server (e.g. npx)."
        },
        "args": {
          "anyOf": [
            {
              "items": {
                "type": "string"
              },
              "type": "array"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Args",
          "description": "The arguments for the server command."
        },
        "read_timeout_seconds": {
          "anyOf": [
            {
              "type": "integer"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Read Timeout Seconds",
          "description": "The timeout in seconds for the server connection."
        },
        "url": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Url",
          "description": "The URL for the server (e.g. for SSE transport)."
        },
        "auth": {
          "anyOf": [
            {
              "$ref": "#/$defs/MCPServerAuthSettings"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "The authentication configuration for the server."
        },
        "roots": {
          "anyOf": [
            {
              "items": {
                "$ref": "#/$defs/MCPRootSettings"
              },
              "type": "array"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Roots",
          "description": "Root directories this server has access to."
        },
        "env": {
          "anyOf": [
            {
              "additionalProperties": {
                "type": "string"
              },
              "type": "object"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Env",
          "description": "Environment variables to pass to the server process."
        }
      },
      "title": "MCPServerSettings",
      "type": "object"
    },
    "MCPSettings": {
      "additionalProperties": true,
      "description": "Configuration for all MCP servers.",
      "properties": {
        "servers": {
          "additionalProperties": {
            "$ref": "#/$defs/MCPServerSettings"
          },
          "default": {},
          "title": "Servers",
          "type": "object"
        }
      },
      "title": "MCPSettings",
      "type": "object"
    },
    "OpenAISettings": {
      "additionalProperties": true,
      "description": "Settings for using OpenAI models in the MCP Agent application.",
      "properties": {
        "api_key": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Api Key"
        },
        "reasoning_effort": {
          "default": "medium",
          "enum": [
            "low",
            "medium",
            "high"
          ],
          "title": "Reasoning Effort",
          "type": "string"
        },
        "base_url": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Base Url"
        }
      },
      "title": "OpenAISettings",
      "type": "object"
    },
    "AzureSettings": {
      "additionalProperties": true,
      "description": "Settings for using Azure models in the MCP Agent application.",
      "properties": {
        "api_key": {
          "anyOf": [
            {
              "type": "string"
            }
          ],
          "default": null,
          "title": "Api Key"
        },
        "endpoint": {
          "anyOf": [
            {
              "type": "string"
            }
          ],
          "default": null,
          "title": "Azure Endpoint"
        },
        "api_version": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "API Version"
        }
      },
      "required": [
        "api_key",
        "endpoint"
      ],
      "title": "AzureSettings",
      "type": "object"
    },
    "BedrockSettings": {
      "additionalProperties": true,
      "description": "Settings for using AWS Bedrock models in the MCP Agent application.",
      "properties": {
        "aws_region": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Region"
        },
        "aws_access_key_id": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Access Key Id"
        },
        "aws_secret_access_key": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Secret Access Key"
        },
        "aws_session_token": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Session Token"
        },
        "profile": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Profile"
        }
      },
      "title": "BedrockSettings",
      "type": "object"
    },
    "OpenTelemetrySettings": {
      "description": "OTEL settings for the MCP Agent application.",
      "properties": {
        "enabled": {
          "default": true,
          "title": "Enabled",
          "type": "boolean"
        },
        "service_name": {
          "default": "mcp-agent",
          "title": "Service Name",
          "type": "string"
        },
        "service_instance_id": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Service Instance Id"
        },
        "service_version": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Service Version"
        },
        "otlp_endpoint": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Otlp Endpoint",
          "description": "OTLP endpoint for OpenTelemetry tracing"
        },
        "console_debug": {
          "default": false,
          "title": "Console Debug",
          "type": "boolean",
          "description": "Log spans to console"
        },
        "sample_rate": {
          "default": 1.0,
          "title": "Sample Rate",
          "type": "number",
          "description": "Sample rate for tracing (1.0 = sample everything)"
        }
      },
      "title": "OpenTelemetrySettings",
      "type": "object"
    },
    "TemporalSettings": {
      "description": "Temporal settings for the MCP Agent application.",
      "properties": {
        "host": {
          "title": "Host",
          "type": "string"
        },
        "namespace": {
          "default": "default",
          "title": "Namespace",
          "type": "string"
        },
        "task_queue": {
          "title": "Task Queue",
          "type": "string"
        },
        "api_key": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Api Key"
        }
      },
      "required": [
        "host",
        "task_queue"
      ],
      "title": "TemporalSettings",
      "type": "object"
    },
    "UsageTelemetrySettings": {
      "description": "Settings for usage telemetry in the MCP Agent application.\nAnonymized usage metrics are sent to a telemetry server to help improve the product.",
      "properties": {
        "enabled": {
          "default": true,
          "title": "Enabled",
          "type": "boolean",
          "description": "Enable usage telemetry in the MCP Agent application."
        },
        "enable_detailed_telemetry": {
          "default": false,
          "title": "Enable Detailed Telemetry",
          "type": "boolean",
          "description": "If enabled, detailed telemetry data, including prompts and agents, will be sent to the telemetry server."
        }
      },
      "title": "UsageTelemetrySettings",
      "type": "object"
    }
  },
  "additionalProperties": true,
  "description": "Configuration schema for MCP Agent applications",
  "properties": {
    "mcp": {
      "anyOf": [
        {
          "$ref": "#/$defs/MCPSettings"
        },
        {
          "type": "null"
        }
      ],
      "default": {
        "servers": {}
      },
      "description": "MCP config, such as MCP servers"
    },
    "execution_engine": {
      "default": "asyncio",
      "enum": [
        "asyncio",
        "temporal"
      ],
      "title": "Execution Engine",
      "type": "string",
      "description": "Execution engine for the MCP Agent application"
    },
    "temporal": {
      "anyOf": [
        {
          "$ref": "#/$defs/TemporalSettings"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "description": "Settings for Temporal workflow orchestration"
    },
    "anthropic": {
      "anyOf": [
        {
          "$ref": "#/$defs/AnthropicSettings"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "description": "Settings for using Anthropic models in the MCP Agent application"
    },
    "cohere": {
      "anyOf": [
        {
          "$ref": "#/$defs/CohereSettings"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "description": "Settings for using Cohere models in the MCP Agent application"
    },
    "openai": {
      "anyOf": [
        {
          "$ref": "#/$defs/OpenAISettings"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "description": "Settings for using OpenAI models in the MCP Agent application"
    },
    "azure": {
      "anyOf": [
        {
          "$ref": "#/$defs/AzureSettings"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "description": "Settings for using Azure models in the MCP Agent application"
    },
    "bedrock": {
      "anyOf": [
        {
          "$ref": "#/$defs/BedrockSettings"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "description": "Settings for using Bedrock models in the MCP Agent application"
    },
    "otel": {
      "anyOf": [
        {
          "$ref": "#/$defs/OpenTelemetrySettings"
        },
        {
          "type": "null"
        }
      ],
      "default": {
        "enabled": true,
        "service_name": "mcp-agent",
        "service_instance_id": null,
        "service_version": null,
        "otlp_endpoint": null,
        "console_debug": false,
        "sample_rate": 1.0
      },
      "description": "OpenTelemetry logging settings for the MCP Agent application"
    },
    "logger": {
      "anyOf": [
        {
          "$ref": "#/$defs/LoggerSettings"
        },
        {
          "type": "null"
        }
      ],
      "default": {
        "type": "console",
        "transports": [],
        "level": "info",
        "progress_display": true,
        "path": "mcp-agent.jsonl",
        "path_settings": null,
        "batch_size": 100,
        "flush_interval": 2.0,
        "max_queue_size": 2048,
        "http_endpoint": null,
        "http_headers": null,
        "http_timeout": 5.0
      },
      "description": "Logger settings for the MCP Agent application"
    },
    "usage_telemetry": {
      "anyOf": [
        {
          "$ref": "#/$defs/UsageTelemetrySettings"
        },
        {
          "type": "null"
        }
      ],
      "default": {
        "enabled": true,
        "enable_detailed_telemetry": false
      },
      "description": "Usage tracking settings for the MCP Agent application"
    }
  },
  "title": "MCP Agent Configuration Schema",
  "type": "object",
  "$schema": "http://json-schema.org/draft-07/schema#"
}



================================================
FILE: tools/__init__.py
================================================
[Empty file]


================================================
FILE: tools/bocha_search_server.py
================================================
import os
import sys
import json

import httpx
from dotenv import load_dotenv
from mcp.server.fastmcp import FastMCP

load_dotenv()


# Initialize FastMCP server
server = FastMCP(
    "bocha-search-mcp",
    prompt="""
# Bocha Search MCP Server

Bocha is a Chinese search engine for AI, This server provides tools for searching the web using Bocha Search API.
It allows you to get enhanced search details from billions of web documents, including weather, news, wikis, healthcare, train tickets, images, and more.

## Available Tools

### 1. bocha_web_search
Search with Bocha Web Search and get enhanced search details from billions of web documents, including page titles, urls, summaries, site names, site icons, publication dates, image links, and more.

### 2. bocha_ai_search
Search with Bocha AI Search, recognizes the semantics of search terms and additionally returns structured modal cards with content from vertical domains.

## Output Format

All search results will be formatted as text with clear sections for each
result item, including:

- Bocha Web search: Title, URL, Description, Published date and Site name
- Bocha AI search: Title, URL, Description, Published date, Site name, and structured data card

If the API key is missing or invalid, appropriate error messages will be returned.
""",
)


@server.tool()
async def bocha_web_search(
    query: str, freshness: str = "noLimit", count: int = 10
) -> str:
    """Search with Bocha Web Search and get enhanced search details from billions of web documents,
    including page titles, urls, summaries, site names, site icons, publication dates, image links, and more.

    Args:
        query: Search query (required)
        freshness: The time range for the search results. (Available options YYYY-MM-DD, YYYY-MM-DD..YYYY-MM-DD, noLimit, oneYear, oneMonth, oneWeek, oneDay. Default is noLimit)
        count: Number of results (1-50, default 10)
    """
    # Get API key from environment
    boch_api_key = os.environ.get("BOCHA_API_KEY", "")

    if not boch_api_key:
        return (
            "Error: Bocha API key is not configured. Please set the "
            "BOCHA_API_KEY environment variable."
        )

    # Endpoint
    endpoint = "https://api.bochaai.com/v1/web-search?utm_source=bocha-mcp-local"

    try:
        payload = {
            "query": query,
            "summary": True,
            "freshness": freshness,
            "count": count,
        }

        headers = {
            "Authorization": f"Bearer {boch_api_key}",
            "Content-Type": "application/json",
        }

        async with httpx.AsyncClient() as client:
            response = await client.post(
                endpoint, headers=headers, json=payload, timeout=10.0
            )

            response.raise_for_status()
            resp = response.json()
            if "data" not in resp:
                return "Search error."

            data = resp["data"]

            if "webPages" not in data:
                return "No results found."

            results = []
            for result in data["webPages"]["value"]:
                results.append(
                    f"Title: {result['name']}\n"
                    f"URL: {result['url']}\n"
                    f"Description: {result['summary']}\n"
                    f"Published date: {result['datePublished']}\n"
                    f"Site name: {result['siteName']}"
                )

            return "\n\n".join(results)

    except httpx.HTTPStatusError as e:
        return f"Bocha Web Search API HTTP error occurred: {e.response.status_code} - {e.response.text}"
    except httpx.RequestError as e:
        return f"Error communicating with Bocha Web Search API: {str(e)}"
    except Exception as e:
        return f"Unexpected error: {str(e)}"


@server.tool()
async def bocha_ai_search(
    query: str, freshness: str = "noLimit", count: int = 10
) -> str:
    """Search with Bocha AI Search, recognizes the semantics of search terms
    and additionally returns structured modal cards with content from vertical domains.

    Args:
        query: Search query (required)
        freshness: The time range for the search results. (Available options noLimit, oneYear, oneMonth, oneWeek, oneDay. Default is noLimit)
        count: Number of results (1-50, default 10)
    """
    # Get API key from environment
    boch_api_key = os.environ.get("BOCHA_API_KEY", "")

    if not boch_api_key:
        return (
            "Error: Bocha API key is not configured. Please set the "
            "BOCHA_API_KEY environment variable."
        )

    # Endpoint
    endpoint = "https://api.bochaai.com/v1/ai-search?utm_source=bocha-mcp-local"

    try:
        payload = {
            "query": query,
            "freshness": freshness,
            "count": count,
            "answer": False,
            "stream": False,
        }

        headers = {
            "Authorization": f"Bearer {boch_api_key}",
            "Content-Type": "application/json",
        }

        async with httpx.AsyncClient() as client:
            response = await client.post(
                endpoint, headers=headers, json=payload, timeout=10.0
            )

            response.raise_for_status()
            response = response.json()
            results = []
            if "messages" in response:
                for message in response["messages"]:
                    content = {}
                    try:
                        content = json.loads(message["content"])
                    except (json.JSONDecodeError, TypeError):
                        content = {}

                    # 网页
                    if message["content_type"] == "webpage":
                        if "value" in content:
                            for item in content["value"]:
                                results.append(
                                    f"Title: {item['name']}\n"
                                    f"URL: {item['url']}\n"
                                    f"Description: {item['summary']}\n"
                                    f"Published date: {item['datePublished']}\n"
                                    f"Site name: {item['siteName']}"
                                )
                    elif (
                        message["content_type"] != "image"
                        and message["content"] != "{}"
                    ):
                        results.append(message["content"])

            if not results:
                return "No results found."

            return "\n\n".join(results)

    except httpx.HTTPStatusError as e:
        return f"Bocha AI Search API HTTP error occurred: {e.response.status_code} - {e.response.text}"
    except httpx.RequestError as e:
        return f"Error communicating with Bocha AI Search API: {str(e)}"
    except Exception as e:
        return f"Unexpected error: {str(e)}"


def main():
    """Initialize and run the MCP server."""

    # Check for required environment variables
    if "BOCHA_API_KEY" not in os.environ:
        print(
            "Error: BOCHA_API_KEY environment variable is required",
            file=sys.stderr,
        )
        print(
            "Get a Bocha API key from: " "https://open.bochaai.com",
            file=sys.stderr,
        )
        sys.exit(1)

    print("Starting Bocha Search MCP server...", file=sys.stderr)

    server.run(transport="stdio")


if __name__ == "__main__":
    main()



================================================
FILE: tools/code_reference_indexer.py
================================================
#!/usr/bin/env python3
"""
Code Reference Indexer MCP Tool - Unified Version

Specialized MCP tool for searching relevant index content in indexes folder
and formatting it for LLM code implementation reference.

Core Features:
1. **UNIFIED TOOL**: Combined search_code_references that handles directory setup, loading, and searching in one call
2. Match relevant reference code based on target file path and functionality requirements
3. Format output of relevant code examples, functions and concepts
4. Provide structured reference information for LLM use

Key Improvement:
- Single tool call that handles all steps internally
- Agent only needs to provide indexes_path and target_file
- No dependency on calling order or global state management
"""

import json
from pathlib import Path
from typing import Dict, List, Tuple
from dataclasses import dataclass
import logging

# Import MCP modules
from mcp.server.fastmcp import FastMCP

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Create FastMCP server instance
mcp = FastMCP("code-reference-indexer")


@dataclass
class CodeReference:
    """Code reference information structure"""

    file_path: str
    file_type: str
    main_functions: List[str]
    key_concepts: List[str]
    dependencies: List[str]
    summary: str
    lines_of_code: int
    repo_name: str
    confidence_score: float = 0.0


@dataclass
class RelationshipInfo:
    """Relationship information structure"""

    repo_file_path: str
    target_file_path: str
    relationship_type: str
    confidence_score: float
    helpful_aspects: List[str]
    potential_contributions: List[str]
    usage_suggestions: str


def load_index_files_from_directory(indexes_directory: str) -> Dict[str, Dict]:
    """Load all index files from specified directory"""
    indexes_path = Path(indexes_directory).resolve()

    if not indexes_path.exists():
        logger.warning(f"Indexes directory does not exist: {indexes_path}")
        return {}

    index_cache = {}

    for index_file in indexes_path.glob("*.json"):
        try:
            with open(index_file, "r", encoding="utf-8") as f:
                index_data = json.load(f)
                index_cache[index_file.stem] = index_data
                logger.info(f"Loaded index file: {index_file.name}")
        except Exception as e:
            logger.error(f"Failed to load index file {index_file.name}: {e}")

    logger.info(f"Loaded {len(index_cache)} index files from {indexes_path}")
    return index_cache


def extract_code_references(index_data: Dict) -> List[CodeReference]:
    """Extract code reference information from index data"""
    references = []

    repo_name = index_data.get("repo_name", "Unknown")
    file_summaries = index_data.get("file_summaries", [])

    for file_summary in file_summaries:
        reference = CodeReference(
            file_path=file_summary.get("file_path", ""),
            file_type=file_summary.get("file_type", ""),
            main_functions=file_summary.get("main_functions", []),
            key_concepts=file_summary.get("key_concepts", []),
            dependencies=file_summary.get("dependencies", []),
            summary=file_summary.get("summary", ""),
            lines_of_code=file_summary.get("lines_of_code", 0),
            repo_name=repo_name,
        )
        references.append(reference)

    return references


def extract_relationships(index_data: Dict) -> List[RelationshipInfo]:
    """Extract relationship information from index data"""
    relationships = []

    relationship_list = index_data.get("relationships", [])

    for rel in relationship_list:
        relationship = RelationshipInfo(
            repo_file_path=rel.get("repo_file_path", ""),
            target_file_path=rel.get("target_file_path", ""),
            relationship_type=rel.get("relationship_type", ""),
            confidence_score=rel.get("confidence_score", 0.0),
            helpful_aspects=rel.get("helpful_aspects", []),
            potential_contributions=rel.get("potential_contributions", []),
            usage_suggestions=rel.get("usage_suggestions", ""),
        )
        relationships.append(relationship)

    return relationships


def calculate_relevance_score(
    target_file: str, reference: CodeReference, keywords: List[str] = None
) -> float:
    """Calculate relevance score between reference code and target file"""
    score = 0.0

    # File name similarity
    target_name = Path(target_file).stem.lower()
    ref_name = Path(reference.file_path).stem.lower()

    if target_name in ref_name or ref_name in target_name:
        score += 0.3

    # File type matching
    target_extension = Path(target_file).suffix
    ref_extension = Path(reference.file_path).suffix

    if target_extension == ref_extension:
        score += 0.2

    # Keyword matching
    if keywords:
        keyword_matches = 0
        total_searchable_text = (
            " ".join(reference.key_concepts)
            + " "
            + " ".join(reference.main_functions)
            + " "
            + reference.summary
            + " "
            + reference.file_type
        ).lower()

        for keyword in keywords:
            if keyword.lower() in total_searchable_text:
                keyword_matches += 1

        if keywords:
            score += (keyword_matches / len(keywords)) * 0.5

    return min(score, 1.0)


def find_relevant_references_in_cache(
    target_file: str,
    index_cache: Dict[str, Dict],
    keywords: List[str] = None,
    max_results: int = 10,
) -> List[Tuple[CodeReference, float]]:
    """Find reference code relevant to target file from provided cache"""
    all_references = []

    # Collect reference information from all index files
    for repo_name, index_data in index_cache.items():
        references = extract_code_references(index_data)
        for ref in references:
            relevance_score = calculate_relevance_score(target_file, ref, keywords)
            if relevance_score > 0.1:  # Only keep results with certain relevance
                all_references.append((ref, relevance_score))

    # Sort by relevance score
    all_references.sort(key=lambda x: x[1], reverse=True)

    return all_references[:max_results]


def find_direct_relationships_in_cache(
    target_file: str, index_cache: Dict[str, Dict]
) -> List[RelationshipInfo]:
    """Find direct relationships with target file from provided cache"""
    relationships = []

    # Normalize target file path (remove common prefixes if exists)
    common_prefixes = ["src/", "core/", "lib/", "main/", "./"]
    normalized_target = target_file.strip("/")
    for prefix in common_prefixes:
        if normalized_target.startswith(prefix):
            normalized_target = normalized_target[len(prefix) :]
            break

    # Collect relationship information from all index files
    for repo_name, index_data in index_cache.items():
        repo_relationships = extract_relationships(index_data)
        for rel in repo_relationships:
            # Normalize target file path in relationship
            normalized_rel_target = rel.target_file_path.strip("/")
            for prefix in common_prefixes:
                if normalized_rel_target.startswith(prefix):
                    normalized_rel_target = normalized_rel_target[len(prefix) :]
                    break

            # Check target file path matching (support multiple matching methods)
            if (
                normalized_target == normalized_rel_target
                or normalized_target in normalized_rel_target
                or normalized_rel_target in normalized_target
                or target_file in rel.target_file_path
                or rel.target_file_path in target_file
            ):
                relationships.append(rel)

    # Sort by confidence score
    relationships.sort(key=lambda x: x.confidence_score, reverse=True)

    return relationships


def format_reference_output(
    target_file: str,
    relevant_refs: List[Tuple[CodeReference, float]],
    relationships: List[RelationshipInfo],
) -> str:
    """Format reference information output"""
    output_lines = []

    output_lines.append(f"# Code Reference Information - {target_file}")
    output_lines.append("=" * 80)
    output_lines.append("")

    # Direct relationship information
    if relationships:
        output_lines.append("## 🎯 Direct Relationships")
        output_lines.append("")

        for i, rel in enumerate(relationships[:5], 1):
            output_lines.append(f"### {i}. {rel.repo_file_path}")
            output_lines.append(f"**Relationship Type**: {rel.relationship_type}")
            output_lines.append(f"**Confidence Score**: {rel.confidence_score:.2f}")
            output_lines.append(
                f"**Helpful Aspects**: {', '.join(rel.helpful_aspects)}"
            )
            output_lines.append(
                f"**Potential Contributions**: {', '.join(rel.potential_contributions)}"
            )
            output_lines.append(f"**Usage Suggestions**: {rel.usage_suggestions}")
            output_lines.append("")

    # Relevant code references
    if relevant_refs:
        output_lines.append("## 📚 Relevant Code References")
        output_lines.append("")

        for i, (ref, score) in enumerate(relevant_refs[:8], 1):
            output_lines.append(f"### {i}. {ref.file_path} (Relevance: {score:.2f})")
            output_lines.append(f"**Repository**: {ref.repo_name}")
            output_lines.append(f"**File Type**: {ref.file_type}")
            output_lines.append(
                f"**Main Functions**: {', '.join(ref.main_functions[:5])}"
            )
            output_lines.append(f"**Key Concepts**: {', '.join(ref.key_concepts[:8])}")
            output_lines.append(f"**Dependencies**: {', '.join(ref.dependencies[:6])}")
            output_lines.append(f"**Lines of Code**: {ref.lines_of_code}")
            output_lines.append(f"**Summary**: {ref.summary[:300]}...")
            output_lines.append("")

    # Implementation suggestions
    output_lines.append("## 💡 Implementation Suggestions")
    output_lines.append("")

    if relevant_refs:
        # Collect all function names and concepts
        all_functions = set()
        all_concepts = set()
        all_dependencies = set()

        for ref, _ in relevant_refs[:5]:
            all_functions.update(ref.main_functions)
            all_concepts.update(ref.key_concepts)
            all_dependencies.update(ref.dependencies)

        output_lines.append("**Reference Function Name Patterns**:")
        for func in sorted(list(all_functions))[:10]:
            output_lines.append(f"- {func}")
        output_lines.append("")

        output_lines.append("**Important Concepts and Patterns**:")
        for concept in sorted(list(all_concepts))[:15]:
            output_lines.append(f"- {concept}")
        output_lines.append("")

        output_lines.append("**Potential Dependencies Needed**:")
        for dep in sorted(list(all_dependencies))[:10]:
            output_lines.append(f"- {dep}")
        output_lines.append("")

    output_lines.append("## 🚀 Next Actions")
    output_lines.append(
        "1. Analyze design patterns and architectural styles from the above reference code"
    )
    output_lines.append("2. Determine core functionalities and interfaces to implement")
    output_lines.append("3. Choose appropriate dependency libraries and tools")
    output_lines.append(
        "4. Design implementation solution consistent with existing code style"
    )
    output_lines.append("5. Start writing specific code implementation")

    return "\n".join(output_lines)


# ==================== MCP Tool Definitions ====================


@mcp.tool()
async def search_code_references(
    indexes_path: str, target_file: str, keywords: str = "", max_results: int = 10
) -> str:
    """
    **UNIFIED TOOL**: Search relevant reference code from index files for target file implementation.
    This tool combines directory setup, index loading, and searching in a single call.

    Args:
        indexes_path: Path to the indexes directory containing JSON index files
        target_file: Target file path (file to be implemented)
        keywords: Search keywords, comma-separated
        max_results: Maximum number of results to return

    Returns:
        Formatted reference code information JSON string
    """
    try:
        # Step 1: Load index files from specified directory
        logger.info(f"Loading index files from: {indexes_path}")
        index_cache = load_index_files_from_directory(indexes_path)

        if not index_cache:
            result = {
                "status": "error",
                "message": f"No index files found or failed to load from: {indexes_path}",
                "target_file": target_file,
                "indexes_path": indexes_path,
            }
            return json.dumps(result, ensure_ascii=False, indent=2)

        # Step 2: Parse keywords
        keyword_list = (
            [kw.strip() for kw in keywords.split(",") if kw.strip()] if keywords else []
        )

        # Step 3: Find relevant reference code
        relevant_refs = find_relevant_references_in_cache(
            target_file, index_cache, keyword_list, max_results
        )

        # Step 4: Find direct relationships
        relationships = find_direct_relationships_in_cache(target_file, index_cache)

        # Step 5: Format output
        formatted_output = format_reference_output(
            target_file, relevant_refs, relationships
        )

        result = {
            "status": "success",
            "target_file": target_file,
            "indexes_path": indexes_path,
            "keywords_used": keyword_list,
            "total_references_found": len(relevant_refs),
            "total_relationships_found": len(relationships),
            "formatted_content": formatted_output,
            "indexes_loaded": list(index_cache.keys()),
            "total_indexes_loaded": len(index_cache),
        }

        logger.info(
            f"Successfully found {len(relevant_refs)} references and {len(relationships)} relationships for {target_file}"
        )
        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        logger.error(f"Error in search_code_references: {str(e)}")
        result = {
            "status": "error",
            "message": f"Failed to search reference code: {str(e)}",
            "target_file": target_file,
            "indexes_path": indexes_path,
        }
        return json.dumps(result, ensure_ascii=False, indent=2)


@mcp.tool()
async def get_indexes_overview(indexes_path: str) -> str:
    """
    Get overview of all available reference code index information from specified directory

    Args:
        indexes_path: Path to the indexes directory containing JSON index files

    Returns:
        Overview information of all available reference code JSON string
    """
    try:
        # Load index files from specified directory
        index_cache = load_index_files_from_directory(indexes_path)

        if not index_cache:
            result = {
                "status": "error",
                "message": f"No index files found in: {indexes_path}",
                "indexes_path": indexes_path,
            }
            return json.dumps(result, ensure_ascii=False, indent=2)

        overview = {"total_repos": len(index_cache), "repositories": {}}

        for repo_name, index_data in index_cache.items():
            repo_info = {
                "repo_name": index_data.get("repo_name", repo_name),
                "total_files": index_data.get("total_files", 0),
                "file_types": [],
                "main_concepts": [],
                "total_relationships": len(index_data.get("relationships", [])),
            }

            # Collect file types and concepts
            file_summaries = index_data.get("file_summaries", [])
            file_types = set()
            concepts = set()

            for file_summary in file_summaries:
                file_types.add(file_summary.get("file_type", "Unknown"))
                concepts.update(file_summary.get("key_concepts", []))

            repo_info["file_types"] = sorted(list(file_types))
            repo_info["main_concepts"] = sorted(list(concepts))[
                :20
            ]  # Limit concept count

            overview["repositories"][repo_name] = repo_info

        result = {
            "status": "success",
            "overview": overview,
            "indexes_directory": str(Path(indexes_path).resolve()),
            "total_indexes_loaded": len(index_cache),
        }

        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        result = {
            "status": "error",
            "message": f"Failed to get indexes overview: {str(e)}",
            "indexes_path": indexes_path,
        }
        return json.dumps(result, ensure_ascii=False, indent=2)


def main():
    """Main function"""
    logger.info("Starting unified Code Reference Indexer MCP server")
    logger.info("Available tools:")
    logger.info(
        "1. search_code_references(indexes_path, target_file, keywords, max_results) - UNIFIED TOOL"
    )
    logger.info(
        "2. get_indexes_overview(indexes_path) - Get overview of available indexes"
    )

    # Run MCP server
    mcp.run()


if __name__ == "__main__":
    main()



================================================
FILE: tools/command_executor.py
================================================
#!/usr/bin/env python3
"""
Command Executor MCP Tool / 命令执行器 MCP 工具

专门负责执行LLM生成的shell命令来创建文件树结构
Specialized in executing LLM-generated shell commands to create file tree structures
"""

import subprocess
from pathlib import Path
from typing import List, Dict
from mcp.server.models import InitializationOptions
import mcp.types as types
from mcp.server import NotificationOptions, Server
import mcp.server.stdio

# 创建MCP服务器实例 / Create MCP server instance
app = Server("command-executor")


@app.list_tools()
async def handle_list_tools() -> list[types.Tool]:
    """
    列出可用工具 / List available tools
    """
    return [
        types.Tool(
            name="execute_commands",
            description="""
            执行shell命令列表来创建文件树结构
            Execute shell command list to create file tree structure

            Args:
                commands: 要执行的shell命令列表（每行一个命令）
                working_directory: 执行命令的工作目录

            Returns:
                命令执行结果和详细报告
            """,
            inputSchema={
                "type": "object",
                "properties": {
                    "commands": {
                        "type": "string",
                        "title": "Commands",
                        "description": "要执行的shell命令列表，每行一个命令",
                    },
                    "working_directory": {
                        "type": "string",
                        "title": "Working Directory",
                        "description": "执行命令的工作目录",
                    },
                },
                "required": ["commands", "working_directory"],
            },
        ),
        types.Tool(
            name="execute_single_command",
            description="""
            执行单个shell命令
            Execute single shell command

            Args:
                command: 要执行的单个命令
                working_directory: 执行命令的工作目录

            Returns:
                命令执行结果
            """,
            inputSchema={
                "type": "object",
                "properties": {
                    "command": {
                        "type": "string",
                        "title": "Command",
                        "description": "要执行的单个shell命令",
                    },
                    "working_directory": {
                        "type": "string",
                        "title": "Working Directory",
                        "description": "执行命令的工作目录",
                    },
                },
                "required": ["command", "working_directory"],
            },
        ),
    ]


@app.call_tool()
async def handle_call_tool(name: str, arguments: dict) -> list[types.TextContent]:
    """
    处理工具调用 / Handle tool calls
    """
    try:
        if name == "execute_commands":
            return await execute_command_batch(
                arguments.get("commands", ""), arguments.get("working_directory", ".")
            )
        elif name == "execute_single_command":
            return await execute_single_command(
                arguments.get("command", ""), arguments.get("working_directory", ".")
            )
        else:
            raise ValueError(f"未知工具 / Unknown tool: {name}")

    except Exception as e:
        return [
            types.TextContent(
                type="text",
                text=f"工具执行错误 / Error executing tool {name}: {str(e)}",
            )
        ]


async def execute_command_batch(
    commands: str, working_directory: str
) -> list[types.TextContent]:
    """
    执行多个shell命令 / Execute multiple shell commands

    Args:
        commands: 命令列表，每行一个命令 / Command list, one command per line
        working_directory: 工作目录 / Working directory

    Returns:
        执行结果 / Execution results
    """
    try:
        # 确保工作目录存在 / Ensure working directory exists
        Path(working_directory).mkdir(parents=True, exist_ok=True)

        # 分割命令行 / Split command lines
        command_lines = [
            cmd.strip() for cmd in commands.strip().split("\n") if cmd.strip()
        ]

        if not command_lines:
            return [
                types.TextContent(
                    type="text", text="没有提供有效命令 / No valid commands provided"
                )
            ]

        results = []
        stats = {"successful": 0, "failed": 0, "timeout": 0}

        for i, command in enumerate(command_lines, 1):
            try:
                # 执行命令 / Execute command
                result = subprocess.run(
                    command,
                    shell=True,
                    cwd=working_directory,
                    capture_output=True,
                    text=True,
                    timeout=30,  # 30秒超时
                )

                if result.returncode == 0:
                    results.append(f"✅ Command {i}: {command}")
                    if result.stdout.strip():
                        results.append(f"   输出 / Output: {result.stdout.strip()}")
                    stats["successful"] += 1
                else:
                    results.append(f"❌ Command {i}: {command}")
                    if result.stderr.strip():
                        results.append(f"   错误 / Error: {result.stderr.strip()}")
                    stats["failed"] += 1

            except subprocess.TimeoutExpired:
                results.append(f"⏱️ Command {i} 超时 / timeout: {command}")
                stats["timeout"] += 1
            except Exception as e:
                results.append(f"💥 Command {i} 异常 / exception: {command} - {str(e)}")
                stats["failed"] += 1

        # 生成执行报告 / Generate execution report
        summary = generate_execution_summary(working_directory, command_lines, stats)
        final_result = summary + "\n" + "\n".join(results)

        return [types.TextContent(type="text", text=final_result)]

    except Exception as e:
        return [
            types.TextContent(
                type="text",
                text=f"批量命令执行失败 / Failed to execute command batch: {str(e)}",
            )
        ]


async def execute_single_command(
    command: str, working_directory: str
) -> list[types.TextContent]:
    """
    执行单个shell命令 / Execute single shell command

    Args:
        command: 要执行的命令 / Command to execute
        working_directory: 工作目录 / Working directory

    Returns:
        执行结果 / Execution result
    """
    try:
        # 确保工作目录存在 / Ensure working directory exists
        Path(working_directory).mkdir(parents=True, exist_ok=True)

        # 执行命令 / Execute command
        result = subprocess.run(
            command,
            shell=True,
            cwd=working_directory,
            capture_output=True,
            text=True,
            timeout=30,
        )

        # 格式化输出 / Format output
        output = format_single_command_result(command, working_directory, result)

        return [types.TextContent(type="text", text=output)]

    except subprocess.TimeoutExpired:
        return [
            types.TextContent(
                type="text", text=f"⏱️ 命令超时 / Command timeout: {command}"
            )
        ]
    except Exception as e:
        return [
            types.TextContent(
                type="text", text=f"💥 命令执行错误 / Command execution error: {str(e)}"
            )
        ]


def generate_execution_summary(
    working_directory: str, command_lines: List[str], stats: Dict[str, int]
) -> str:
    """
    生成执行总结 / Generate execution summary

    Args:
        working_directory: 工作目录 / Working directory
        command_lines: 命令列表 / Command list
        stats: 统计信息 / Statistics

    Returns:
        格式化的总结 / Formatted summary
    """
    return f"""
命令执行总结 / Command Execution Summary:
{'='*50}
工作目录 / Working Directory: {working_directory}
总命令数 / Total Commands: {len(command_lines)}
成功 / Successful: {stats['successful']}
失败 / Failed: {stats['failed']}
超时 / Timeout: {stats['timeout']}

详细结果 / Detailed Results:
{'-'*50}"""


def format_single_command_result(
    command: str, working_directory: str, result: subprocess.CompletedProcess
) -> str:
    """
    格式化单命令执行结果 / Format single command execution result

    Args:
        command: 执行的命令 / Executed command
        working_directory: 工作目录 / Working directory
        result: 执行结果 / Execution result

    Returns:
        格式化的结果 / Formatted result
    """
    output = f"""
单命令执行 / Single Command Execution:
{'='*40}
工作目录 / Working Directory: {working_directory}
命令 / Command: {command}
返回码 / Return Code: {result.returncode}

"""

    if result.returncode == 0:
        output += "✅ 状态 / Status: SUCCESS / 成功\n"
        if result.stdout.strip():
            output += f"输出 / Output:\n{result.stdout.strip()}\n"
    else:
        output += "❌ 状态 / Status: FAILED / 失败\n"
        if result.stderr.strip():
            output += f"错误 / Error:\n{result.stderr.strip()}\n"

    return output


async def main():
    """
    运行MCP服务器 / Run MCP server
    """
    # 通过stdio运行服务器 / Run server via stdio
    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
        await app.run(
            read_stream,
            write_stream,
            InitializationOptions(
                server_name="command-executor",
                server_version="1.0.0",
                capabilities=app.get_capabilities(
                    notification_options=NotificationOptions(),
                    experimental_capabilities={},
                ),
            ),
        )


if __name__ == "__main__":
    import asyncio

    asyncio.run(main())



================================================
FILE: tools/git_command.py
================================================
#!/usr/bin/env python3
"""
GitHub Repository Downloader MCP Tool using FastMCP
"""

import asyncio
import os
import re
from typing import Dict, List, Optional
from pathlib import Path

from mcp.server import FastMCP

# 创建 FastMCP 实例
mcp = FastMCP("github-downloader")


class GitHubURLExtractor:
    """提取GitHub URL的工具类"""

    @staticmethod
    def extract_github_urls(text: str) -> List[str]:
        """从文本中提取GitHub URLs"""
        patterns = [
            # 标准HTTPS URL
            r"https?://github\.com/[\w\-\.]+/[\w\-\.]+(?:\.git)?",
            # SSH URL
            r"git@github\.com:[\w\-\.]+/[\w\-\.]+(?:\.git)?",
            # 短格式 owner/repo - 更严格的匹配
            r"(?<!\S)(?<!/)(?<!\.)([\w\-\.]+/[\w\-\.]+)(?!/)(?!\S)",
        ]

        urls = []
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                # 处理短格式
                if isinstance(match, tuple):
                    match = match[0]

                # 清理URL
                if match.startswith("git@"):
                    url = match.replace("git@github.com:", "https://github.com/")
                elif match.startswith("http"):
                    url = match
                else:
                    # 处理短格式 (owner/repo) - 添加更多验证
                    if "/" in match and not any(
                        x in match for x in ["./", "../", "deepcode_lab", "tools"]
                    ):
                        parts = match.split("/")
                        if (
                            len(parts) == 2
                            and all(
                                part.replace("-", "").replace("_", "").isalnum()
                                for part in parts
                            )
                            and not any(part.startswith(".") for part in parts)
                        ):
                            url = f"https://github.com/{match}"
                        else:
                            continue
                    else:
                        continue

                # 规范化 URL
                url = url.rstrip(".git")
                url = url.rstrip("/")

                # 修复重复的 github.com
                if "github.com/github.com/" in url:
                    url = url.replace("github.com/github.com/", "github.com/")

                urls.append(url)

        return list(set(urls))  # 去重

    @staticmethod
    def extract_target_path(text: str) -> Optional[str]:
        """从文本中提取目标路径"""
        # 路径指示词模式
        patterns = [
            r'(?:to|into|in|at)\s+(?:folder|directory|path)?\s*["\']?([^\s"\']+)["\']?',
            r'(?:save|download|clone)\s+(?:to|into|at)\s+["\']?([^\s"\']+)["\']?',
            # 中文支持
            r'(?:到|在|保存到|下载到|克隆到)\s*["\']?([^\s"\']+)["\']?',
        ]

        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                path = match.group(1).strip("。，,.")
                # 过滤掉通用词
                if path and path.lower() not in [
                    "here",
                    "there",
                    "current",
                    "local",
                    "这里",
                    "当前",
                    "本地",
                ]:
                    return path

        return None

    @staticmethod
    def infer_repo_name(url: str) -> str:
        """从URL推断仓库名称"""
        url = url.rstrip(".git")
        if "github.com" in url:
            parts = url.split("/")
            if len(parts) >= 2:
                return parts[-1]
        return "repository"


async def check_git_installed() -> bool:
    """检查Git是否安装"""
    try:
        proc = await asyncio.create_subprocess_exec(
            "git",
            "--version",
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        await proc.wait()
        return proc.returncode == 0
    except Exception:
        return False


async def clone_repository(repo_url: str, target_path: str) -> Dict[str, any]:
    """执行git clone命令"""
    try:
        proc = await asyncio.create_subprocess_exec(
            "git",
            "clone",
            repo_url,
            target_path,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )

        stdout, stderr = await proc.communicate()

        return {
            "success": proc.returncode == 0,
            "stdout": stdout.decode("utf-8", errors="replace"),
            "stderr": stderr.decode("utf-8", errors="replace"),
            "returncode": proc.returncode,
        }
    except Exception as e:
        return {"success": False, "error": str(e)}


@mcp.tool()
async def download_github_repo(instruction: str) -> str:
    """
    Download GitHub repositories from natural language instructions.

    Args:
        instruction: Natural language text containing GitHub URLs and optional target paths

    Returns:
        Status message about the download operation

    Examples:
        - "Download https://github.com/openai/gpt-3"
        - "Clone microsoft/vscode to my-projects folder"
        - "Get https://github.com/facebook/react"
    """
    # 检查Git是否安装
    if not await check_git_installed():
        return "❌ Error: Git is not installed or not in system PATH"

    extractor = GitHubURLExtractor()

    # 提取GitHub URLs
    urls = extractor.extract_github_urls(instruction)
    if not urls:
        return "❌ No GitHub URLs found in the instruction"

    # 提取目标路径
    target_path = extractor.extract_target_path(instruction)

    # 下载仓库
    results = []
    for url in urls:
        try:
            # 准备目标路径
            if target_path:
                # 判断是否为绝对路径
                if os.path.isabs(target_path):
                    # 如果是绝对路径，直接使用
                    final_path = target_path
                    # 如果目标路径是目录，添加仓库名
                    if os.path.basename(target_path) == "" or target_path.endswith("/"):
                        final_path = os.path.join(
                            target_path, extractor.infer_repo_name(url)
                        )
                else:
                    # 如果是相对路径，保持相对路径
                    final_path = target_path
                    # 如果目标路径是目录，添加仓库名
                    if os.path.basename(target_path) == "" or target_path.endswith("/"):
                        final_path = os.path.join(
                            target_path, extractor.infer_repo_name(url)
                        )
            else:
                final_path = extractor.infer_repo_name(url)

            # 如果是相对路径，确保使用相对路径格式
            if not os.path.isabs(final_path):
                final_path = os.path.normpath(final_path)
                if final_path.startswith("/"):
                    final_path = final_path.lstrip("/")

            # 确保父目录存在
            parent_dir = os.path.dirname(final_path)
            if parent_dir:
                os.makedirs(parent_dir, exist_ok=True)

            # 检查目标路径是否已存在
            if os.path.exists(final_path):
                results.append(
                    f"❌ Failed to download {url}: Target path already exists: {final_path}"
                )
                continue

            # 执行克隆
            result = await clone_repository(url, final_path)

            if result["success"]:
                msg = f"✅ Successfully downloaded: {url}\n"
                msg += f"   Location: {final_path}"
                if result.get("stdout"):
                    msg += f"\n   {result['stdout'].strip()}"
            else:
                msg = f"❌ Failed to download: {url}\n"
                msg += f"   Error: {result.get('error', result.get('stderr', 'Unknown error'))}"

        except Exception as e:
            msg = f"❌ Failed to download: {url}\n"
            msg += f"   Error: {str(e)}"

        results.append(msg)

    return "\n\n".join(results)


@mcp.tool()
async def parse_github_urls(text: str) -> str:
    """
    Extract GitHub URLs and target paths from text.

    Args:
        text: Text containing GitHub URLs

    Returns:
        Parsed GitHub URLs and target path information
    """
    extractor = GitHubURLExtractor()

    urls = extractor.extract_github_urls(text)
    target_path = extractor.extract_target_path(text)

    content = "📝 Parsed information:\n\n"

    if urls:
        content += "GitHub URLs found:\n"
        for url in urls:
            content += f"  • {url}\n"
    else:
        content += "No GitHub URLs found\n"

    if target_path:
        content += f"\nTarget path: {target_path}"
    else:
        content += "\nTarget path: Not specified (will use repository name)"

    return content


@mcp.tool()
async def git_clone(
    repo_url: str, target_path: Optional[str] = None, branch: Optional[str] = None
) -> str:
    """
    Clone a specific GitHub repository.

    Args:
        repo_url: GitHub repository URL
        target_path: Optional target directory path
        branch: Optional branch name to clone

    Returns:
        Status message about the clone operation
    """
    # 检查Git是否安装
    if not await check_git_installed():
        return "❌ Error: Git is not installed or not in system PATH"

    # 准备目标路径
    if not target_path:
        extractor = GitHubURLExtractor()
        target_path = extractor.infer_repo_name(repo_url)

    # 转换为绝对路径
    if not os.path.isabs(target_path):
        target_path = str(Path.cwd() / target_path)

    # 检查目标路径
    if os.path.exists(target_path):
        return f"❌ Error: Target path already exists: {target_path}"

    # 构建命令
    cmd = ["git", "clone"]
    if branch:
        cmd.extend(["-b", branch])
    cmd.extend([repo_url, target_path])

    # 执行克隆
    try:
        proc = await asyncio.create_subprocess_exec(
            *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
        )

        stdout, stderr = await proc.communicate()

        if proc.returncode == 0:
            result = "✅ Successfully cloned repository\n"
            result += f"Repository: {repo_url}\n"
            result += f"Location: {target_path}"
            if branch:
                result += f"\nBranch: {branch}"
            return result
        else:
            return f"❌ Clone failed\nError: {stderr.decode('utf-8', errors='replace')}"

    except Exception as e:
        return f"❌ Clone failed\nError: {str(e)}"


# 主程序入口
if __name__ == "__main__":
    print("🚀 GitHub Repository Downloader MCP Tool")
    print("📝 Starting server with FastMCP...")
    print("\nAvailable tools:")
    print("  • download_github_repo - Download repos from natural language")
    print("  • parse_github_urls - Extract GitHub URLs from text")
    print("  • git_clone - Clone a specific repository")
    print("")

    # 运行服务器
    mcp.run()



================================================
FILE: tools/indexer_config.yaml
================================================
# Code Indexer Configuration File
# Configure various aspects of the code indexing process

# Paths Configuration
paths:
  code_base_path: "D:/Documents/GitHub/Code-Agent/examples/input/paper1/code_base"
  output_dir: "D:/Documents/GitHub/Code-Agent/examples/input/paper1/indexes"

# File Analysis Settings
file_analysis:
  # Supported file extensions for analysis
  supported_extensions:
    - ".py"      # Python
    - ".js"      # JavaScript
    - ".ts"      # TypeScript
    - ".java"    # Java
    - ".cpp"     # C++
    - ".c"       # C
    - ".h"       # C Header
    - ".hpp"     # C++ Header
    - ".cs"      # C#
    - ".php"     # PHP
    - ".rb"      # Ruby
    - ".go"      # Go
    - ".rs"      # Rust
    - ".scala"   # Scala
    - ".kt"      # Kotlin
    - ".swift"   # Swift
    - ".r"       # R
    - ".sql"     # SQL
    - ".sh"      # Shell Script
    - ".bat"     # Batch File
    - ".ps1"     # PowerShell
    - ".yaml"    # YAML
    - ".yml"     # YAML
    - ".json"    # JSON
    - ".xml"     # XML
    - ".toml"    # TOML

  # Directories to skip during traversal
  skip_directories:
    - "__pycache__"
    - "node_modules"
    - "target"
    - "build"
    - "dist"
    - "venv"
    - "env"
    - ".git"
    - ".svn"
    - ".hg"
    - "coverage"
    - ".pytest_cache"
    - ".mypy_cache"

  # Maximum file size to analyze (in bytes)
  max_file_size: 1048576  # 1MB

  # Maximum content length to send to LLM (in characters)
  max_content_length: 3000

# LLM Configuration
llm:
  # Model selection: "anthropic" or "openai"
  model_provider: "openai"

  # Request parameters
  max_tokens: 4000
  temperature: 0.3

  # System prompt for analysis
  system_prompt: "You are a code analysis expert. Provide precise, structured analysis of code relationships and similarities."

  # Rate limiting (seconds between requests)
  request_delay: 0.1

  # Retry configuration
  max_retries: 3
  retry_delay: 1.0

# Relationship Analysis Settings
relationships:
  # Minimum confidence score to include a relationship
  min_confidence_score: 0.3

  # High confidence threshold for reporting
  high_confidence_threshold: 0.7

  # Relationship types and their priorities
  relationship_types:
    direct_match: 1.0      # Direct implementation match
    partial_match: 0.8     # Partial functionality match
    reference: 0.6         # Reference or utility function
    utility: 0.4           # General utility or helper

# Output Configuration
output:
  # JSON formatting options
  json_indent: 2
  ensure_ascii: false

  # Generate additional report files
  generate_summary: true
  generate_statistics: true

  # Include metadata in output
  include_metadata: true

  # File naming pattern (use {repo_name} placeholder)
  index_filename_pattern: "{repo_name}_index.json"
  summary_filename: "indexing_summary.json"
  stats_filename: "indexing_statistics.json"

# Logging Configuration
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  log_to_file: true
  log_file: "indexer.log"
  log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# Performance Settings
performance:
  # Enable concurrent processing of files within a repository
  enable_concurrent_analysis: true
  max_concurrent_files: 5

  # Memory optimization
  enable_content_caching: false
  max_cache_size: 100

# Debug and Development Settings
debug:
  # Save raw LLM responses for debugging
  save_raw_responses: false
  raw_responses_dir: "debug_responses"

  # Verbose output during processing
  verbose_output: false

  # Skip LLM calls for testing (uses mock responses)
  mock_llm_responses: false



================================================
FILE: tools/pdf_converter.py
================================================
#!/usr/bin/env python3
"""
PDF Converter Utility

This module provides functionality for converting various document formats to PDF,
including Office documents (.doc, .docx, .ppt, .pptx, .xls, .xlsx) and text files (.txt, .md).

Requirements:
- LibreOffice for Office document conversion
- ReportLab for text-to-PDF conversion
"""

from __future__ import annotations

import argparse
import logging
import subprocess
import tempfile
import shutil
import platform
from pathlib import Path
from typing import Union, Optional, Dict, Any


class PDFConverter:
    """
    PDF conversion utility class.

    Provides methods to convert Office documents and text files to PDF format.
    """

    # Define supported file formats
    OFFICE_FORMATS = {".doc", ".docx", ".ppt", ".pptx", ".xls", ".xlsx"}
    TEXT_FORMATS = {".txt", ".md"}

    # Class-level logger
    logger = logging.getLogger(__name__)

    def __init__(self) -> None:
        """Initialize the PDF converter."""
        pass

    @staticmethod
    def convert_office_to_pdf(
        doc_path: Union[str, Path], output_dir: Optional[str] = None
    ) -> Path:
        """
        Convert Office document (.doc, .docx, .ppt, .pptx, .xls, .xlsx) to PDF.
        Requires LibreOffice to be installed.

        Args:
            doc_path: Path to the Office document file
            output_dir: Output directory for the PDF file

        Returns:
            Path to the generated PDF file
        """
        try:
            # Convert to Path object for easier handling
            doc_path = Path(doc_path)
            if not doc_path.exists():
                raise FileNotFoundError(f"Office document does not exist: {doc_path}")

            name_without_suff = doc_path.stem

            # Prepare output directory
            if output_dir:
                base_output_dir = Path(output_dir)
            else:
                base_output_dir = doc_path.parent / "pdf_output"

            base_output_dir.mkdir(parents=True, exist_ok=True)

            # Check if LibreOffice is available
            libreoffice_available = False
            working_libreoffice_cmd: Optional[str] = None

            # Prepare subprocess parameters to hide console window on Windows
            subprocess_kwargs: Dict[str, Any] = {
                "capture_output": True,
                "check": True,
                "timeout": 10,
                "encoding": "utf-8",
                "errors": "ignore",
            }

            # Hide console window on Windows
            if platform.system() == "Windows":
                subprocess_kwargs["creationflags"] = (
                    0x08000000  # subprocess.CREATE_NO_WINDOW
                )

            try:
                result = subprocess.run(
                    ["libreoffice", "--version"], **subprocess_kwargs
                )
                libreoffice_available = True
                working_libreoffice_cmd = "libreoffice"
                logging.info(f"LibreOffice detected: {result.stdout.strip()}")  # type: ignore
            except (
                subprocess.CalledProcessError,
                FileNotFoundError,
                subprocess.TimeoutExpired,
            ):
                pass

            # Try alternative commands for LibreOffice
            if not libreoffice_available:
                for cmd in ["soffice", "libreoffice"]:
                    try:
                        result = subprocess.run([cmd, "--version"], **subprocess_kwargs)
                        libreoffice_available = True
                        working_libreoffice_cmd = cmd
                        logging.info(
                            f"LibreOffice detected with command '{cmd}': {result.stdout.strip()}"  # type: ignore
                        )
                        break
                    except (
                        subprocess.CalledProcessError,
                        FileNotFoundError,
                        subprocess.TimeoutExpired,
                    ):
                        continue

            if not libreoffice_available:
                raise RuntimeError(
                    "LibreOffice is required for Office document conversion but was not found.\n"
                    "Please install LibreOffice:\n"
                    "- Windows: Download from https://www.libreoffice.org/download/download/\n"
                    "- macOS: brew install --cask libreoffice\n"
                    "- Ubuntu/Debian: sudo apt-get install libreoffice\n"
                    "- CentOS/RHEL: sudo yum install libreoffice\n"
                    "Alternatively, convert the document to PDF manually."
                )

            # Create temporary directory for PDF conversion
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_path = Path(temp_dir)

                # Convert to PDF using LibreOffice
                logging.info(f"Converting {doc_path.name} to PDF using LibreOffice...")

                # Use the working LibreOffice command first, then try alternatives if it fails
                commands_to_try = [working_libreoffice_cmd]
                if working_libreoffice_cmd == "libreoffice":
                    commands_to_try.append("soffice")
                else:
                    commands_to_try.append("libreoffice")

                conversion_successful = False
                for cmd in commands_to_try:
                    if cmd is None:
                        continue
                    try:
                        convert_cmd = [
                            cmd,
                            "--headless",
                            "--convert-to",
                            "pdf",
                            "--outdir",
                            str(temp_path),
                            str(doc_path),
                        ]

                        # Prepare conversion subprocess parameters
                        convert_subprocess_kwargs: Dict[str, Any] = {
                            "capture_output": True,
                            "text": True,
                            "timeout": 60,  # 60 second timeout
                            "encoding": "utf-8",
                            "errors": "ignore",
                        }

                        # Hide console window on Windows
                        if platform.system() == "Windows":
                            convert_subprocess_kwargs["creationflags"] = (
                                0x08000000  # subprocess.CREATE_NO_WINDOW
                            )

                        result = subprocess.run(
                            convert_cmd, **convert_subprocess_kwargs
                        )

                        if result.returncode == 0:  # type: ignore
                            conversion_successful = True
                            logging.info(
                                f"Successfully converted {doc_path.name} to PDF"
                            )
                            break
                        else:
                            logging.warning(
                                f"LibreOffice command '{cmd}' failed: {result.stderr}"  # type: ignore
                            )
                    except subprocess.TimeoutExpired:
                        logging.warning(f"LibreOffice command '{cmd}' timed out")
                    except Exception as e:
                        logging.error(
                            f"LibreOffice command '{cmd}' failed with exception: {e}"
                        )

                if not conversion_successful:
                    raise RuntimeError(
                        f"LibreOffice conversion failed for {doc_path.name}. "
                        f"Please check if the file is corrupted or try converting manually."
                    )

                # Find the generated PDF
                pdf_files = list(temp_path.glob("*.pdf"))
                if not pdf_files:
                    raise RuntimeError(
                        f"PDF conversion failed for {doc_path.name} - no PDF file generated. "
                        f"Please check LibreOffice installation or try manual conversion."
                    )

                pdf_path = pdf_files[0]
                logging.info(
                    f"Generated PDF: {pdf_path.name} ({pdf_path.stat().st_size} bytes)"
                )

                # Validate the generated PDF
                if pdf_path.stat().st_size < 100:  # Very small file, likely empty
                    raise RuntimeError(
                        "Generated PDF appears to be empty or corrupted. "
                        "Original file may have issues or LibreOffice conversion failed."
                    )

                # Copy PDF to final output directory
                final_pdf_path = base_output_dir / f"{name_without_suff}.pdf"
                shutil.copy2(pdf_path, final_pdf_path)

                return final_pdf_path

        except Exception as e:
            logging.error(f"Error in convert_office_to_pdf: {str(e)}")
            raise

    @staticmethod
    def convert_text_to_pdf(
        text_path: Union[str, Path], output_dir: Optional[str] = None
    ) -> Path:
        """
        Convert text file (.txt, .md) to PDF using ReportLab with full markdown support.

        Args:
            text_path: Path to the text file
            output_dir: Output directory for the PDF file

        Returns:
            Path to the generated PDF file
        """
        try:
            text_path = Path(text_path)
            if not text_path.exists():
                raise FileNotFoundError(f"Text file does not exist: {text_path}")

            # Supported text formats
            supported_text_formats = {".txt", ".md"}
            if text_path.suffix.lower() not in supported_text_formats:
                raise ValueError(f"Unsupported text format: {text_path.suffix}")

            # Read the text content
            try:
                with open(text_path, "r", encoding="utf-8") as f:
                    text_content = f.read()
            except UnicodeDecodeError:
                # Try with different encodings
                for encoding in ["gbk", "latin-1", "cp1252"]:
                    try:
                        with open(text_path, "r", encoding=encoding) as f:
                            text_content = f.read()
                        logging.info(f"Successfully read file with {encoding} encoding")
                        break
                    except UnicodeDecodeError:
                        continue
                else:
                    raise RuntimeError(
                        f"Could not decode text file {text_path.name} with any supported encoding"
                    )

            # Prepare output directory
            if output_dir:
                base_output_dir = Path(output_dir)
            else:
                base_output_dir = text_path.parent / "pdf_output"

            base_output_dir.mkdir(parents=True, exist_ok=True)
            pdf_path = base_output_dir / f"{text_path.stem}.pdf"

            # Convert text to PDF
            logging.info(f"Converting {text_path.name} to PDF...")

            try:
                from reportlab.lib.pagesizes import A4
                from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
                from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
                from reportlab.lib.units import inch
                from reportlab.pdfbase import pdfmetrics

                # Create PDF document
                doc = SimpleDocTemplate(
                    str(pdf_path),
                    pagesize=A4,
                    leftMargin=inch,
                    rightMargin=inch,
                    topMargin=inch,
                    bottomMargin=inch,
                )

                # Get styles
                styles = getSampleStyleSheet()
                normal_style = styles["Normal"]
                heading_style = styles["Heading1"]

                # Try to register a font that supports Chinese characters
                try:
                    # Try to use system fonts that support Chinese
                    system = platform.system()
                    if system == "Windows":
                        # Try common Windows fonts
                        for font_name in ["SimSun", "SimHei", "Microsoft YaHei"]:
                            try:
                                from reportlab.pdfbase.cidfonts import (
                                    UnicodeCIDFont,
                                )

                                pdfmetrics.registerFont(UnicodeCIDFont(font_name))  # type: ignore
                                normal_style.fontName = font_name
                                heading_style.fontName = font_name
                                break
                            except Exception:
                                continue
                    elif system == "Darwin":  # macOS
                        for font_name in ["STSong-Light", "STHeiti"]:
                            try:
                                from reportlab.pdfbase.cidfonts import (
                                    UnicodeCIDFont,
                                )

                                pdfmetrics.registerFont(UnicodeCIDFont(font_name))  # type: ignore
                                normal_style.fontName = font_name
                                heading_style.fontName = font_name
                                break
                            except Exception:
                                continue
                except Exception:
                    pass  # Use default fonts if Chinese font setup fails

                # Build content
                story = []

                # Handle markdown or plain text
                if text_path.suffix.lower() == ".md":
                    # Handle markdown content - simplified implementation
                    lines = text_content.split("\n")
                    for line in lines:
                        line = line.strip()
                        if not line:
                            story.append(Spacer(1, 12))
                            continue

                        # Headers
                        if line.startswith("#"):
                            level = len(line) - len(line.lstrip("#"))
                            header_text = line.lstrip("#").strip()
                            if header_text:
                                header_style = ParagraphStyle(
                                    name=f"Heading{level}",
                                    parent=heading_style,
                                    fontSize=max(16 - level, 10),
                                    spaceAfter=8,
                                    spaceBefore=16 if level <= 2 else 12,
                                )
                                story.append(Paragraph(header_text, header_style))
                        else:
                            # Regular text
                            processed_line = PDFConverter._process_inline_markdown(line)
                            story.append(Paragraph(processed_line, normal_style))
                            story.append(Spacer(1, 6))
                else:
                    # Handle plain text files (.txt)
                    logging.info(
                        f"Processing plain text file with {len(text_content)} characters..."
                    )

                    # Split text into lines and process each line
                    lines = text_content.split("\n")
                    line_count = 0

                    for line in lines:
                        line = line.rstrip()
                        line_count += 1

                        # Empty lines
                        if not line.strip():
                            story.append(Spacer(1, 6))
                            continue

                        # Regular text lines
                        # Escape special characters for ReportLab
                        safe_line = (
                            line.replace("&", "&amp;")
                            .replace("<", "&lt;")
                            .replace(">", "&gt;")
                        )

                        # Create paragraph
                        story.append(Paragraph(safe_line, normal_style))
                        story.append(Spacer(1, 3))

                    logging.info(f"Added {line_count} lines to PDF")

                    # If no content was added, add a placeholder
                    if not story:
                        story.append(Paragraph("(Empty text file)", normal_style))

                # Build PDF
                doc.build(story)
                logging.info(
                    f"Successfully converted {text_path.name} to PDF ({pdf_path.stat().st_size / 1024:.1f} KB)"
                )

            except ImportError:
                raise RuntimeError(
                    "reportlab is required for text-to-PDF conversion. "
                    "Please install it using: pip install reportlab"
                )
            except Exception as e:
                raise RuntimeError(
                    f"Failed to convert text file {text_path.name} to PDF: {str(e)}"
                )

            # Validate the generated PDF
            if not pdf_path.exists() or pdf_path.stat().st_size < 100:
                raise RuntimeError(
                    f"PDF conversion failed for {text_path.name} - generated PDF is empty or corrupted."
                )

            return pdf_path

        except Exception as e:
            logging.error(f"Error in convert_text_to_pdf: {str(e)}")
            raise

    @staticmethod
    def _process_inline_markdown(text: str) -> str:
        """
        Process inline markdown formatting (bold, italic, code, links)

        Args:
            text: Raw text with markdown formatting

        Returns:
            Text with ReportLab markup
        """
        import re

        # Escape special characters for ReportLab
        text = text.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")

        # Bold text: **text** or __text__
        text = re.sub(r"\*\*(.*?)\*\*", r"<b>\1</b>", text)
        text = re.sub(r"__(.*?)__", r"<b>\1</b>", text)

        # Italic text: *text* or _text_ (but not in the middle of words)
        text = re.sub(r"(?<!\w)\*([^*\n]+?)\*(?!\w)", r"<i>\1</i>", text)
        text = re.sub(r"(?<!\w)_([^_\n]+?)_(?!\w)", r"<i>\1</i>", text)

        # Inline code: `code`
        text = re.sub(
            r"`([^`]+?)`",
            r'<font name="Courier" size="9" color="darkred">\1</font>',
            text,
        )

        # Links: [text](url) - convert to text with URL annotation
        def link_replacer(match):
            link_text = match.group(1)
            url = match.group(2)
            return f'<link href="{url}" color="blue"><u>{link_text}</u></link>'

        text = re.sub(r"\[([^\]]+?)\]\(([^)]+?)\)", link_replacer, text)

        # Strikethrough: ~~text~~
        text = re.sub(r"~~(.*?)~~", r"<strike>\1</strike>", text)

        return text

    def convert_to_pdf(
        self,
        file_path: Union[str, Path],
        output_dir: Optional[str] = None,
    ) -> Path:
        """
        Convert document to PDF based on file extension

        Args:
            file_path: Path to the file to be converted
            output_dir: Output directory path

        Returns:
            Path to the generated PDF file
        """
        # Convert to Path object
        file_path = Path(file_path)
        if not file_path.exists():
            raise FileNotFoundError(f"File does not exist: {file_path}")

        # Get file extension
        ext = file_path.suffix.lower()

        # Choose appropriate conversion method based on file type
        if ext in self.OFFICE_FORMATS:
            return self.convert_office_to_pdf(file_path, output_dir)
        elif ext in self.TEXT_FORMATS:
            return self.convert_text_to_pdf(file_path, output_dir)
        else:
            raise ValueError(
                f"Unsupported file format: {ext}. "
                f"Supported formats: {', '.join(self.OFFICE_FORMATS | self.TEXT_FORMATS)}"
            )

    def check_dependencies(self) -> dict:
        """
        Check if required dependencies are available

        Returns:
            dict: Dictionary with dependency check results
        """
        results = {
            "libreoffice": False,
            "reportlab": False,
        }

        # Check LibreOffice
        try:
            subprocess_kwargs: Dict[str, Any] = {
                "capture_output": True,
                "text": True,
                "check": True,
                "encoding": "utf-8",
                "errors": "ignore",
            }

            if platform.system() == "Windows":
                subprocess_kwargs["creationflags"] = (
                    0x08000000  # subprocess.CREATE_NO_WINDOW
                )

            subprocess.run(["libreoffice", "--version"], **subprocess_kwargs)
            results["libreoffice"] = True
        except (subprocess.CalledProcessError, FileNotFoundError):
            try:
                subprocess.run(["soffice", "--version"], **subprocess_kwargs)
                results["libreoffice"] = True
            except (subprocess.CalledProcessError, FileNotFoundError):
                pass

        # Check ReportLab
        import importlib.util

        if importlib.util.find_spec("reportlab") is not None:
            results["reportlab"] = True

        return results


def main():
    """
    Main function to run the PDF converter from command line
    """
    parser = argparse.ArgumentParser(description="Convert documents to PDF format")
    parser.add_argument("file_path", nargs="?", help="Path to the document to convert")
    parser.add_argument("--output", "-o", help="Output directory path")
    parser.add_argument(
        "--check",
        action="store_true",
        help="Check dependencies installation",
    )
    parser.add_argument(
        "--verbose", "-v", action="store_true", help="Enable verbose logging"
    )

    args = parser.parse_args()

    # Configure logging
    log_level = logging.INFO if args.verbose else logging.WARNING
    logging.basicConfig(
        level=log_level,
        format="%(asctime)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )

    # Initialize converter
    converter = PDFConverter()

    # Check dependencies if requested
    if args.check:
        print("🔍 Checking dependencies...")
        deps = converter.check_dependencies()

        print(
            f"LibreOffice: {'✅ Available' if deps['libreoffice'] else '❌ Not found'}"
        )
        print(f"ReportLab: {'✅ Available' if deps['reportlab'] else '❌ Not found'}")

        if not deps["libreoffice"]:
            print("\n📋 To install LibreOffice:")
            print("  - Windows: Download from https://www.libreoffice.org/")
            print("  - macOS: brew install --cask libreoffice")
            print("  - Ubuntu/Debian: sudo apt-get install libreoffice")

        if not deps["reportlab"]:
            print("\n📋 To install ReportLab:")
            print("  pip install reportlab")

        return 0

    # If not checking dependencies, file_path is required
    if not args.file_path:
        parser.error("file_path is required when not using --check")

    try:
        # Convert the file
        output_pdf = converter.convert_to_pdf(
            file_path=args.file_path,
            output_dir=args.output,
        )

        print(f"✅ Successfully converted to PDF: {output_pdf}")
        print(f"📄 File size: {output_pdf.stat().st_size / 1024:.1f} KB")

    except Exception as e:
        print(f"❌ Error: {str(e)}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())



================================================
FILE: tools/pdf_downloader.py
================================================
#!/usr/bin/env python3
"""
Smart PDF Downloader MCP Tool

A standardized MCP tool using FastMCP for intelligent file downloading and document conversion.
Supports natural language instructions for downloading files from URLs, moving local files,
and automatic conversion to Markdown format with image extraction.

Features:
- Natural language instruction parsing
- URL and local path extraction
- Automatic document conversion (PDF, DOCX, PPTX, HTML, etc.)
- Image extraction and preservation
- Multi-format support with fallback options
"""

import os
import re
import aiohttp
import aiofiles
import shutil
import sys
import io
from typing import List, Dict, Optional, Any
from urllib.parse import urlparse, unquote
from datetime import datetime

from mcp.server import FastMCP

# Docling imports for document conversion
try:
    from docling.document_converter import DocumentConverter
    from docling.datamodel.base_models import InputFormat
    from docling.datamodel.pipeline_options import PdfPipelineOptions
    from docling.document_converter import PdfFormatOption

    DOCLING_AVAILABLE = True
except ImportError:
    DOCLING_AVAILABLE = False
    print(
        "Warning: docling package not available. Document conversion will be disabled."
    )

# Fallback PDF text extraction
try:
    import PyPDF2

    PYPDF2_AVAILABLE = True
except ImportError:
    PYPDF2_AVAILABLE = False
    print(
        "Warning: PyPDF2 package not available. Fallback PDF extraction will be disabled."
    )

# 设置标准输出编码为UTF-8
if sys.stdout.encoding != "utf-8":
    try:
        if hasattr(sys.stdout, "reconfigure"):
            sys.stdout.reconfigure(encoding="utf-8")
            sys.stderr.reconfigure(encoding="utf-8")
        else:
            sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding="utf-8")
            sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding="utf-8")
    except Exception as e:
        print(f"Warning: Could not set UTF-8 encoding: {e}")

# 创建 FastMCP 实例
mcp = FastMCP("smart-pdf-downloader")


# 辅助函数
def format_success_message(action: str, details: Dict[str, Any]) -> str:
    """格式化成功消息"""
    return f"✅ {action}\n" + "\n".join(f"   {k}: {v}" for k, v in details.items())


def format_error_message(action: str, error: str) -> str:
    """格式化错误消息"""
    return f"❌ {action}\n   Error: {error}"


def format_warning_message(action: str, warning: str) -> str:
    """格式化警告消息"""
    return f"⚠️ {action}\n   Warning: {warning}"


async def perform_document_conversion(
    file_path: str, extract_images: bool = True
) -> Optional[str]:
    """
    执行文档转换的共用逻辑

    Args:
        file_path: 文件路径
        extract_images: 是否提取图片

    Returns:
        转换信息字符串，如果没有转换则返回None
    """
    if not file_path:
        return None

    conversion_msg = ""

    # 首先尝试使用简单的PDF转换器（对于PDF文件）
    # 检查文件是否实际为PDF（无论扩展名如何）
    is_pdf_file = False
    if PYPDF2_AVAILABLE:
        try:
            with open(file_path, "rb") as f:
                header = f.read(8)
                is_pdf_file = header.startswith(b"%PDF")
        except Exception:
            is_pdf_file = file_path.lower().endswith(".pdf")

    if is_pdf_file and PYPDF2_AVAILABLE:
        try:
            simple_converter = SimplePdfConverter()
            conversion_result = simple_converter.convert_pdf_to_markdown(file_path)
            if conversion_result["success"]:
                conversion_msg = "\n   [INFO] PDF converted to Markdown (PyPDF2)"
                conversion_msg += (
                    f"\n   Markdown file: {conversion_result['output_file']}"
                )
                conversion_msg += (
                    f"\n   Conversion time: {conversion_result['duration']:.2f} seconds"
                )
                conversion_msg += (
                    f"\n   Pages extracted: {conversion_result['pages_extracted']}"
                )

            else:
                conversion_msg = f"\n   [WARNING] PDF conversion failed: {conversion_result['error']}"
        except Exception as conv_error:
            conversion_msg = f"\n   [WARNING] PDF conversion error: {str(conv_error)}"

    # 如果简单转换失败，尝试使用docling（支持图片提取）
    # if not conversion_success and DOCLING_AVAILABLE:
    #     try:
    #         converter = DoclingConverter()
    #         if converter.is_supported_format(file_path):
    #             conversion_result = converter.convert_to_markdown(
    #                 file_path, extract_images=extract_images
    #             )
    #             if conversion_result["success"]:
    #                 conversion_msg = (
    #                     "\n   [INFO] Document converted to Markdown (docling)"
    #                 )
    #                 conversion_msg += (
    #                     f"\n   Markdown file: {conversion_result['output_file']}"
    #                 )
    #                 conversion_msg += f"\n   Conversion time: {conversion_result['duration']:.2f} seconds"
    #                 if conversion_result.get("images_extracted", 0) > 0:
    #                     conversion_msg += f"\n   Images extracted: {conversion_result['images_extracted']}"
    #                     images_dir = os.path.join(
    #                         os.path.dirname(conversion_result["output_file"]), "images"
    #                     )
    #                     conversion_msg += f"\n   Images saved to: {images_dir}"
    #             else:
    #                 conversion_msg = f"\n   [WARNING] Docling conversion failed: {conversion_result['error']}"
    #     except Exception as conv_error:
    #         conversion_msg = (
    #             f"\n   [WARNING] Docling conversion error: {str(conv_error)}"
    #         )

    return conversion_msg if conversion_msg else None


def format_file_operation_result(
    operation: str,
    source: str,
    destination: str,
    result: Dict[str, Any],
    conversion_msg: Optional[str] = None,
) -> str:
    """
    格式化文件操作结果的共用逻辑

    Args:
        operation: 操作类型 ("download" 或 "move")
        source: 源文件/URL
        destination: 目标路径
        result: 操作结果字典
        conversion_msg: 转换消息

    Returns:
        格式化的结果消息
    """
    if result["success"]:
        size_mb = result["size"] / (1024 * 1024)
        msg = f"[SUCCESS] Successfully {operation}d: {source}\n"

        if operation == "download":
            msg += f"   File: {destination}\n"
            msg += f"   Size: {size_mb:.2f} MB\n"
            msg += f"   Time: {result['duration']:.2f} seconds\n"
            speed_mb = result.get("speed", 0) / (1024 * 1024)
            msg += f"   Speed: {speed_mb:.2f} MB/s"
        else:  # move
            msg += f"   To: {destination}\n"
            msg += f"   Size: {size_mb:.2f} MB\n"
            msg += f"   Time: {result['duration']:.2f} seconds"

        if conversion_msg:
            msg += conversion_msg

        return msg
    else:
        return f"[ERROR] Failed to {operation}: {source}\n   Error: {result.get('error', 'Unknown error')}"


class LocalPathExtractor:
    """本地路径提取器"""

    @staticmethod
    def is_local_path(path: str) -> bool:
        """判断是否为本地路径"""
        path = path.strip("\"'")

        # 检查是否为URL
        if re.match(r"^https?://", path, re.IGNORECASE) or re.match(
            r"^ftp://", path, re.IGNORECASE
        ):
            return False

        # 路径指示符
        path_indicators = [os.path.sep, "/", "\\", "~", ".", ".."]
        has_extension = bool(os.path.splitext(path)[1])

        if any(indicator in path for indicator in path_indicators) or has_extension:
            expanded_path = os.path.expanduser(path)
            return os.path.exists(expanded_path) or any(
                indicator in path for indicator in path_indicators
            )

        return False

    @staticmethod
    def extract_local_paths(text: str) -> List[str]:
        """从文本中提取本地文件路径"""
        patterns = [
            r'"([^"]+)"',
            r"'([^']+)'",
            r"(?:^|\s)((?:[~./\\]|[A-Za-z]:)?(?:[^/\\\s]+[/\\])*[^/\\\s]+\.[A-Za-z0-9]+)(?:\s|$)",
            r"(?:^|\s)((?:~|\.{1,2})?/[^\s]+)(?:\s|$)",
            r"(?:^|\s)([A-Za-z]:[/\\][^\s]+)(?:\s|$)",
            r"(?:^|\s)(\.{1,2}[/\\][^\s]+)(?:\s|$)",
        ]

        local_paths = []
        potential_paths = []

        for pattern in patterns:
            matches = re.findall(pattern, text, re.MULTILINE)
            potential_paths.extend(matches)

        for path in potential_paths:
            path = path.strip()
            if path and LocalPathExtractor.is_local_path(path):
                expanded_path = os.path.expanduser(path)
                if expanded_path not in local_paths:
                    local_paths.append(expanded_path)

        return local_paths


class URLExtractor:
    """URL提取器"""

    URL_PATTERNS = [
        r"https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+(?:/(?:[-\w._~!$&\'()*+,;=:@]|%[\da-fA-F]{2})*)*(?:\?(?:[-\w._~!$&\'()*+,;=:@/?]|%[\da-fA-F]{2})*)?(?:#(?:[-\w._~!$&\'()*+,;=:@/?]|%[\da-fA-F]{2})*)?",
        r"ftp://(?:[-\w.]|(?:%[\da-fA-F]{2}))+(?:/(?:[-\w._~!$&\'()*+,;=:@]|%[\da-fA-F]{2})*)*",
        r"(?<!\S)(?:www\.)?[-\w]+(?:\.[-\w]+)+/(?:[-\w._~!$&\'()*+,;=:@/]|%[\da-fA-F]{2})+",
    ]

    @staticmethod
    def convert_arxiv_url(url: str) -> str:
        """将arXiv网页链接转换为PDF下载链接"""
        # 匹配arXiv论文ID的正则表达式
        arxiv_pattern = r"arxiv\.org/abs/(\d+\.\d+)(?:v\d+)?"
        match = re.search(arxiv_pattern, url, re.IGNORECASE)
        if match:
            paper_id = match.group(1)
            return f"https://arxiv.org/pdf/{paper_id}.pdf"
        return url

    @classmethod
    def extract_urls(cls, text: str) -> List[str]:
        """从文本中提取URL"""
        urls = []

        # 首先处理特殊情况：@开头的URL
        at_url_pattern = r"@(https?://[^\s]+)"
        at_matches = re.findall(at_url_pattern, text, re.IGNORECASE)
        for match in at_matches:
            # 处理arXiv链接
            url = cls.convert_arxiv_url(match.rstrip("/"))
            urls.append(url)

        # 然后使用原有的正则模式
        for pattern in cls.URL_PATTERNS:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                # 处理可能缺少协议的URL
                if not match.startswith(("http://", "https://", "ftp://")):
                    # 检查是否是 www 开头
                    if match.startswith("www."):
                        match = "https://" + match
                    else:
                        # 其他情况也添加 https
                        match = "https://" + match

                # 处理arXiv链接
                url = cls.convert_arxiv_url(match.rstrip("/"))
                urls.append(url)

        # 去重并保持顺序
        seen = set()
        unique_urls = []
        for url in urls:
            if url not in seen:
                seen.add(url)
                unique_urls.append(url)

        return unique_urls

    @staticmethod
    def infer_filename_from_url(url: str) -> str:
        """从URL推断文件名"""
        parsed = urlparse(url)
        path = unquote(parsed.path)

        # 从路径中提取文件名
        filename = os.path.basename(path)

        # 特殊处理：arxiv PDF链接
        if "arxiv.org" in parsed.netloc and "/pdf/" in path:
            if filename:
                # 检查是否已经有合适的文件扩展名
                if not filename.lower().endswith((".pdf", ".doc", ".docx", ".txt")):
                    filename = f"{filename}.pdf"
            else:
                path_parts = [p for p in path.split("/") if p]
                if path_parts and path_parts[-1]:
                    filename = f"{path_parts[-1]}.pdf"
                else:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"arxiv_paper_{timestamp}.pdf"

        # 如果没有文件名或没有扩展名，生成一个
        elif not filename or "." not in filename:
            # 尝试从URL生成有意义的文件名
            domain = parsed.netloc.replace("www.", "").replace(".", "_")
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            # 尝试根据路径推断文件类型
            if not path or path == "/":
                filename = f"{domain}_{timestamp}.html"
            else:
                # 使用路径的最后一部分
                path_parts = [p for p in path.split("/") if p]
                if path_parts:
                    filename = f"{path_parts[-1]}_{timestamp}"
                else:
                    filename = f"{domain}_{timestamp}"

                # 如果还是没有扩展名，根据路径推断
                if "." not in filename:
                    # 根据路径中的关键词推断文件类型
                    if "/pdf/" in path.lower() or path.lower().endswith("pdf"):
                        filename += ".pdf"
                    elif any(
                        ext in path.lower() for ext in ["/doc/", "/word/", ".docx"]
                    ):
                        filename += ".docx"
                    elif any(
                        ext in path.lower()
                        for ext in ["/ppt/", "/powerpoint/", ".pptx"]
                    ):
                        filename += ".pptx"
                    elif any(ext in path.lower() for ext in ["/csv/", ".csv"]):
                        filename += ".csv"
                    elif any(ext in path.lower() for ext in ["/zip/", ".zip"]):
                        filename += ".zip"
                    else:
                        filename += ".html"

        return filename


class PathExtractor:
    """路径提取器"""

    @staticmethod
    def extract_target_path(text: str) -> Optional[str]:
        """从文本中提取目标路径"""
        patterns = [
            r'(?:save|download|store|put|place|write|copy|move)\s+(?:to|into|in|at)\s+["\']?([^\s"\']+)["\']?',
            r'(?:to|into|in|at)\s+(?:folder|directory|dir|path|location)\s*["\']?([^\s"\']+)["\']?',
            r'(?:destination|target|output)\s*(?:is|:)?\s*["\']?([^\s"\']+)["\']?',
            r'(?:保存|下载|存储|放到|写入|复制|移动)(?:到|至|去)\s*["\']?([^\s"\']+)["\']?',
            r'(?:到|在|至)\s*["\']?([^\s"\']+)["\']?\s*(?:文件夹|目录|路径|位置)',
        ]

        filter_words = {
            "here",
            "there",
            "current",
            "local",
            "this",
            "that",
            "这里",
            "那里",
            "当前",
            "本地",
            "这个",
            "那个",
        }

        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                path = match.group(1).strip("。，,.、")
                if path and path.lower() not in filter_words:
                    return path

        return None


class SimplePdfConverter:
    """简单的PDF转换器，使用PyPDF2提取文本"""

    def convert_pdf_to_markdown(
        self, input_file: str, output_file: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        使用PyPDF2将PDF转换为Markdown格式

        Args:
            input_file: 输入PDF文件路径
            output_file: 输出Markdown文件路径（可选）

        Returns:
            转换结果字典
        """
        if not PYPDF2_AVAILABLE:
            return {"success": False, "error": "PyPDF2 package is not available"}

        try:
            # 检查输入文件是否存在
            if not os.path.exists(input_file):
                return {
                    "success": False,
                    "error": f"Input file not found: {input_file}",
                }

            # 如果没有指定输出文件，自动生成
            if not output_file:
                base_name = os.path.splitext(input_file)[0]
                output_file = f"{base_name}.md"

            # 确保输出目录存在
            output_dir = os.path.dirname(output_file)
            if output_dir:
                os.makedirs(output_dir, exist_ok=True)

            # 执行转换
            start_time = datetime.now()

            # 读取PDF文件
            with open(input_file, "rb") as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text_content = []

                # 提取每页文本
                for page_num, page in enumerate(pdf_reader.pages, 1):
                    text = page.extract_text()
                    if text.strip():
                        text_content.append(f"## Page {page_num}\n\n{text.strip()}\n\n")

            # 生成Markdown内容
            markdown_content = f"# Extracted from {os.path.basename(input_file)}\n\n"
            markdown_content += f"*Total pages: {len(pdf_reader.pages)}*\n\n"
            markdown_content += "---\n\n"
            markdown_content += "".join(text_content)

            # 保存到文件
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(markdown_content)

            # 计算转换时间
            duration = (datetime.now() - start_time).total_seconds()

            # 获取文件大小
            input_size = os.path.getsize(input_file)
            output_size = os.path.getsize(output_file)

            return {
                "success": True,
                "input_file": input_file,
                "output_file": output_file,
                "input_size": input_size,
                "output_size": output_size,
                "duration": duration,
                "markdown_content": markdown_content,
                "pages_extracted": len(pdf_reader.pages),
            }

        except Exception as e:
            return {
                "success": False,
                "input_file": input_file,
                "error": f"Conversion failed: {str(e)}",
            }


class DoclingConverter:
    """文档转换器，使用docling将文档转换为Markdown格式，支持图片提取"""

    def __init__(self):
        if not DOCLING_AVAILABLE:
            raise ImportError(
                "docling package is not available. Please install it first."
            )

        # 配置PDF处理选项
        pdf_pipeline_options = PdfPipelineOptions()
        pdf_pipeline_options.do_ocr = False  # 暂时禁用OCR以避免认证问题
        pdf_pipeline_options.do_table_structure = False  # 暂时禁用表格结构识别

        # 创建文档转换器（使用基础模式）
        try:
            self.converter = DocumentConverter(
                format_options={
                    InputFormat.PDF: PdfFormatOption(
                        pipeline_options=pdf_pipeline_options
                    )
                }
            )
        except Exception:
            # 如果失败，尝试更简单的配置
            self.converter = DocumentConverter()

    def is_supported_format(self, file_path: str) -> bool:
        """检查文件格式是否支持转换"""
        if not DOCLING_AVAILABLE:
            return False

        supported_extensions = {".pdf", ".docx", ".pptx", ".html", ".md", ".txt"}
        file_extension = os.path.splitext(file_path)[1].lower()
        return file_extension in supported_extensions

    def is_url(self, path: str) -> bool:
        """检查路径是否为URL"""
        try:
            result = urlparse(path)
            return result.scheme in ("http", "https")
        except Exception:
            return False

    def extract_images(self, doc, output_dir: str) -> Dict[str, str]:
        """
        提取文档中的图片并保存到本地

        Args:
            doc: docling文档对象
            output_dir: 输出目录

        Returns:
            图片ID到本地文件路径的映射
        """
        images_dir = os.path.join(output_dir, "images")
        os.makedirs(images_dir, exist_ok=True)
        image_map = {}  # docling图片id -> 本地文件名

        try:
            # 获取文档中的图片
            images = getattr(doc, "images", [])

            for idx, img in enumerate(images):
                try:
                    # 获取图片格式，默认为png
                    ext = getattr(img, "format", None) or "png"
                    if ext.lower() not in ["png", "jpg", "jpeg", "gif", "bmp", "webp"]:
                        ext = "png"

                    # 生成文件名
                    filename = f"image_{idx+1}.{ext}"
                    filepath = os.path.join(images_dir, filename)

                    # 保存图片数据
                    img_data = getattr(img, "data", None)
                    if img_data:
                        with open(filepath, "wb") as f:
                            f.write(img_data)

                        # 计算相对路径
                        rel_path = os.path.relpath(filepath, output_dir)
                        img_id = getattr(img, "id", str(idx + 1))
                        image_map[img_id] = rel_path

                except Exception as img_error:
                    print(f"Warning: Failed to extract image {idx+1}: {img_error}")
                    continue

        except Exception as e:
            print(f"Warning: Failed to extract images: {e}")

        return image_map

    def process_markdown_with_images(
        self, markdown_content: str, image_map: Dict[str, str]
    ) -> str:
        """
        处理Markdown内容，替换图片占位符为实际的图片路径

        Args:
            markdown_content: 原始Markdown内容
            image_map: 图片ID到本地路径的映射

        Returns:
            处理后的Markdown内容
        """

        def replace_img(match):
            img_id = match.group(1)
            if img_id in image_map:
                return f"![Image]({image_map[img_id]})"
            else:
                return match.group(0)

        # 替换docling的图片占位符
        processed_content = re.sub(
            r"!\[Image\]\(docling://image/([^)]+)\)", replace_img, markdown_content
        )

        return processed_content

    def convert_to_markdown(
        self,
        input_file: str,
        output_file: Optional[str] = None,
        extract_images: bool = True,
    ) -> Dict[str, Any]:
        """
        将文档转换为Markdown格式，支持图片提取

        Args:
            input_file: 输入文件路径或URL
            output_file: 输出Markdown文件路径（可选）
            extract_images: 是否提取图片（默认True）

        Returns:
            转换结果字典
        """
        if not DOCLING_AVAILABLE:
            return {"success": False, "error": "docling package is not available"}

        try:
            # 检查输入文件（如果不是URL）
            if not self.is_url(input_file):
                if not os.path.exists(input_file):
                    return {
                        "success": False,
                        "error": f"Input file not found: {input_file}",
                    }

                # 检查文件格式是否支持
                if not self.is_supported_format(input_file):
                    return {
                        "success": False,
                        "error": f"Unsupported file format: {os.path.splitext(input_file)[1]}",
                    }
            else:
                # 对于URL，检查是否为支持的格式
                if not input_file.lower().endswith(
                    (".pdf", ".docx", ".pptx", ".html", ".md", ".txt")
                ):
                    return {
                        "success": False,
                        "error": f"Unsupported URL format: {input_file}",
                    }

            # 如果没有指定输出文件，自动生成
            if not output_file:
                if self.is_url(input_file):
                    # 从URL生成文件名
                    filename = URLExtractor.infer_filename_from_url(input_file)
                    base_name = os.path.splitext(filename)[0]
                else:
                    base_name = os.path.splitext(input_file)[0]
                output_file = f"{base_name}.md"

            # 确保输出目录存在
            output_dir = os.path.dirname(output_file) or "."
            os.makedirs(output_dir, exist_ok=True)

            # 执行转换
            start_time = datetime.now()
            result = self.converter.convert(input_file)
            doc = result.document

            # 提取图片（如果启用）
            image_map = {}
            images_extracted = 0
            if extract_images:
                image_map = self.extract_images(doc, output_dir)
                images_extracted = len(image_map)

            # 获取Markdown内容
            markdown_content = doc.export_to_markdown()

            # 处理图片占位符
            if extract_images and image_map:
                markdown_content = self.process_markdown_with_images(
                    markdown_content, image_map
                )

            # 保存到文件
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(markdown_content)

            # 计算转换时间
            duration = (datetime.now() - start_time).total_seconds()

            # 获取文件大小
            if self.is_url(input_file):
                input_size = 0  # URL无法直接获取大小
            else:
                input_size = os.path.getsize(input_file)
            output_size = os.path.getsize(output_file)

            return {
                "success": True,
                "input_file": input_file,
                "output_file": output_file,
                "input_size": input_size,
                "output_size": output_size,
                "duration": duration,
                "markdown_content": markdown_content,
                "images_extracted": images_extracted,
                "image_map": image_map,
            }

        except Exception as e:
            return {
                "success": False,
                "input_file": input_file,
                "error": f"Conversion failed: {str(e)}",
            }


async def check_url_accessible(url: str) -> Dict[str, Any]:
    """检查URL是否可访问"""
    try:
        timeout = aiohttp.ClientTimeout(total=10)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.head(url, allow_redirects=True) as response:
                return {
                    "accessible": response.status < 400,
                    "status": response.status,
                    "content_type": response.headers.get("Content-Type", ""),
                    "content_length": response.headers.get("Content-Length", 0),
                }
    except Exception:
        return {
            "accessible": False,
            "status": 0,
            "content_type": "",
            "content_length": 0,
        }


async def download_file(url: str, destination: str) -> Dict[str, Any]:
    """下载单个文件"""
    start_time = datetime.now()
    chunk_size = 8192

    try:
        timeout = aiohttp.ClientTimeout(total=300)  # 5分钟超时
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.get(url) as response:
                # 检查响应状态
                response.raise_for_status()

                # 获取文件信息
                content_type = response.headers.get(
                    "Content-Type", "application/octet-stream"
                )

                # 确保目标目录存在
                parent_dir = os.path.dirname(destination)
                if parent_dir:
                    os.makedirs(parent_dir, exist_ok=True)

                # 下载文件
                downloaded = 0
                async with aiofiles.open(destination, "wb") as file:
                    async for chunk in response.content.iter_chunked(chunk_size):
                        await file.write(chunk)
                        downloaded += len(chunk)

                # 计算下载时间
                duration = (datetime.now() - start_time).total_seconds()

                return {
                    "success": True,
                    "url": url,
                    "destination": destination,
                    "size": downloaded,
                    "content_type": content_type,
                    "duration": duration,
                    "speed": downloaded / duration if duration > 0 else 0,
                }

    except aiohttp.ClientError as e:
        return {
            "success": False,
            "url": url,
            "destination": destination,
            "error": f"Network error: {str(e)}",
        }
    except Exception as e:
        return {
            "success": False,
            "url": url,
            "destination": destination,
            "error": f"Download error: {str(e)}",
        }


async def move_local_file(source_path: str, destination: str) -> Dict[str, Any]:
    """移动本地文件到目标位置"""
    start_time = datetime.now()

    try:
        # 检查源文件是否存在
        if not os.path.exists(source_path):
            return {
                "success": False,
                "source": source_path,
                "destination": destination,
                "error": f"Source file not found: {source_path}",
            }

        # 获取源文件信息
        source_size = os.path.getsize(source_path)

        # 确保目标目录存在
        parent_dir = os.path.dirname(destination)
        if parent_dir:
            os.makedirs(parent_dir, exist_ok=True)

        # 执行移动操作
        shutil.move(source_path, destination)

        # 计算操作时间
        duration = (datetime.now() - start_time).total_seconds()

        return {
            "success": True,
            "source": source_path,
            "destination": destination,
            "size": source_size,
            "duration": duration,
            "operation": "move",
        }

    except Exception as e:
        return {
            "success": False,
            "source": source_path,
            "destination": destination,
            "error": f"Move error: {str(e)}",
        }


@mcp.tool()
async def download_files(instruction: str) -> str:
    """
    Download files from URLs or move local files mentioned in natural language instructions.

    Args:
        instruction: Natural language instruction containing URLs/local paths and optional destination paths

    Returns:
        Status message about the download/move operations

    Examples:
        - "Download https://example.com/file.pdf to documents folder"
        - "Move /home/user/file.pdf to documents folder"
        - "Please get https://raw.githubusercontent.com/user/repo/main/data.csv and save it to ~/downloads"
        - "移动 ~/Desktop/report.docx 到 /tmp/documents/"
        - "Download www.example.com/report.xlsx"
    """
    urls = URLExtractor.extract_urls(instruction)
    local_paths = LocalPathExtractor.extract_local_paths(instruction)

    if not urls and not local_paths:
        return format_error_message(
            "Failed to parse instruction",
            "No downloadable URLs or movable local files found",
        )

    target_path = PathExtractor.extract_target_path(instruction)

    # 处理文件
    results = []

    # 处理URL下载
    for url in urls:
        try:
            # 推断文件名
            filename = URLExtractor.infer_filename_from_url(url)

            # 构建完整的目标路径
            if target_path:
                # 处理路径
                if target_path.startswith("~"):
                    target_path = os.path.expanduser(target_path)

                # 确保使用相对路径（如果不是绝对路径）
                if not os.path.isabs(target_path):
                    target_path = os.path.normpath(target_path)

                # 判断是文件路径还是目录路径
                if os.path.splitext(target_path)[1]:  # 有扩展名，是文件
                    destination = target_path
                else:  # 是目录
                    destination = os.path.join(target_path, filename)
            else:
                # 默认下载到当前目录
                destination = filename

            # 检查文件是否已存在
            if os.path.exists(destination):
                results.append(
                    f"[WARNING] Skipped {url}: File already exists at {destination}"
                )
                continue

            # 先检查URL是否可访问
            check_result = await check_url_accessible(url)
            if not check_result["accessible"]:
                results.append(
                    f"[ERROR] Failed to access {url}: HTTP {check_result['status'] or 'Connection failed'}"
                )
                continue

            # 执行下载
            result = await download_file(url, destination)

            # 执行转换（如果成功下载）
            conversion_msg = None
            if result["success"]:
                conversion_msg = await perform_document_conversion(
                    destination, extract_images=True
                )

            # 格式化结果
            msg = format_file_operation_result(
                "download", url, destination, result, conversion_msg
            )

        except Exception as e:
            msg = f"[ERROR] Failed to download: {url}\n"
            msg += f"   Error: {str(e)}"

        results.append(msg)

    # 处理本地文件移动
    for local_path in local_paths:
        try:
            # 获取文件名
            filename = os.path.basename(local_path)

            # 构建完整的目标路径
            if target_path:
                # 处理路径
                if target_path.startswith("~"):
                    target_path = os.path.expanduser(target_path)

                # 确保使用相对路径（如果不是绝对路径）
                if not os.path.isabs(target_path):
                    target_path = os.path.normpath(target_path)

                # 判断是文件路径还是目录路径
                if os.path.splitext(target_path)[1]:  # 有扩展名，是文件
                    destination = target_path
                else:  # 是目录
                    destination = os.path.join(target_path, filename)
            else:
                # 默认移动到当前目录
                destination = filename

            # 检查目标文件是否已存在
            if os.path.exists(destination):
                results.append(
                    f"[WARNING] Skipped {local_path}: File already exists at {destination}"
                )
                continue

            # 执行移动
            result = await move_local_file(local_path, destination)

            # 执行转换（如果成功移动）
            conversion_msg = None
            if result["success"]:
                conversion_msg = await perform_document_conversion(
                    destination, extract_images=True
                )

            # 格式化结果
            msg = format_file_operation_result(
                "move", local_path, destination, result, conversion_msg
            )

        except Exception as e:
            msg = f"[ERROR] Failed to move: {local_path}\n"
            msg += f"   Error: {str(e)}"

        results.append(msg)

    return "\n\n".join(results)


@mcp.tool()
async def parse_download_urls(text: str) -> str:
    """
    Extract URLs, local paths and target paths from text without downloading or moving.

    Args:
        text: Text containing URLs, local paths and optional destination paths

    Returns:
        Parsed URLs, local paths and target path information
    """
    urls = URLExtractor.extract_urls(text)
    local_paths = LocalPathExtractor.extract_local_paths(text)
    target_path = PathExtractor.extract_target_path(text)

    content = "📋 Parsed file operation information:\n\n"

    if urls:
        content += f"🔗 URLs found ({len(urls)}):\n"
        for i, url in enumerate(urls, 1):
            filename = URLExtractor.infer_filename_from_url(url)
            content += f"  {i}. {url}\n     📄 Filename: {filename}\n"
    else:
        content += "🔗 No URLs found\n"

    if local_paths:
        content += f"\n📁 Local files found ({len(local_paths)}):\n"
        for i, path in enumerate(local_paths, 1):
            exists = os.path.exists(path)
            content += f"  {i}. {path}\n"
            content += f"     ✅ Exists: {'Yes' if exists else 'No'}\n"
            if exists:
                size_mb = os.path.getsize(path) / (1024 * 1024)
                content += f"     📊 Size: {size_mb:.2f} MB\n"
    else:
        content += "\n📁 No local files found\n"

    if target_path:
        content += f"\n🎯 Target path: {target_path}"
        if target_path.startswith("~"):
            content += f"\n   (Expanded: {os.path.expanduser(target_path)})"
    else:
        content += "\n🎯 Target path: Not specified (will use current directory)"

    return content


@mcp.tool()
async def download_file_to(
    url: str, destination: Optional[str] = None, filename: Optional[str] = None
) -> str:
    """
    Download a specific file with detailed options.

    Args:
        url: URL to download from
        destination: Target directory or full file path (optional)
        filename: Specific filename to use (optional, ignored if destination is a full file path)

    Returns:
        Status message about the download operation
    """
    # 确定文件名
    if not filename:
        filename = URLExtractor.infer_filename_from_url(url)

    # 确定完整路径
    if destination:
        # 展开用户目录
        if destination.startswith("~"):
            destination = os.path.expanduser(destination)

        # 检查是否是完整文件路径
        if os.path.splitext(destination)[1]:  # 有扩展名
            target_path = destination
        else:  # 是目录
            target_path = os.path.join(destination, filename)
    else:
        target_path = filename

    # 确保使用相对路径（如果不是绝对路径）
    if not os.path.isabs(target_path):
        target_path = os.path.normpath(target_path)

    # 检查文件是否已存在
    if os.path.exists(target_path):
        return format_error_message(
            "Download aborted", f"File already exists at {target_path}"
        )

    # 先检查URL
    check_result = await check_url_accessible(url)
    if not check_result["accessible"]:
        return format_error_message(
            "Cannot access URL",
            f"{url} (HTTP {check_result['status'] or 'Connection failed'})",
        )

    # 显示下载信息
    size_mb = (
        int(check_result["content_length"]) / (1024 * 1024)
        if check_result["content_length"]
        else 0
    )
    msg = "[INFO] Downloading file:\n"
    msg += f"   URL: {url}\n"
    msg += f"   Target: {target_path}\n"
    if size_mb > 0:
        msg += f"   Expected size: {size_mb:.2f} MB\n"
    msg += "\n"

    # 执行下载
    result = await download_file(url, target_path)

    # 执行转换（如果成功下载）
    conversion_msg = None
    if result["success"]:
        conversion_msg = await perform_document_conversion(
            target_path, extract_images=True
        )

        # 添加下载信息前缀
        actual_size_mb = result["size"] / (1024 * 1024)
        speed_mb = result["speed"] / (1024 * 1024)
        info_msg = "[SUCCESS] Download completed!\n"
        info_msg += f"   Saved to: {target_path}\n"
        info_msg += f"   Size: {actual_size_mb:.2f} MB\n"
        info_msg += f"   Duration: {result['duration']:.2f} seconds\n"
        info_msg += f"   Speed: {speed_mb:.2f} MB/s\n"
        info_msg += f"   Type: {result['content_type']}"

        if conversion_msg:
            info_msg += conversion_msg

        return msg + info_msg
    else:
        return msg + f"[ERROR] Download failed!\n   Error: {result['error']}"


@mcp.tool()
async def move_file_to(
    source: str, destination: Optional[str] = None, filename: Optional[str] = None
) -> str:
    """
    Move a local file to a new location with detailed options.

    Args:
        source: Source file path to move
        destination: Target directory or full file path (optional)
        filename: Specific filename to use (optional, ignored if destination is a full file path)

    Returns:
        Status message about the move operation
    """
    # 展开源路径
    if source.startswith("~"):
        source = os.path.expanduser(source)

    # 检查源文件是否存在
    if not os.path.exists(source):
        return format_error_message("Move aborted", f"Source file not found: {source}")

    # 确定文件名
    if not filename:
        filename = os.path.basename(source)

    # 确定完整路径
    if destination:
        # 展开用户目录
        if destination.startswith("~"):
            destination = os.path.expanduser(destination)

        # 检查是否是完整文件路径
        if os.path.splitext(destination)[1]:  # 有扩展名
            target_path = destination
        else:  # 是目录
            target_path = os.path.join(destination, filename)
    else:
        target_path = filename

    # 确保使用相对路径（如果不是绝对路径）
    if not os.path.isabs(target_path):
        target_path = os.path.normpath(target_path)

    # 检查目标文件是否已存在
    if os.path.exists(target_path):
        return f"[ERROR] Target file already exists: {target_path}"

    # 显示移动信息
    source_size_mb = os.path.getsize(source) / (1024 * 1024)
    msg = "[INFO] Moving file:\n"
    msg += f"   Source: {source}\n"
    msg += f"   Target: {target_path}\n"
    msg += f"   Size: {source_size_mb:.2f} MB\n"
    msg += "\n"

    # 执行移动
    result = await move_local_file(source, target_path)

    # 执行转换（如果成功移动）
    conversion_msg = None
    if result["success"]:
        conversion_msg = await perform_document_conversion(
            target_path, extract_images=True
        )

        # 添加移动信息前缀
        info_msg = "[SUCCESS] File moved successfully!\n"
        info_msg += f"   From: {source}\n"
        info_msg += f"   To: {target_path}\n"
        info_msg += f"   Duration: {result['duration']:.2f} seconds"

        if conversion_msg:
            info_msg += conversion_msg

        return msg + info_msg
    else:
        return msg + f"[ERROR] Move failed!\n   Error: {result['error']}"


# @mcp.tool()
# async def convert_document_to_markdown(
#     file_path: str, output_path: Optional[str] = None, extract_images: bool = True
# ) -> str:
#     """
#     Convert a document to Markdown format with image extraction support.

#     Supports both local files and URLs. Uses docling for advanced conversion with image extraction,
#     or falls back to PyPDF2 for simple PDF text extraction.

#     Args:
#         file_path: Path to the input document file or URL (supports PDF, DOCX, PPTX, HTML, TXT, MD)
#         output_path: Path for the output Markdown file (optional, auto-generated if not provided)
#         extract_images: Whether to extract images from the document (default: True)

#     Returns:
#         Status message about the conversion operation with preview of converted content

#     Examples:
#         - "convert_document_to_markdown('paper.pdf')"
#         - "convert_document_to_markdown('https://example.com/doc.pdf', 'output.md')"
#         - "convert_document_to_markdown('presentation.pptx', extract_images=False)"
#     """
#     # 检查是否为URL
#     is_url_input = False
#     try:
#         parsed = urlparse(file_path)
#         is_url_input = parsed.scheme in ("http", "https")
#     except Exception:
#         is_url_input = False

#     # 检查文件是否存在（如果不是URL）
#     if not is_url_input and not os.path.exists(file_path):
#         return f"[ERROR] Input file not found: {file_path}"

#     # 检查是否是PDF文件，优先使用简单转换器（仅对本地文件）
#     if (
#         not is_url_input
#         and file_path.lower().endswith(".pdf")
#         and PYPDF2_AVAILABLE
#         and not extract_images
#     ):
#         try:
#             simple_converter = SimplePdfConverter()
#             result = simple_converter.convert_pdf_to_markdown(file_path, output_path)
#         except Exception as e:
#             return f"[ERROR] PDF conversion error: {str(e)}"
#     elif DOCLING_AVAILABLE:
#         try:
#             converter = DoclingConverter()

#             # 检查文件格式是否支持
#             if not is_url_input and not converter.is_supported_format(file_path):
#                 supported_formats = [".pdf", ".docx", ".pptx", ".html", ".md", ".txt"]
#                 return f"[ERROR] Unsupported file format. Supported formats: {', '.join(supported_formats)}"
#             elif is_url_input and not file_path.lower().endswith(
#                 (".pdf", ".docx", ".pptx", ".html", ".md", ".txt")
#             ):
#                 return f"[ERROR] Unsupported URL format: {file_path}"

#             # 执行转换（支持图片提取）
#             result = converter.convert_to_markdown(
#                 file_path, output_path, extract_images
#             )
#         except Exception as e:
#             return f"[ERROR] Docling conversion error: {str(e)}"
#     else:
#         return (
#             "[ERROR] No conversion tools available. Please install docling or PyPDF2."
#         )

#     if result["success"]:
#         msg = "[SUCCESS] Document converted successfully!\n"
#         msg += f"   Input: {result['input_file']}\n"
#         msg += f"   Output file: {result['output_file']}\n"
#         msg += f"   Conversion time: {result['duration']:.2f} seconds\n"

#         if result["input_size"] > 0:
#             msg += f"   Original size: {result['input_size'] / 1024:.1f} KB\n"
#         msg += f"   Markdown size: {result['output_size'] / 1024:.1f} KB\n"

#         # 显示图片提取信息
#         if extract_images and "images_extracted" in result:
#             images_count = result["images_extracted"]
#             if images_count > 0:
#                 msg += f"   Images extracted: {images_count}\n"
#                 msg += f"   Images saved to: {os.path.join(os.path.dirname(result['output_file']), 'images')}\n"
#             else:
#                 msg += "   No images found in document\n"

#         # 显示Markdown内容的前几行作为预览
#         content_lines = result["markdown_content"].split("\n")
#         preview_lines = content_lines[:5]
#         if len(content_lines) > 5:
#             preview_lines.append("...")

#         msg += "\n[PREVIEW] First few lines of converted Markdown:\n"
#         for line in preview_lines:
#             msg += f"   {line}\n"
#     else:
#         msg = "[ERROR] Conversion failed!\n"
#         msg += f"   Error: {result['error']}"

#     return msg


if __name__ == "__main__":
    print("📄 Smart PDF Downloader MCP Tool")
    print("📝 Starting server with FastMCP...")

    if DOCLING_AVAILABLE:
        print("✅ Document conversion to Markdown is ENABLED (docling available)")
    else:
        print("❌ Document conversion to Markdown is DISABLED (docling not available)")
        print("   Install docling to enable: pip install docling")

    print("\nAvailable tools:")
    print(
        "  • download_files - Download files or move local files from natural language"
    )
    print("  • parse_download_urls - Extract URLs, local paths and destination paths")
    print("  • download_file_to - Download a specific file with options")
    print("  • move_file_to - Move a specific local file with options")
    print("  • convert_document_to_markdown - Convert documents to Markdown format")

    if DOCLING_AVAILABLE:
        print("\nSupported formats: PDF, DOCX, PPTX, HTML, TXT, MD")
        print("Features: Image extraction, Layout preservation, Automatic conversion")

    print("")

    # 运行服务器
    mcp.run()



================================================
FILE: tools/pdf_utils.py
================================================
"""
PDF utility functions for the DeepCode agent system.
"""

from pathlib import Path
import PyPDF2


def read_pdf_metadata(file_path: Path) -> dict:
    """Read PDF metadata with proper encoding handling."""
    try:
        print(f"\nAttempting to read PDF metadata from: {file_path}")
        with open(file_path, "rb") as file:
            pdf_reader = PyPDF2.PdfReader(file)
            info = pdf_reader.metadata
            first_page = pdf_reader.pages[0]
            text = first_page.extract_text()
            lines = text.split("\n")[:10]

            title = None
            authors = []

            if info:
                title = info.get("/Title", "").strip().replace("\x00", "")
                author = info.get("/Author", "").strip().replace("\x00", "")
                if author:
                    authors = [author]

            if not title and lines:
                title = lines[0].strip()

            if not authors and len(lines) > 1:
                for line in lines[1:3]:
                    if "author" in line.lower() or "by" in line.lower():
                        authors = [line.strip()]
                        break

            return {
                "title": title if title else "Unknown Title",
                "authors": authors if authors else ["Unknown Author"],
                "year": info.get("/CreationDate", "")[:4] if info else "Unknown Year",
                "first_lines": lines,
            }

    except Exception as e:
        print(f"\nError reading PDF: {str(e)}")
        return {
            "title": "Error reading PDF",
            "authors": ["Unknown"],
            "year": "Unknown",
            "first_lines": [],
        }



================================================
FILE: ui/__init__.py
================================================
"""
UI Module

Streamlit application user interface components module

Contains the following submodules:
- styles: CSS styles
- components: UI components
- layout: Page layout
- handlers: Event handlers
- streamlit_app: Main application
- app: Application entry
"""

__version__ = "1.0.0"
__author__ = "DeepCode Team"

# Import main components
from .layout import main_layout
from .components import display_header, display_features, display_status
from .handlers import initialize_session_state
from .styles import get_main_styles

# Import application main function
try:
    from .streamlit_app import main as streamlit_main
except ImportError:
    # Fallback to absolute import if relative import fails
    import sys
    import os

    sys.path.insert(0, os.path.dirname(__file__))
    from streamlit_app import main as streamlit_main

__all__ = [
    "main_layout",
    "display_header",
    "display_features",
    "display_status",
    "initialize_session_state",
    "get_main_styles",
    "streamlit_main",
]



================================================
FILE: ui/app.py
================================================
"""
DeepCode UI Application Entry Point

This file serves as the unified entry point for the UI module
"""

from .streamlit_app import main

# Directly export main function for external calls
__all__ = ["main"]

if __name__ == "__main__":
    main()



================================================
FILE: ui/handlers.py
================================================
"""
Streamlit Event Handlers Module

Contains all event handling and business logic
"""

import asyncio
import time
import os
import traceback
import atexit
import signal
from datetime import datetime
from typing import Dict, Any

import streamlit as st
import nest_asyncio
import concurrent.futures

# Import necessary modules
from mcp_agent.app import MCPApp
from workflows.agent_orchestration_engine import (
    execute_multi_agent_research_pipeline,
    execute_chat_based_planning_pipeline,
)


def _emergency_cleanup():
    """
    Emergency resource cleanup function
    Called when program exits abnormally
    """
    try:
        cleanup_resources()
    except Exception:
        pass  # Silent handling to avoid new exceptions during exit


def _signal_handler(signum, frame):
    """
    Signal handler for program termination signals
    """
    try:
        cleanup_resources()
    except Exception:
        pass
    finally:
        # Restore default signal handling and resend signal
        signal.signal(signum, signal.SIG_DFL)
        os.kill(os.getpid(), signum)


# Register exit cleanup function
atexit.register(_emergency_cleanup)


def _safe_register_signal_handlers():
    """Safely register signal handlers"""
    try:
        # Check if running in main thread
        import threading

        if threading.current_thread() is not threading.main_thread():
            return  # Signal handlers can only be registered in main thread

        # Try to register signal handlers
        signal.signal(signal.SIGTERM, _signal_handler)
        signal.signal(signal.SIGINT, _signal_handler)
        if hasattr(signal, "SIGBREAK"):  # Windows
            signal.signal(signal.SIGBREAK, _signal_handler)
    except (AttributeError, OSError, ValueError):
        # Some signals are not available on certain platforms or disabled in some environments
        # This is common in web frameworks like Streamlit
        pass


# Delayed signal handler registration to avoid import-time errors
try:
    _safe_register_signal_handlers()
except Exception:
    # If registration fails, silently ignore and don't affect app startup
    pass


async def process_input_async(
    input_source: str,
    input_type: str,
    enable_indexing: bool = True,
    progress_callback=None,
) -> Dict[str, Any]:
    """
    Process input asynchronously

    Args:
        input_source: Input source
        input_type: Input type
        enable_indexing: Whether to enable indexing functionality
        progress_callback: Progress callback function

    Returns:
        Processing result
    """
    try:
        # Create and use MCP app in the same async context
        app = MCPApp(name="paper_to_code")

        async with app.run() as agent_app:
            logger = agent_app.logger
            context = agent_app.context
            context.config.mcp.servers["filesystem"].args.extend([os.getcwd()])

            # Initialize progress
            if progress_callback:
                if input_type == "chat":
                    progress_callback(
                        5, "🚀 Initializing chat-based planning pipeline..."
                    )
                else:
                    progress_callback(5, "🚀 Initializing AI research engine...")

            # Choose pipeline based on input type
            if input_type == "chat":
                # Use chat-based planning pipeline for user requirements
                repo_result = await execute_chat_based_planning_pipeline(
                    input_source,  # User's coding requirements
                    logger,
                    progress_callback,
                    enable_indexing=enable_indexing,  # Pass indexing control parameter
                )
            else:
                # Use traditional multi-agent research pipeline for files/URLs
                repo_result = await execute_multi_agent_research_pipeline(
                    input_source,
                    logger,
                    progress_callback,
                    enable_indexing=enable_indexing,  # Pass indexing control parameter
                )

            return {
                "analysis_result": "Integrated into complete workflow",
                "download_result": "Integrated into complete workflow",
                "repo_result": repo_result,
                "status": "success",
            }

    except Exception as e:
        error_msg = str(e)
        traceback_msg = traceback.format_exc()

        return {"error": error_msg, "traceback": traceback_msg, "status": "error"}


def run_async_task(coro):
    """
    Helper function to run async tasks

    Args:
        coro: Coroutine object

    Returns:
        Task result
    """
    # Apply nest_asyncio to support nested event loops
    nest_asyncio.apply()

    # Save current Streamlit context
    try:
        from streamlit.runtime.scriptrunner import get_script_run_ctx
        from streamlit.runtime.scriptrunner.script_run_context import (
            SCRIPT_RUN_CONTEXT_ATTR_NAME,
        )

        current_ctx = get_script_run_ctx()
        context_available = True
    except ImportError:
        # If Streamlit context modules can't be imported, use fallback method
        current_ctx = None
        context_available = False

    def run_in_new_loop():
        """Run coroutine in new event loop"""
        # Set Streamlit context in new thread (if available)
        if context_available and current_ctx:
            try:
                import threading

                setattr(
                    threading.current_thread(),
                    SCRIPT_RUN_CONTEXT_ATTR_NAME,
                    current_ctx,
                )
            except Exception:
                pass  # Ignore context setting errors

        loop = None
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            result = loop.run_until_complete(coro)
            return result
        except Exception as e:
            raise e
        finally:
            # Clean up resources
            if loop:
                try:
                    loop.close()
                except Exception:
                    pass
            asyncio.set_event_loop(None)

            # Clean up thread context (if available)
            if context_available:
                try:
                    import threading

                    if hasattr(
                        threading.current_thread(), SCRIPT_RUN_CONTEXT_ATTR_NAME
                    ):
                        delattr(
                            threading.current_thread(), SCRIPT_RUN_CONTEXT_ATTR_NAME
                        )
                except Exception:
                    pass  # Ignore cleanup errors

            # Force garbage collection
            import gc

            gc.collect()

    # Use thread pool to run async task, avoiding event loop conflicts
    executor = None
    try:
        executor = concurrent.futures.ThreadPoolExecutor(
            max_workers=1, thread_name_prefix="deepcode_ctx_async"
        )
        future = executor.submit(run_in_new_loop)
        result = future.result(timeout=300)  # 5 minute timeout
        return result
    except concurrent.futures.TimeoutError:
        st.error("Processing timeout after 5 minutes. Please try again.")
        raise TimeoutError("Processing timeout")
    except Exception as e:
        # If thread pool execution fails, try direct execution
        st.warning(f"Threaded async execution failed: {e}, trying direct execution...")
        try:
            # Fallback method: run directly in current thread
            loop = None
            try:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                result = loop.run_until_complete(coro)
                return result
            finally:
                if loop:
                    try:
                        loop.close()
                    except Exception:
                        pass
                asyncio.set_event_loop(None)
                import gc

                gc.collect()
        except Exception as backup_error:
            st.error(f"All execution methods failed: {backup_error}")
            raise backup_error
    finally:
        # Ensure thread pool is properly closed
        if executor:
            try:
                executor.shutdown(wait=True, cancel_futures=True)
            except Exception:
                pass
        # Force garbage collection
        import gc

        gc.collect()


def run_async_task_simple(coro):
    """
    Simple async task runner, avoiding threading issues

    Args:
        coro: Coroutine object

    Returns:
        Task result
    """
    # Apply nest_asyncio to support nested event loops
    nest_asyncio.apply()

    try:
        # Try to run in current event loop
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # If current loop is running, use improved thread pool method
            import concurrent.futures
            import gc

            def run_in_thread():
                # Create new event loop and set as current thread's loop
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)
                try:
                    result = new_loop.run_until_complete(coro)
                    return result
                except Exception as e:
                    # Ensure exception information is properly passed
                    raise e
                finally:
                    # Ensure loop is properly closed
                    try:
                        new_loop.close()
                    except Exception:
                        pass
                    # Clear current thread's event loop reference
                    asyncio.set_event_loop(None)
                    # Force garbage collection
                    gc.collect()

            # Use context manager to ensure thread pool is properly closed
            executor = None
            try:
                executor = concurrent.futures.ThreadPoolExecutor(
                    max_workers=1, thread_name_prefix="deepcode_async"
                )
                future = executor.submit(run_in_thread)
                result = future.result(timeout=300)  # 5 minute timeout
                return result
            except concurrent.futures.TimeoutError:
                st.error(
                    "Processing timeout after 5 minutes. Please try again with a smaller file."
                )
                raise TimeoutError("Processing timeout")
            except Exception as e:
                st.error(f"Async processing error: {e}")
                raise e
            finally:
                # Ensure thread pool is properly closed
                if executor:
                    try:
                        executor.shutdown(wait=True, cancel_futures=True)
                    except Exception:
                        pass
                # Force garbage collection
                gc.collect()
        else:
            # Run directly in current loop
            return loop.run_until_complete(coro)
    except Exception:
        # Final fallback method: create new event loop
        loop = None
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            result = loop.run_until_complete(coro)
            return result
        except Exception as backup_error:
            st.error(f"All async methods failed: {backup_error}")
            raise backup_error
        finally:
            if loop:
                try:
                    loop.close()
                except Exception:
                    pass
            asyncio.set_event_loop(None)
            # Force garbage collection
            import gc

            gc.collect()


def handle_processing_workflow(
    input_source: str, input_type: str, enable_indexing: bool = True
) -> Dict[str, Any]:
    """
    Main processing function for workflow

    Args:
        input_source: Input source
        input_type: Input type
        enable_indexing: Whether to enable indexing functionality

    Returns:
        Processing result
    """
    from .components import (
        enhanced_progress_display_component,
        update_step_indicator,
        display_status,
    )

    # Display enhanced progress components
    chat_mode = input_type == "chat"
    progress_bar, status_text, step_indicators, workflow_steps = (
        enhanced_progress_display_component(enable_indexing, chat_mode)
    )

    # Step mapping: map progress percentages to step indices - adjust based on mode and indexing toggle
    if chat_mode:
        # Chat mode step mapping: Initialize -> Planning -> Setup -> Save Plan -> Implement
        step_mapping = {
            5: 0,  # Initialize
            30: 1,  # Planning (analyzing requirements)
            50: 2,  # Setup (creating workspace)
            70: 3,  # Save Plan (saving implementation plan)
            85: 4,  # Implement (generating code)
            100: 4,  # Complete
        }
    elif not enable_indexing:
        # Skip indexing-related steps progress mapping - fast mode order: Initialize -> Analyze -> Download -> Plan -> Implement
        step_mapping = {
            5: 0,  # Initialize
            10: 1,  # Analyze
            25: 2,  # Download
            40: 3,  # Plan (now prioritized over References, 40%)
            85: 4,  # Implement (skip References, Repos and Index)
            100: 4,  # Complete
        }
    else:
        # Full workflow step mapping - new order: Initialize -> Analyze -> Download -> Plan -> References -> Repos -> Index -> Implement
        step_mapping = {
            5: 0,  # Initialize
            10: 1,  # Analyze
            25: 2,  # Download
            40: 3,  # Plan (now 4th position, 40%)
            50: 4,  # References (now 5th position, conditional, 50%)
            60: 5,  # Repos (GitHub download)
            70: 6,  # Index (code indexing)
            85: 7,  # Implement (code implementation)
            100: 7,  # Complete
        }

    current_step = 0

    # Define enhanced progress callback function
    def update_progress(progress: int, message: str):
        nonlocal current_step

        # Update progress bar
        progress_bar.progress(progress)
        status_text.markdown(f"**{message}**")

        # Determine current step
        new_step = step_mapping.get(progress, current_step)
        if new_step != current_step:
            current_step = new_step
            update_step_indicator(
                step_indicators, workflow_steps, current_step, "active"
            )

        time.sleep(0.3)  # Brief pause for users to see progress changes

    # Step 1: Initialization
    if chat_mode:
        update_progress(5, "🚀 Initializing chat-based planning engine...")
    elif enable_indexing:
        update_progress(5, "🚀 Initializing AI research engine and loading models...")
    else:
        update_progress(
            5, "🚀 Initializing AI research engine (Fast mode - indexing disabled)..."
        )
    update_step_indicator(step_indicators, workflow_steps, 0, "active")

    # Start async processing with progress callback
    with st.spinner("🔄 Processing workflow stages..."):
        try:
            # First try using simple async processing method
            result = run_async_task_simple(
                process_input_async(
                    input_source, input_type, enable_indexing, update_progress
                )
            )
        except Exception as e:
            st.warning(f"Primary async method failed: {e}")
            # Fallback method: use original thread pool method
            try:
                result = run_async_task(
                    process_input_async(
                        input_source, input_type, enable_indexing, update_progress
                    )
                )
            except Exception as backup_error:
                st.error(f"Both async methods failed. Error: {backup_error}")
                return {
                    "status": "error",
                    "error": str(backup_error),
                    "traceback": traceback.format_exc(),
                }

    # Update final status based on results
    if result["status"] == "success":
        # Complete all steps
        update_progress(100, "✅ All processing stages completed successfully!")
        update_step_indicator(
            step_indicators, workflow_steps, len(workflow_steps), "completed"
        )

        # Display success information
        st.balloons()  # Add celebration animation
        if chat_mode:
            display_status(
                "🎉 Chat workflow completed! Your requirements have been analyzed and code has been generated.",
                "success",
            )
        elif enable_indexing:
            display_status(
                "🎉 Workflow completed! Your research paper has been successfully processed and code has been generated.",
                "success",
            )
        else:
            display_status(
                "🎉 Fast workflow completed! Your research paper has been processed (indexing skipped for faster processing).",
                "success",
            )

    else:
        # Processing failed
        update_progress(0, "❌ Processing failed - see error details below")
        update_step_indicator(step_indicators, workflow_steps, current_step, "error")
        display_status(
            f"❌ Processing encountered an error: {result.get('error', 'Unknown error')}",
            "error",
        )

    # Wait a moment for users to see completion status
    time.sleep(2.5)

    return result


def update_session_state_with_result(result: Dict[str, Any], input_type: str):
    """
    Update session state with result

    Args:
        result: Processing result
        input_type: Input type
    """
    if result["status"] == "success":
        # Save result to session state
        st.session_state.last_result = result
        st.session_state.show_results = True

        # Save to history
        st.session_state.results.append(
            {
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "input_type": input_type,
                "status": "success",
                "result": result,
            }
        )
    else:
        # Save error information to session state for display
        st.session_state.last_error = result.get("error", "Unknown error")

        # Save error to history
        st.session_state.results.append(
            {
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "input_type": input_type,
                "status": "error",
                "error": result.get("error", "Unknown error"),
            }
        )

    # Limit history to maximum 50 records
    if len(st.session_state.results) > 50:
        st.session_state.results = st.session_state.results[-50:]


def cleanup_temp_file(input_source: str, input_type: str):
    """
    Cleanup temporary file

    Args:
        input_source: Input source
        input_type: Input type
    """
    if input_type == "file" and input_source and os.path.exists(input_source):
        try:
            os.unlink(input_source)
        except Exception:
            pass


def handle_start_processing_button(input_source: str, input_type: str):
    """
    Handle start processing button click

    Args:
        input_source: Input source
        input_type: Input type
    """
    from .components import display_status

    st.session_state.processing = True

    # Get indexing toggle status
    enable_indexing = st.session_state.get("enable_indexing", True)

    try:
        # Process workflow
        result = handle_processing_workflow(input_source, input_type, enable_indexing)

        # Display result status
        if result["status"] == "success":
            display_status("All operations completed successfully! 🎉", "success")
        else:
            display_status("Error during processing", "error")

        # Update session state
        update_session_state_with_result(result, input_type)

    except Exception as e:
        # Handle exceptional cases
        st.error(f"Unexpected error during processing: {e}")
        result = {"status": "error", "error": str(e)}
        update_session_state_with_result(result, input_type)

    finally:
        # Reset state and clean up resources after processing
        st.session_state.processing = False

        # Clean up temporary files
        cleanup_temp_file(input_source, input_type)

        # Clean up system resources
        cleanup_resources()

        # Rerun to display results or errors
        st.rerun()


def handle_error_display():
    """Handle error display"""
    if hasattr(st.session_state, "last_error") and st.session_state.last_error:
        st.error(f"❌ Error: {st.session_state.last_error}")
        if st.button("🔄 Try Again", type="secondary", use_container_width=True):
            st.session_state.last_error = None
            st.session_state.task_counter += 1
            st.rerun()


def initialize_session_state():
    """Initialize session state"""
    if "processing" not in st.session_state:
        st.session_state.processing = False
    if "results" not in st.session_state:
        st.session_state.results = []
    if "current_step" not in st.session_state:
        st.session_state.current_step = 0
    if "task_counter" not in st.session_state:
        st.session_state.task_counter = 0
    if "show_results" not in st.session_state:
        st.session_state.show_results = False
    if "last_result" not in st.session_state:
        st.session_state.last_result = None
    if "last_error" not in st.session_state:
        st.session_state.last_error = None
    if "enable_indexing" not in st.session_state:
        st.session_state.enable_indexing = (
            False  # Default enable indexing functionality
        )


def cleanup_resources():
    """
    Clean up system resources to prevent memory leaks
    """
    try:
        import gc
        import threading
        import multiprocessing
        import asyncio
        import sys

        # 1. Clean up asyncio-related resources
        try:
            # Get current event loop (if exists)
            try:
                loop = asyncio.get_running_loop()
                # Cancel all pending tasks
                if loop and not loop.is_closed():
                    pending_tasks = [
                        task for task in asyncio.all_tasks(loop) if not task.done()
                    ]
                    if pending_tasks:
                        for task in pending_tasks:
                            if not task.cancelled():
                                task.cancel()
                        # Wait for task cancellation to complete
                        try:
                            if pending_tasks:
                                # Use timeout to avoid blocking too long
                                import time

                                time.sleep(0.1)
                        except Exception:
                            pass
            except RuntimeError:
                # No running event loop, continue with other cleanup
                pass
        except Exception:
            pass

        # 2. Force garbage collection
        gc.collect()

        # 3. Clean up active threads (except main thread)
        active_threads = threading.active_count()
        if active_threads > 1:
            # Wait some time for threads to naturally finish
            import time

            time.sleep(0.5)

        # 4. Clean up multiprocessing resources
        try:
            # Clean up possible multiprocessing resources
            if hasattr(multiprocessing, "active_children"):
                for child in multiprocessing.active_children():
                    if child.is_alive():
                        child.terminate()
                        child.join(timeout=1.0)
                        # If join times out, force kill
                        if child.is_alive():
                            try:
                                child.kill()
                                child.join(timeout=0.5)
                            except Exception:
                                pass

            # Clean up multiprocessing-related resource tracker
            try:
                import multiprocessing.resource_tracker

                if hasattr(multiprocessing.resource_tracker, "_resource_tracker"):
                    tracker = multiprocessing.resource_tracker._resource_tracker
                    if tracker and hasattr(tracker, "_stop"):
                        tracker._stop()
            except Exception:
                pass

        except Exception:
            pass

        # 5. Force clean up Python internal caches
        try:
            # Clean up some temporary objects in module cache
            import sys

            # Don't delete key modules, only clean up possible temporary resources
            if hasattr(sys, "_clear_type_cache"):
                sys._clear_type_cache()
        except Exception:
            pass

        # 6. Final garbage collection
        gc.collect()

    except Exception as e:
        # Silently handle cleanup errors to avoid affecting main flow
        # But can log errors in debug mode
        try:
            import logging

            logging.getLogger(__name__).debug(f"Resource cleanup warning: {e}")
        except Exception:
            pass



================================================
FILE: ui/layout.py
================================================
"""
Streamlit Page Layout Module

Contains main page layout and flow control
"""

import streamlit as st

from .components import (
    display_header,
    display_features,
    sidebar_control_panel,
    input_method_selector,
    results_display_component,
    footer_component,
)
from .handlers import (
    initialize_session_state,
    handle_start_processing_button,
    handle_error_display,
)
from .styles import get_main_styles


def setup_page_config():
    """Setup page configuration"""
    st.set_page_config(
        page_title="DeepCode - AI Research Engine",
        page_icon="🧬",
        layout="wide",
        initial_sidebar_state="expanded",
    )


def apply_custom_styles():
    """Apply custom styles"""
    st.markdown(get_main_styles(), unsafe_allow_html=True)


def render_main_content():
    """Render main content area"""
    # Display header and features
    display_header()
    display_features()
    st.markdown("---")

    # Display results if available
    if st.session_state.show_results and st.session_state.last_result:
        results_display_component(
            st.session_state.last_result, st.session_state.task_counter
        )
        st.markdown("---")
        return

    # Show input interface only when not displaying results
    if not st.session_state.show_results:
        render_input_interface()

    # Display error messages if any
    handle_error_display()


def render_input_interface():
    """Render input interface"""
    # Get input source and type
    input_source, input_type = input_method_selector(st.session_state.task_counter)

    # Processing button
    if input_source and not st.session_state.processing:
        if st.button("🚀 Start Processing", type="primary", use_container_width=True):
            handle_start_processing_button(input_source, input_type)

    elif st.session_state.processing:
        st.info("🔄 Processing in progress... Please wait.")
        st.warning("⚠️ Do not refresh the page or close the browser during processing.")

    elif not input_source:
        st.info("👆 Please upload a file or enter a URL to start processing.")


def render_sidebar():
    """Render sidebar"""
    return sidebar_control_panel()


def main_layout():
    """Main layout function"""
    # Initialize session state
    initialize_session_state()

    # Setup page configuration
    setup_page_config()

    # Apply custom styles
    apply_custom_styles()

    # Render sidebar
    sidebar_info = render_sidebar()

    # Render main content
    render_main_content()

    # Display footer
    footer_component()

    return sidebar_info



================================================
FILE: ui/streamlit_app.py
================================================
"""
DeepCode - AI Research Engine

Streamlit Web Interface Main Application File
"""

import os
import sys

# Disable .pyc file generation
os.environ["PYTHONDONTWRITEBYTECODE"] = "1"

# Add parent directory to path for module imports
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

# Import UI modules
from ui.layout import main_layout


def main():
    """
    Main function - Streamlit application entry

    All UI logic has been modularized into ui/ folder
    """
    # Run main layout
    sidebar_info = main_layout()

    # Additional global logic can be added here if needed

    return sidebar_info


if __name__ == "__main__":
    main()



================================================
FILE: utils/__init__.py
================================================
"""
Utils package for paper processing tools.
"""

from .file_processor import FileProcessor
from .dialogue_logger import (
    DialogueLogger,
    create_dialogue_logger,
    extract_paper_id_from_path,
)

__all__ = [
    "FileProcessor",
    "DialogueLogger",
    "create_dialogue_logger",
    "extract_paper_id_from_path",
]



================================================
FILE: utils/cli_interface.py
================================================
#!/usr/bin/env python3
"""
Professional CLI Interface Module
专业CLI界面模块 - 包含logo、颜色定义和界面组件
"""

import os
import time
import platform
from pathlib import Path
from typing import Optional
import tkinter as tk
from tkinter import filedialog


class Colors:
    """ANSI color codes for terminal styling"""

    HEADER = "\033[95m"
    OKBLUE = "\033[94m"
    OKCYAN = "\033[96m"
    OKGREEN = "\033[92m"
    WARNING = "\033[93m"
    FAIL = "\033[91m"
    ENDC = "\033[0m"
    BOLD = "\033[1m"
    UNDERLINE = "\033[4m"

    # Gradient colors
    PURPLE = "\033[35m"
    MAGENTA = "\033[95m"
    BLUE = "\033[34m"
    CYAN = "\033[36m"
    GREEN = "\033[32m"
    YELLOW = "\033[33m"


class CLIInterface:
    """Professional CLI interface with modern styling"""

    def __init__(self):
        self.uploaded_file = None
        self.is_running = True

        # Check tkinter availability
        self.tkinter_available = True
        try:
            import tkinter as tk

            # Test if tkinter can create a window (some systems have tkinter but no display)
            test_root = tk.Tk()
            test_root.withdraw()
            test_root.destroy()
        except Exception:
            self.tkinter_available = False

    def clear_screen(self):
        """Clear terminal screen"""
        os.system("cls" if os.name == "nt" else "clear")

    def print_logo(self):
        """Print a beautiful ASCII logo with gradient colors and tech elements"""
        # 确保每行总共79个字符（不包括颜色代码），边框完美对齐
        logo = f"""
{Colors.CYAN}╔═══════════════════════════════════════════════════════════════════════════════╗
║                                                                               ║
║  {Colors.BOLD}{Colors.MAGENTA}██████╗  ███████╗██████╗ ██████╗  ██████╗     █████╗ ██╗{Colors.CYAN}                ║
║  {Colors.BOLD}{Colors.PURPLE}██╔══██╗ ██╔════╝██╔══██╗██╔══██╗██╔═══██╗   ██╔══██╗██║{Colors.CYAN}                ║
║  {Colors.BOLD}{Colors.BLUE}██████╔╝ █████╗  ██████╔╝██████╔╝██║   ██║   ███████║██║{Colors.CYAN}                ║
║  {Colors.BOLD}{Colors.OKBLUE}██╔══██╗ ██╔══╝  ██╔═══╝ ██╔══██╗██║   ██║   ██╔══██║██║{Colors.CYAN}                ║
║  {Colors.BOLD}{Colors.OKCYAN}██║  ██║ ███████╗██║     ██║  ██║╚██████╔╝   ██║  ██║██║{Colors.CYAN}                ║
║  {Colors.BOLD}{Colors.GREEN}╚═╝  ╚═╝ ╚══════╝╚═╝     ╚═╝  ╚═╝ ╚═════╝    ╚═╝  ╚═╝╚═╝{Colors.CYAN}                ║
║                                                                               ║
║  {Colors.BOLD}{Colors.YELLOW}┌─────────────────────────────────────────────────────────────────────────┐{Colors.CYAN}   ║
║  {Colors.BOLD}{Colors.YELLOW}│  🤖 AI-POWERED RESEARCH PAPER REPRODUCTION ENGINE 🚀                  │{Colors.CYAN}   ║
║  {Colors.BOLD}{Colors.YELLOW}│  ⚡ INTELLIGENT • AUTOMATED • CUTTING-EDGE ⚡                        │{Colors.CYAN}   ║
║  {Colors.BOLD}{Colors.YELLOW}└─────────────────────────────────────────────────────────────────────────┘{Colors.CYAN}   ║
║                                                                               ║
║  {Colors.BOLD}{Colors.GREEN}💎 CORE CAPABILITIES:{Colors.ENDC}                                                        {Colors.CYAN}║
║    {Colors.BOLD}{Colors.OKCYAN}▶ Neural PDF Analysis & Code Extraction                                 {Colors.CYAN}║
║    {Colors.BOLD}{Colors.OKCYAN}▶ Advanced Document Processing Engine                                   {Colors.CYAN}║
║    {Colors.BOLD}{Colors.OKCYAN}▶ Multi-Format Support (PDF•DOCX•PPTX•HTML)                           {Colors.CYAN}║
║    {Colors.BOLD}{Colors.OKCYAN}▶ Smart File Upload Interface                                          {Colors.CYAN}║
║    {Colors.BOLD}{Colors.OKCYAN}▶ Automated Repository Management                                      {Colors.CYAN}║
║                                                                               ║
║  {Colors.BOLD}{Colors.PURPLE}🔬 TECH STACK: Python•AI•MCP•Docling•LLM                                   {Colors.CYAN}║
║                                                                               ║
╚═══════════════════════════════════════════════════════════════════════════════╝{Colors.ENDC}
"""
        print(logo)

    def print_welcome_banner(self):
        """Print welcome banner with version info"""
        banner = f"""
{Colors.BOLD}{Colors.CYAN}╔═══════════════════════════════════════════════════════════════════════════════╗
║                              WELCOME TO ReproAI                              ║
╠═══════════════════════════════════════════════════════════════════════════════╣
║                                                                               ║
║  {Colors.YELLOW}Version: 2.0.0 | Build: Professional Edition                                 {Colors.CYAN}║
║  {Colors.GREEN}Status: Ready | Engine: Initialized                                          {Colors.CYAN}║
║  {Colors.PURPLE}Author: AI Research Team | License: MIT                                      {Colors.CYAN}║
║                                                                               ║
╚═══════════════════════════════════════════════════════════════════════════════╝{Colors.ENDC}
"""
        print(banner)

    def print_separator(self, char="═", length=79, color=Colors.CYAN):
        """Print a styled separator line"""
        print(f"{color}{char * length}{Colors.ENDC}")

    def print_status(self, message: str, status_type: str = "info"):
        """Print status message with appropriate styling"""
        status_styles = {
            "success": f"{Colors.OKGREEN}✅",
            "error": f"{Colors.FAIL}❌",
            "warning": f"{Colors.WARNING}⚠️ ",
            "info": f"{Colors.OKBLUE}ℹ️ ",
            "processing": f"{Colors.YELLOW}⏳",
            "upload": f"{Colors.PURPLE}📁",
            "download": f"{Colors.CYAN}📥",
            "analysis": f"{Colors.MAGENTA}🔍",
        }

        icon = status_styles.get(status_type, status_styles["info"])
        print(f"{icon} {Colors.BOLD}{message}{Colors.ENDC}")

    def create_menu(self):
        """Create an interactive menu"""
        menu = f"""
{Colors.BOLD}{Colors.CYAN}╔═══════════════════════════════════════════════════════════════════════════════╗
║                                MAIN MENU                                      ║
╠═══════════════════════════════════════════════════════════════════════════════╣
║                                                                               ║
║  {Colors.OKGREEN}🌐 [U] Process URL       {Colors.CYAN}│  {Colors.PURPLE}📁 [F] Upload File    {Colors.CYAN}│  {Colors.FAIL}❌ [Q] Quit{Colors.CYAN}         ║
║                                                                               ║
║  {Colors.YELLOW}📝 Enter a research paper URL (arXiv, IEEE, ACM, etc.)                      {Colors.CYAN}║
║  {Colors.YELLOW}   or upload a PDF/DOC file for intelligent analysis                        {Colors.CYAN}║
║                                                                               ║
║  {Colors.OKCYAN}💡 Tip: Press 'F' to open file browser or 'U' to enter URL manually        {Colors.CYAN}║
║                                                                               ║
╚═══════════════════════════════════════════════════════════════════════════════╝{Colors.ENDC}
"""
        print(menu)

    def get_user_input(self):
        """Get user input with styled prompt"""
        print(f"\n{Colors.BOLD}{Colors.OKCYAN}➤ Your choice: {Colors.ENDC}", end="")
        return input().strip().lower()

    def upload_file_gui(self) -> Optional[str]:
        """Modern file upload interface using tkinter with cross-platform compatibility"""
        # Check if tkinter is available
        if not self.tkinter_available:
            self.print_status("GUI file dialog not available on this system", "warning")
            self.print_status("Using manual file path input instead", "info")
            return self._get_manual_file_path()

        def select_file():
            try:
                # Create a hidden root window
                root = tk.Tk()
                root.withdraw()  # Hide the main window

                # Platform-specific configurations
                system = platform.system()

                if system == "Darwin":  # macOS
                    # macOS specific settings
                    try:
                        root.call("wm", "attributes", ".", "-topmost", True)
                    except Exception:
                        pass

                    # macOS compatible file types
                    file_types = [
                        ("PDF Files", ".pdf"),
                        ("Word Documents", ".docx .doc"),
                        ("PowerPoint Files", ".pptx .ppt"),
                        ("HTML Files", ".html .htm"),
                        ("Text Files", ".txt .md"),
                        ("All Files", ".*"),
                    ]
                else:
                    # Windows and Linux
                    root.attributes("-topmost", True)

                    # Windows/Linux compatible file types
                    file_types = [
                        ("PDF Files", "*.pdf"),
                        ("Word Documents", "*.docx;*.doc"),
                        ("PowerPoint Files", "*.pptx;*.ppt"),
                        ("HTML Files", "*.html;*.htm"),
                        ("Text Files", "*.txt;*.md"),
                        ("All Files", "*.*"),
                    ]

                # Set window title
                root.title("Repro-AI - File Selector")

                try:
                    # Open file dialog with platform-appropriate settings
                    file_path = filedialog.askopenfilename(
                        title="Select Research Paper File",
                        filetypes=file_types,
                        initialdir=os.getcwd(),
                    )
                except Exception as e:
                    self.print_status(f"File dialog error: {str(e)}", "error")
                    return None
                finally:
                    # Clean up
                    try:
                        root.destroy()
                    except Exception:
                        pass

                return file_path

            except Exception as e:
                # Fallback: destroy root if it exists
                try:
                    if "root" in locals():
                        root.destroy()
                except Exception:
                    pass

                # Print error and suggest alternative
                self.print_status(f"GUI file dialog failed: {str(e)}", "error")
                self.print_status(
                    "Please use manual file path input instead", "warning"
                )
                return self._get_manual_file_path()

        self.print_status("Opening file browser dialog...", "upload")
        file_path = select_file()

        if file_path:
            # Validate file
            if not os.path.exists(file_path):
                self.print_status("File not found!", "error")
                return None

            file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB
            file_ext = Path(file_path).suffix.lower()

            # Display file info with beautiful formatting
            file_name = Path(file_path).name
            directory = str(Path(file_path).parent)

            # Truncate long paths for display
            if len(file_name) > 50:
                display_name = file_name[:47] + "..."
            else:
                display_name = file_name

            if len(directory) > 49:
                display_dir = "..." + directory[-46:]
            else:
                display_dir = directory

            print(f"""
{Colors.OKGREEN}╔═══════════════════════════════════════════════════════════════════════════════╗
║                               FILE SELECTED                                   ║
╠═══════════════════════════════════════════════════════════════════════════════╣
║                                                                               ║
║  {Colors.BOLD}📄 File Name:{Colors.ENDC} {Colors.CYAN}{display_name:<50}{Colors.OKGREEN}║
║  {Colors.BOLD}📁 Directory:{Colors.ENDC} {Colors.YELLOW}{display_dir:<49}{Colors.OKGREEN}║
║  {Colors.BOLD}📊 File Size:{Colors.ENDC} {Colors.PURPLE}{file_size:.2f} MB{Colors.OKGREEN}                                      ║
║  {Colors.BOLD}🔖 File Type:{Colors.ENDC} {Colors.MAGENTA}{file_ext.upper():<50}{Colors.OKGREEN}║
║                                                                               ║
╚═══════════════════════════════════════════════════════════════════════════════╝{Colors.ENDC}
""")

            self.print_status(f"File successfully selected: {file_name}", "success")
            return file_path
        else:
            self.print_status("No file selected", "warning")
            return None

    def _get_manual_file_path(self) -> Optional[str]:
        """Fallback method for manual file path input when GUI fails"""
        print(
            f"\n{Colors.BOLD}{Colors.CYAN}╔═══════════════════════════════════════════════════════════════════════════════╗"
        )
        print(
            "║                           MANUAL FILE INPUT                                   ║"
        )
        print(
            f"╚═══════════════════════════════════════════════════════════════════════════════╝{Colors.ENDC}"
        )

        print(f"\n{Colors.YELLOW}📝 Supported file types:{Colors.ENDC}")
        print(f"   {Colors.CYAN}• PDF files (.pdf)")
        print(f"   {Colors.CYAN}• Word documents (.docx, .doc)")
        print(f"   {Colors.CYAN}• PowerPoint files (.pptx, .ppt)")
        print(f"   {Colors.CYAN}• HTML files (.html, .htm)")
        print(f"   {Colors.CYAN}• Text files (.txt, .md){Colors.ENDC}")

        print(
            f"\n{Colors.BOLD}{Colors.OKCYAN}📁 Enter file path (or drag & drop): {Colors.ENDC}",
            end="",
        )
        file_path = input().strip()

        # Clean up the path (remove quotes if present)
        file_path = file_path.strip("\"'")

        if file_path:
            # Expand user directory if needed
            file_path = os.path.expanduser(file_path)

            # Check if file exists
            if os.path.exists(file_path):
                self.print_status(
                    f"File found: {os.path.basename(file_path)}", "success"
                )
                return file_path
            else:
                self.print_status("File not found at the specified path", "error")
                return None
        else:
            self.print_status("No file path provided", "warning")
            return None

    def get_url_input(self) -> str:
        """Get URL input with validation and examples"""
        print(
            f"\n{Colors.BOLD}{Colors.CYAN}╔═══════════════════════════════════════════════════════════════════════════════╗"
        )
        print(
            "║                              URL INPUT                                        ║"
        )
        print(
            f"╚═══════════════════════════════════════════════════════════════════════════════╝{Colors.ENDC}"
        )

        print(f"\n{Colors.YELLOW}📝 Supported URL Examples:{Colors.ENDC}")
        print(f"   {Colors.CYAN}• arXiv: https://arxiv.org/pdf/2403.00813")
        print(f"   {Colors.CYAN}• arXiv: @https://arxiv.org/pdf/2403.00813")
        print(f"   {Colors.CYAN}• IEEE:  https://ieeexplore.ieee.org/document/...")
        print(f"   {Colors.CYAN}• ACM:   https://dl.acm.org/doi/...")
        print(
            f"   {Colors.CYAN}• Direct PDF: https://example.com/paper.pdf{Colors.ENDC}"
        )

        print(
            f"\n{Colors.BOLD}{Colors.OKCYAN}🌐 Enter paper URL: {Colors.ENDC}", end=""
        )
        url = input().strip()

        if url:
            # Basic URL validation
            if any(
                domain in url.lower()
                for domain in ["arxiv.org", "ieee", "acm.org", ".pdf", "researchgate"]
            ):
                self.print_status(f"URL received: {url}", "success")
                return url
            else:
                self.print_status("URL appears valid, proceeding...", "info")
                return url
        else:
            self.print_status("No URL provided", "warning")
            return ""

    def show_progress_bar(self, message: str, duration: float = 2.0):
        """Show a progress animation with enhanced styling"""
        print(f"\n{Colors.YELLOW}{message}{Colors.ENDC}")

        # Progress bar animation with different styles
        bar_length = 50
        for i in range(bar_length + 1):
            percent = (i / bar_length) * 100
            filled = "█" * i
            empty = "░" * (bar_length - i)

            # Color gradient effect
            if percent < 33:
                color = Colors.FAIL
            elif percent < 66:
                color = Colors.WARNING
            else:
                color = Colors.OKGREEN

            print(
                f"\r{color}[{filled}{empty}] {percent:6.1f}%{Colors.ENDC}",
                end="",
                flush=True,
            )
            time.sleep(duration / bar_length)

        print(f"\n{Colors.OKGREEN}✅ {message} completed!{Colors.ENDC}\n")

    def show_spinner(self, message: str, duration: float = 1.0):
        """Show a spinner animation"""
        spinner_chars = "⠋⠙⠹⠸⠼⠴⠦⠧⠇⠏"
        end_time = time.time() + duration

        while time.time() < end_time:
            for char in spinner_chars:
                print(
                    f"\r{Colors.CYAN}{char} {Colors.BOLD}{message}{Colors.ENDC}",
                    end="",
                    flush=True,
                )
                time.sleep(0.1)
                if time.time() >= end_time:
                    break

        print(f"\r{Colors.OKGREEN}✅ {Colors.BOLD}{message} - Done!{Colors.ENDC}")

    def print_results_header(self):
        """Print results section header"""
        header = f"""
{Colors.OKGREEN}╔═══════════════════════════════════════════════════════════════════════════════╗
║                             PROCESSING RESULTS                               ║
╚═══════════════════════════════════════════════════════════════════════════════╝{Colors.ENDC}
"""
        print(header)

    def print_error_box(self, title: str, error_msg: str):
        """Print error message in a styled box"""
        print(f"""
{Colors.FAIL}╔═══════════════════════════════════════════════════════════════════════════════╗
║                                  ERROR                                        ║
╠═══════════════════════════════════════════════════════════════════════════════╣
║                                                                               ║
║  {Colors.BOLD}Title: {title:<66}{Colors.FAIL}║
║  {Colors.BOLD}Error: {error_msg:<66}{Colors.FAIL}║
║                                                                               ║
╚═══════════════════════════════════════════════════════════════════════════════╝{Colors.ENDC}
""")

    def print_goodbye(self):
        """Print goodbye message"""
        goodbye = f"""
{Colors.BOLD}{Colors.YELLOW}╔═══════════════════════════════════════════════════════════════════════════════╗
║                                GOODBYE!                                       ║
╠═══════════════════════════════════════════════════════════════════════════════╣
║                                                                               ║
║  {Colors.CYAN}Thank you for using ReproAI!                                               {Colors.YELLOW}║
║  {Colors.GREEN}🌟 Star us on GitHub: https://github.com/your-repo                        {Colors.YELLOW}║
║  {Colors.PURPLE}📧 Contact: support@reproai.com                                          {Colors.YELLOW}║
║  {Colors.MAGENTA}🐛 Report issues: https://github.com/your-repo/issues                    {Colors.YELLOW}║
║                                                                               ║
║  {Colors.OKGREEN}✨ Happy coding! See you next time! ✨                                   {Colors.YELLOW}║
║                                                                               ║
╚═══════════════════════════════════════════════════════════════════════════════╝{Colors.ENDC}
"""
        print(goodbye)

    def ask_continue(self) -> bool:
        """Ask user if they want to continue"""
        print(
            f"\n{Colors.BOLD}{Colors.CYAN}Press Enter to continue or 'q' to quit: {Colors.ENDC}",
            end="",
        )
        choice = input().strip().lower()
        return choice not in ["q", "quit", "exit"]



================================================
FILE: utils/dialogue_logger.py
================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Comprehensive Dialogue Logger for Code Implementation Workflow
Logs complete conversation rounds with detailed formatting and paper-specific organization
"""

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List


class DialogueLogger:
    """
    Comprehensive dialogue logger for code implementation workflow
    Captures complete conversation rounds with proper formatting and organization
    """

    def __init__(self, paper_id: str, base_path: str = None):
        """
        Initialize dialogue logger for a specific paper

        Args:
            paper_id: Paper identifier (e.g., "1", "2", etc.)
            base_path: Base path for logs (defaults to agent_folders structure)
        """
        self.paper_id = paper_id
        self.base_path = (
            base_path
            or "/data2/bjdwhzzh/project-hku/Code-Agent2.0/Code-Agent/deepcode-mcp/agent_folders"
        )
        self.log_directory = os.path.join(
            self.base_path, "papers", str(paper_id), "logs"
        )

        # Create log directory if it doesn't exist
        Path(self.log_directory).mkdir(parents=True, exist_ok=True)

        # Session tracking (initialize before log file creation)
        self.round_counter = 0
        self.session_start_time = datetime.now()
        self.current_round_data = {}

        # Generate log filename with timestamp
        timestamp = self.session_start_time.strftime("%Y%m%d_%H%M%S")
        self.log_filename = f"dialogue_log_{timestamp}.md"
        self.log_filepath = os.path.join(self.log_directory, self.log_filename)

        # Initialize log file with header
        self._initialize_log_file()

        print(f"📝 Dialogue Logger initialized for Paper {paper_id}")
        print(f"📁 Log file: {self.log_filepath}")

    def _initialize_log_file(self):
        """Initialize the log file with header information"""
        header = f"""# Code Implementation Dialogue Log

**Paper ID:** {self.paper_id}
**Session Start:** {self.session_start_time.strftime('%Y-%m-%d %H:%M:%S')}
**Log File:** {self.log_filename}

---

## Session Overview

This log contains the complete conversation rounds between the user and assistant during the code implementation workflow. Each round includes:

- System prompts and user messages
- Assistant responses with tool calls
- Tool execution results
- Implementation progress markers

---

"""
        try:
            with open(self.log_filepath, "w", encoding="utf-8") as f:
                f.write(header)
        except Exception as e:
            print(f"⚠️ Failed to initialize log file: {e}")

    def start_new_round(
        self, round_type: str = "implementation", context: Dict[str, Any] = None
    ):
        """
        Start a new dialogue round

        Args:
            round_type: Type of round (implementation, summary, error_handling, etc.)
            context: Additional context information (may include 'iteration' to sync with workflow)
        """
        # Use iteration from context if provided, otherwise increment round_counter
        if context and "iteration" in context:
            self.round_counter = context["iteration"]
        else:
            self.round_counter += 1

        self.current_round_data = {
            "round_number": self.round_counter,
            "round_type": round_type,
            "start_time": datetime.now(),
            "context": context or {},
            "messages": [],
            "tool_calls": [],
            "results": [],
            "metadata": {},
        }

        print(f"🔄 Starting Round {self.round_counter}: {round_type}")

    def log_system_prompt(self, prompt: str, prompt_type: str = "system"):
        """
        Log system prompt or instructions

        Args:
            prompt: System prompt content
            prompt_type: Type of prompt (system, instruction, etc.)
        """
        if not self.current_round_data:
            self.start_new_round("system_setup")

        self.current_round_data["messages"].append(
            {
                "role": "system",
                "type": prompt_type,
                "content": prompt,
                "timestamp": datetime.now().isoformat(),
            }
        )

    def log_user_message(self, message: str, message_type: str = "user_input"):
        """
        Log user message

        Args:
            message: User message content
            message_type: Type of message (user_input, feedback, guidance, etc.)
        """
        if not self.current_round_data:
            self.start_new_round("user_interaction")

        self.current_round_data["messages"].append(
            {
                "role": "user",
                "type": message_type,
                "content": message,
                "timestamp": datetime.now().isoformat(),
            }
        )

    def log_assistant_response(
        self, response: str, response_type: str = "assistant_response"
    ):
        """
        Log assistant response

        Args:
            response: Assistant response content
            response_type: Type of response (assistant_response, analysis, etc.)
        """
        if not self.current_round_data:
            self.start_new_round("assistant_interaction")

        self.current_round_data["messages"].append(
            {
                "role": "assistant",
                "type": response_type,
                "content": response,
                "timestamp": datetime.now().isoformat(),
            }
        )

    def log_tool_calls(self, tool_calls: List[Dict[str, Any]]):
        """
        Log tool calls made by the assistant

        Args:
            tool_calls: List of tool calls with id, name, and input
        """
        if not self.current_round_data:
            self.start_new_round("tool_execution")

        for tool_call in tool_calls:
            self.current_round_data["tool_calls"].append(
                {
                    "id": tool_call.get("id", ""),
                    "name": tool_call.get("name", ""),
                    "input": tool_call.get("input", {}),
                    "timestamp": datetime.now().isoformat(),
                }
            )

    def log_tool_results(self, tool_results: List[Dict[str, Any]]):
        """
        Log tool execution results

        Args:
            tool_results: List of tool results with tool_name and result
        """
        if not self.current_round_data:
            self.start_new_round("tool_results")

        for result in tool_results:
            self.current_round_data["results"].append(
                {
                    "tool_name": result.get("tool_name", ""),
                    "result": result.get("result", ""),
                    "timestamp": datetime.now().isoformat(),
                }
            )

    def log_metadata(self, key: str, value: Any):
        """
        Log metadata information

        Args:
            key: Metadata key
            value: Metadata value
        """
        if not self.current_round_data:
            self.start_new_round("metadata")

        self.current_round_data["metadata"][key] = value

    def log_memory_optimization(
        self,
        messages_before: List[Dict],
        messages_after: List[Dict],
        optimization_stats: Dict[str, Any],
        approach: str = "memory_optimization",
    ):
        """
        Log memory optimization details including before/after message content

        Args:
            messages_before: Messages before optimization
            messages_after: Messages after optimization
            optimization_stats: Statistics about the optimization
            approach: Optimization approach used
        """
        if not self.current_round_data:
            self.start_new_round("memory_optimization")

        # Calculate what was removed/kept
        removed_count = len(messages_before) - len(messages_after)
        compression_ratio = (
            (removed_count / len(messages_before) * 100) if messages_before else 0
        )

        # Log the optimization details
        optimization_data = {
            "approach": approach,
            "messages_before_count": len(messages_before),
            "messages_after_count": len(messages_after),
            "messages_removed_count": removed_count,
            "compression_ratio": f"{compression_ratio:.1f}%",
            "optimization_stats": optimization_stats,
            "timestamp": datetime.now().isoformat(),
        }

        # Store the optimization data
        if "memory_optimizations" not in self.current_round_data:
            self.current_round_data["memory_optimizations"] = []

        self.current_round_data["memory_optimizations"].append(
            {
                "optimization_data": optimization_data,
                "messages_before": messages_before,
                "messages_after": messages_after,
            }
        )

        # Log metadata
        self.log_metadata("memory_optimization", optimization_data)

        print(
            f"🧹 Memory optimization logged: {len(messages_before)} → {len(messages_after)} messages ({compression_ratio:.1f}% compression)"
        )

    def complete_round(self, summary: str = "", status: str = "completed"):
        """
        Complete the current round and write to log file

        Args:
            summary: Round summary
            status: Round completion status
        """
        if not self.current_round_data:
            print("⚠️ No active round to complete")
            return

        self.current_round_data["end_time"] = datetime.now()
        self.current_round_data["duration"] = (
            self.current_round_data["end_time"] - self.current_round_data["start_time"]
        ).total_seconds()
        self.current_round_data["summary"] = summary
        self.current_round_data["status"] = status

        # Write round to log file
        self._write_round_to_log()

        print(f"✅ Round {self.round_counter} completed: {status}")

        # Clear current round data
        self.current_round_data = {}

    def _write_round_to_log(self):
        """Write the current round data to the log file in markdown format"""
        try:
            with open(self.log_filepath, "a", encoding="utf-8") as f:
                round_data = self.current_round_data

                # Round header
                f.write(
                    f"\n## Round {round_data['round_number']}: {round_data['round_type'].title()}\n\n"
                )
                f.write(
                    f"**Start Time:** {round_data['start_time'].strftime('%Y-%m-%d %H:%M:%S')}\n"
                )
                f.write(
                    f"**End Time:** {round_data['end_time'].strftime('%Y-%m-%d %H:%M:%S')}\n"
                )
                f.write(f"**Duration:** {round_data['duration']:.2f} seconds\n")
                f.write(f"**Status:** {round_data['status']}\n\n")

                # Context information
                if round_data.get("context"):
                    f.write("### Context\n\n")
                    for key, value in round_data["context"].items():
                        f.write(f"- **{key}:** {value}\n")
                    f.write("\n")

                # Messages
                if round_data.get("messages"):
                    f.write("### Messages\n\n")
                    for i, msg in enumerate(round_data["messages"], 1):
                        role_emoji = {
                            "system": "🔧",
                            "user": "👤",
                            "assistant": "🤖",
                        }.get(msg["role"], "📝")
                        f.write(
                            f"#### {role_emoji} {msg['role'].title()} Message {i}\n\n"
                        )
                        f.write(f"**Type:** {msg['type']}\n")
                        f.write(f"**Timestamp:** {msg['timestamp']}\n\n")
                        f.write("```\n")
                        f.write(msg["content"])
                        f.write("\n```\n\n")

                # Tool calls
                if round_data.get("tool_calls"):
                    f.write("### Tool Calls\n\n")
                    for i, tool_call in enumerate(round_data["tool_calls"], 1):
                        f.write(f"#### 🛠️ Tool Call {i}: {tool_call['name']}\n\n")
                        f.write(f"**ID:** {tool_call['id']}\n")
                        f.write(f"**Timestamp:** {tool_call['timestamp']}\n\n")
                        f.write("**Input:**\n")
                        f.write("```json\n")
                        f.write(
                            json.dumps(tool_call["input"], indent=2, ensure_ascii=False)
                        )
                        f.write("\n```\n\n")

                # Tool results
                if round_data.get("results"):
                    f.write("### Tool Results\n\n")
                    for i, result in enumerate(round_data["results"], 1):
                        f.write(f"#### 📊 Result {i}: {result['tool_name']}\n\n")
                        f.write(f"**Timestamp:** {result['timestamp']}\n\n")
                        f.write("**Result:**\n")
                        f.write("```\n")
                        f.write(str(result["result"]))
                        f.write("\n```\n\n")

                # Memory Optimizations
                if round_data.get("memory_optimizations"):
                    f.write("### Memory Optimizations\n\n")
                    for i, opt in enumerate(round_data["memory_optimizations"], 1):
                        opt_data = opt["optimization_data"]
                        messages_before = opt["messages_before"]
                        messages_after = opt["messages_after"]

                        f.write(f"#### 🧹 Memory Optimization {i}\n\n")
                        f.write(f"**Approach:** {opt_data['approach']}\n")
                        f.write(
                            f"**Messages Before:** {opt_data['messages_before_count']}\n"
                        )
                        f.write(
                            f"**Messages After:** {opt_data['messages_after_count']}\n"
                        )
                        f.write(
                            f"**Messages Removed:** {opt_data['messages_removed_count']}\n"
                        )
                        f.write(
                            f"**Compression Ratio:** {opt_data['compression_ratio']}\n"
                        )
                        f.write(f"**Timestamp:** {opt_data['timestamp']}\n\n")

                        # Show optimization stats
                        if opt_data.get("optimization_stats"):
                            f.write("**Optimization Statistics:**\n")
                            f.write("```json\n")
                            f.write(
                                json.dumps(
                                    opt_data["optimization_stats"],
                                    indent=2,
                                    ensure_ascii=False,
                                )
                            )
                            f.write("\n```\n\n")

                        # Show messages before optimization (limited to last 5 for readability)
                        if messages_before:
                            f.write("**Messages Before Optimization (last 5):**\n\n")
                            for j, msg in enumerate(messages_before[-5:], 1):
                                role = msg.get("role", "unknown")
                                content = msg.get("content", "")
                                # Truncate very long messages
                                if len(content) > 3000:
                                    content = content[:3000] + "...[truncated]"
                                f.write(
                                    f"- **{role} {j}:** {content[:3000]}{'...' if len(content) > 100 else ''}\n"
                                )
                            f.write("\n")

                        # Show messages after optimization
                        if messages_after:
                            f.write("**Messages After Optimization:**\n\n")
                            for j, msg in enumerate(messages_after, 1):
                                role = msg.get("role", "unknown")
                                content = msg.get("content", "")
                                # Truncate very long messages
                                if len(content) > 3000:
                                    content = content[:3000] + "...[truncated]"
                                f.write(
                                    f"- **{role} {j}:** {content[:3000]}{'...' if len(content) > 100 else ''}\n"
                                )
                            f.write("\n")

                        # Show what was removed
                        if len(messages_before) > len(messages_after):
                            removed_messages = (
                                messages_before[: -len(messages_after)]
                                if messages_after
                                else messages_before
                            )
                            f.write(
                                f"**Messages Removed ({len(removed_messages)}):**\n\n"
                            )
                            for j, msg in enumerate(
                                removed_messages[-3:], 1
                            ):  # Show last 3 removed
                                role = msg.get("role", "unknown")
                                content = msg.get("content", "")
                                if len(content) > 3000:
                                    content = content[:3000] + "...[truncated]"
                                f.write(f"- **{role} {j}:** {content}\n")
                            f.write("\n")

                        f.write("\n")

                # Metadata
                if round_data.get("metadata"):
                    f.write("### Metadata\n\n")
                    for key, value in round_data["metadata"].items():
                        if (
                            key != "memory_optimization"
                        ):  # Skip memory optimization metadata as it's shown above
                            f.write(f"- **{key}:** {value}\n")
                    f.write("\n")

                # Summary
                if round_data.get("summary"):
                    f.write("### Summary\n\n")
                    f.write(round_data["summary"])
                    f.write("\n\n")

                # Separator
                f.write("---\n\n")

        except Exception as e:
            print(f"⚠️ Failed to write round to log: {e}")

    def log_complete_exchange(
        self,
        system_prompt: str = "",
        user_message: str = "",
        assistant_response: str = "",
        tool_calls: List[Dict] = None,
        tool_results: List[Dict] = None,
        round_type: str = "exchange",
        context: Dict = None,
        summary: str = "",
    ):
        """
        Log a complete exchange in a single call

        Args:
            system_prompt: System prompt (optional)
            user_message: User message
            assistant_response: Assistant response
            tool_calls: Tool calls made
            tool_results: Tool execution results
            round_type: Type of round
            context: Additional context
            summary: Round summary
        """
        self.start_new_round(round_type, context)

        if system_prompt:
            self.log_system_prompt(system_prompt)

        if user_message:
            self.log_user_message(user_message)

        if assistant_response:
            self.log_assistant_response(assistant_response)

        if tool_calls:
            self.log_tool_calls(tool_calls)

        if tool_results:
            self.log_tool_results(tool_results)

        self.complete_round(summary)

    def get_session_stats(self) -> Dict[str, Any]:
        """Get session statistics"""
        return {
            "paper_id": self.paper_id,
            "session_start": self.session_start_time.isoformat(),
            "total_rounds": self.round_counter,
            "log_file": self.log_filepath,
            "session_duration": (
                datetime.now() - self.session_start_time
            ).total_seconds(),
        }

    def finalize_session(self, final_summary: str = ""):
        """
        Finalize the logging session

        Args:
            final_summary: Final session summary
        """
        try:
            with open(self.log_filepath, "a", encoding="utf-8") as f:
                f.write("\n## Session Summary\n\n")
                f.write(f"**Total Rounds:** {self.round_counter}\n")
                f.write(
                    f"**Session Duration:** {(datetime.now() - self.session_start_time).total_seconds():.2f} seconds\n"
                )
                f.write(
                    f"**End Time:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
                )

                if final_summary:
                    f.write("### Final Summary\n\n")
                    f.write(final_summary)
                    f.write("\n\n")

                f.write("---\n\n")
                f.write("*End of Session*\n")

        except Exception as e:
            print(f"⚠️ Failed to finalize session: {e}")

        print(f"🎯 Session finalized: {self.round_counter} rounds logged")


# Utility functions for easy integration
def create_dialogue_logger(paper_id: str, base_path: str = None) -> DialogueLogger:
    """
    Create a dialogue logger for a specific paper

    Args:
        paper_id: Paper identifier
        base_path: Base path for logs

    Returns:
        DialogueLogger instance
    """
    return DialogueLogger(paper_id, base_path)


def extract_paper_id_from_path(path: str) -> str:
    """
    Extract paper ID from a file path

    Args:
        path: File path containing paper information

    Returns:
        Paper ID string
    """
    # Extract paper ID from path like "/data2/.../papers/1/initial_plan.txt"
    parts = path.split("/")
    for i, part in enumerate(parts):
        if part == "papers" and i + 1 < len(parts):
            return parts[i + 1]
    return "unknown"


# Example usage
if __name__ == "__main__":
    # Test the dialogue logger
    logger = DialogueLogger("1")

    # Log a complete exchange
    logger.log_complete_exchange(
        system_prompt="You are a code implementation assistant.",
        user_message="Implement the transformer model",
        assistant_response="I'll implement the transformer model step by step.",
        tool_calls=[
            {"id": "1", "name": "write_file", "input": {"filename": "transformer.py"}}
        ],
        tool_results=[
            {"tool_name": "write_file", "result": "File created successfully"}
        ],
        round_type="implementation",
        context={"files_implemented": 1},
        summary="Successfully implemented transformer model",
    )

    # Test memory optimization logging
    logger.start_new_round(
        "memory_optimization", {"trigger_reason": "write_file_detected"}
    )

    # Mock messages before and after optimization
    messages_before = [
        {"role": "user", "content": "Original message 1"},
        {"role": "assistant", "content": "Original response 1"},
        {"role": "user", "content": "Original message 2"},
        {"role": "assistant", "content": "Original response 2"},
        {"role": "user", "content": "Original message 3"},
    ]

    messages_after = [
        {"role": "user", "content": "Original message 1"},
        {"role": "assistant", "content": "Original response 1"},
        {"role": "user", "content": "Original message 3"},
    ]

    # Mock optimization stats
    optimization_stats = {
        "implemented_files_tracked": 2,
        "current_round": 5,
        "concise_mode_active": True,
    }

    # Log memory optimization
    logger.log_memory_optimization(
        messages_before=messages_before,
        messages_after=messages_after,
        optimization_stats=optimization_stats,
        approach="clear_after_write_file",
    )

    logger.complete_round("Memory optimization test completed")

    # Finalize session
    logger.finalize_session(
        "Test session with memory optimization logging completed successfully"
    )

    print("✅ Dialogue logger test completed with memory optimization")



================================================
FILE: utils/file_processor.py
================================================
"""
File processing utilities for handling paper files and related operations.
"""

import json
import os
import re
from typing import Dict, List, Optional, Union


class FileProcessor:
    """
    A class to handle file processing operations including path extraction and file reading.
    """

    @staticmethod
    def extract_file_path(file_info: Union[str, Dict]) -> Optional[str]:
        """
        Extract paper directory path from the input information.

        Args:
            file_info: Either a JSON string or a dictionary containing file information

        Returns:
            Optional[str]: The extracted paper directory path or None if not found
        """
        try:
            # Handle direct file path input
            if isinstance(file_info, str):
                # Check if it's a file path (existing or not)
                if file_info.endswith(
                    (".md", ".pdf", ".txt", ".docx", ".doc", ".html", ".htm")
                ):
                    # It's a file path, return the directory
                    return os.path.dirname(os.path.abspath(file_info))
                elif os.path.exists(file_info):
                    if os.path.isfile(file_info):
                        return os.path.dirname(os.path.abspath(file_info))
                    elif os.path.isdir(file_info):
                        return os.path.abspath(file_info)

                # Try to parse as JSON
                try:
                    info_dict = json.loads(file_info)
                except json.JSONDecodeError:
                    # 尝试从文本中提取JSON
                    info_dict = FileProcessor.extract_json_from_text(file_info)
                    if not info_dict:
                        # If not JSON and doesn't look like a file path, raise error
                        raise ValueError(
                            f"Input is neither a valid file path nor JSON: {file_info}"
                        )
            else:
                info_dict = file_info

            # Extract paper path from dictionary
            paper_path = info_dict.get("paper_path")
            if not paper_path:
                raise ValueError("No paper_path found in input dictionary")

            # Get the directory path instead of the file path
            paper_dir = os.path.dirname(paper_path)

            # Convert to absolute path if relative
            if not os.path.isabs(paper_dir):
                paper_dir = os.path.abspath(paper_dir)

            return paper_dir

        except (AttributeError, TypeError) as e:
            raise ValueError(f"Invalid input format: {str(e)}")

    @staticmethod
    def find_markdown_file(directory: str) -> Optional[str]:
        """
        Find the first markdown file in the given directory.

        Args:
            directory: Directory path to search

        Returns:
            Optional[str]: Path to the markdown file or None if not found
        """
        if not os.path.isdir(directory):
            return None

        for file in os.listdir(directory):
            if file.endswith(".md"):
                return os.path.join(directory, file)
        return None

    @staticmethod
    def parse_markdown_sections(content: str) -> List[Dict[str, Union[str, int, List]]]:
        """
        Parse markdown content and organize it by sections based on headers.

        Args:
            content: The markdown content to parse

        Returns:
            List[Dict]: A list of sections, each containing:
                - level: The header level (1-6)
                - title: The section title
                - content: The section content
                - subsections: List of subsections
        """
        # Split content into lines
        lines = content.split("\n")
        sections = []
        current_section = None
        current_content = []

        for line in lines:
            # Check if line is a header
            header_match = re.match(r"^(#{1,6})\s+(.+)$", line)

            if header_match:
                # If we were building a section, save its content
                if current_section is not None:
                    current_section["content"] = "\n".join(current_content).strip()
                    sections.append(current_section)

                # Start a new section
                level = len(header_match.group(1))
                title = header_match.group(2).strip()
                current_section = {
                    "level": level,
                    "title": title,
                    "content": "",
                    "subsections": [],
                }
                current_content = []
            elif current_section is not None:
                current_content.append(line)

        # Don't forget to save the last section
        if current_section is not None:
            current_section["content"] = "\n".join(current_content).strip()
            sections.append(current_section)

        return FileProcessor._organize_sections(sections)

    @staticmethod
    def _organize_sections(sections: List[Dict]) -> List[Dict]:
        """
        Organize sections into a hierarchical structure based on their levels.

        Args:
            sections: List of sections with their levels

        Returns:
            List[Dict]: Organized hierarchical structure of sections
        """
        result = []
        section_stack = []

        for section in sections:
            while section_stack and section_stack[-1]["level"] >= section["level"]:
                section_stack.pop()

            if section_stack:
                section_stack[-1]["subsections"].append(section)
            else:
                result.append(section)

            section_stack.append(section)

        return result

    @staticmethod
    async def read_file_content(file_path: str) -> str:
        """
        Read the content of a file asynchronously.

        Args:
            file_path: Path to the file to read

        Returns:
            str: The content of the file

        Raises:
            FileNotFoundError: If the file doesn't exist
            IOError: If there's an error reading the file
        """
        try:
            # Ensure the file exists
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"File not found: {file_path}")

            # Check if file is actually a PDF by reading the first few bytes
            with open(file_path, "rb") as f:
                header = f.read(8)
                if header.startswith(b"%PDF"):
                    raise IOError(
                        f"File {file_path} is a PDF file, not a text file. Please convert it to markdown format or use PDF processing tools."
                    )

            # Read file content
            # Note: Using async with would be better for large files
            # but for simplicity and compatibility, using regular file reading
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            return content

        except UnicodeDecodeError as e:
            raise IOError(
                f"Error reading file {file_path}: File encoding is not UTF-8. Original error: {str(e)}"
            )
        except Exception as e:
            raise IOError(f"Error reading file {file_path}: {str(e)}")

    @staticmethod
    def format_section_content(section: Dict) -> str:
        """
        Format a section's content with standardized spacing and structure.

        Args:
            section: Dictionary containing section information

        Returns:
            str: Formatted section content
        """
        # Start with section title
        formatted = f"\n{'#' * section['level']} {section['title']}\n"

        # Add section content if it exists
        if section["content"]:
            formatted += f"\n{section['content'].strip()}\n"

        # Process subsections
        if section["subsections"]:
            # Add a separator before subsections if there's content
            if section["content"]:
                formatted += "\n---\n"

            # Process each subsection
            for subsection in section["subsections"]:
                formatted += FileProcessor.format_section_content(subsection)

        # Add section separator
        formatted += "\n" + "=" * 80 + "\n"

        return formatted

    @staticmethod
    def standardize_output(sections: List[Dict]) -> str:
        """
        Convert structured sections into a standardized string format.

        Args:
            sections: List of section dictionaries

        Returns:
            str: Standardized string output
        """
        output = []

        # Process each top-level section
        for section in sections:
            output.append(FileProcessor.format_section_content(section))

        # Join all sections with clear separation
        return "\n".join(output)

    @classmethod
    async def process_file_input(
        cls, file_input: Union[str, Dict], base_dir: str = None
    ) -> Dict:
        """
        Process file input information and return the structured content.

        Args:
            file_input: File input information (JSON string, dict, or direct file path)
            base_dir: Optional base directory to use for creating paper directories (for sync support)

        Returns:
            Dict: The structured content with sections and standardized text
        """
        try:
            # 首先尝试从字符串中提取markdown文件路径
            if isinstance(file_input, str):
                import re

                file_path_match = re.search(r"`([^`]+\.md)`", file_input)
                if file_path_match:
                    paper_path = file_path_match.group(1)
                    file_input = {"paper_path": paper_path}

            # Extract paper directory path
            paper_dir = cls.extract_file_path(file_input)

            # If base_dir is provided, adjust paper_dir to be relative to base_dir
            if base_dir and paper_dir:
                # If paper_dir is using default location, move it to base_dir
                if paper_dir.endswith(("deepcode_lab", "agent_folders")):
                    paper_dir = base_dir
                else:
                    # Extract the relative part and combine with base_dir
                    paper_name = os.path.basename(paper_dir)
                    # 保持原始目录名不变，不做任何替换
                    paper_dir = os.path.join(base_dir, "papers", paper_name)

                # Ensure the directory exists
                os.makedirs(paper_dir, exist_ok=True)

            if not paper_dir:
                raise ValueError("Could not determine paper directory path")

            # Get the actual file path
            file_path = None
            if isinstance(file_input, str):
                # 尝试解析为JSON（处理下载结果）
                try:
                    parsed_json = json.loads(file_input)
                    if isinstance(parsed_json, dict) and "paper_path" in parsed_json:
                        file_path = parsed_json.get("paper_path")
                        # 如果文件不存在，尝试查找markdown文件
                        if file_path and not os.path.exists(file_path):
                            paper_dir = os.path.dirname(file_path)
                            if os.path.isdir(paper_dir):
                                file_path = cls.find_markdown_file(paper_dir)
                                if not file_path:
                                    raise ValueError(
                                        f"No markdown file found in directory: {paper_dir}"
                                    )
                    else:
                        raise ValueError("Invalid JSON format: missing paper_path")
                except json.JSONDecodeError:
                    # 尝试从文本中提取JSON（处理包含额外文本的下载结果）
                    extracted_json = cls.extract_json_from_text(file_input)
                    if extracted_json and "paper_path" in extracted_json:
                        file_path = extracted_json.get("paper_path")
                        # 如果文件不存在，尝试查找markdown文件
                        if file_path and not os.path.exists(file_path):
                            paper_dir = os.path.dirname(file_path)
                            if os.path.isdir(paper_dir):
                                file_path = cls.find_markdown_file(paper_dir)
                                if not file_path:
                                    raise ValueError(
                                        f"No markdown file found in directory: {paper_dir}"
                                    )
                    else:
                        # 不是JSON，按文件路径处理
                        # Check if it's a file path (existing or not)
                        if file_input.endswith(
                            (".md", ".pdf", ".txt", ".docx", ".doc", ".html", ".htm")
                        ):
                            if os.path.exists(file_input):
                                file_path = file_input
                            else:
                                # File doesn't exist, try to find markdown in the directory
                                file_path = cls.find_markdown_file(paper_dir)
                                if not file_path:
                                    raise ValueError(
                                        f"No markdown file found in directory: {paper_dir}"
                                    )
                        elif os.path.exists(file_input):
                            if os.path.isfile(file_input):
                                file_path = file_input
                            elif os.path.isdir(file_input):
                                # If it's a directory, find the markdown file
                                file_path = cls.find_markdown_file(file_input)
                                if not file_path:
                                    raise ValueError(
                                        f"No markdown file found in directory: {file_input}"
                                    )
                        else:
                            raise ValueError(f"Invalid input: {file_input}")
            else:
                # Dictionary input
                file_path = file_input.get("paper_path")
                # If the file doesn't exist, try to find markdown in the directory
                if file_path and not os.path.exists(file_path):
                    paper_dir = os.path.dirname(file_path)
                    if os.path.isdir(paper_dir):
                        file_path = cls.find_markdown_file(paper_dir)
                        if not file_path:
                            raise ValueError(
                                f"No markdown file found in directory: {paper_dir}"
                            )

            if not file_path:
                raise ValueError("No valid file path found")

            # Read file content
            content = await cls.read_file_content(file_path)

            # Parse and structure the content
            structured_content = cls.parse_markdown_sections(content)

            # Generate standardized text output
            standardized_text = cls.standardize_output(structured_content)

            return {
                "paper_dir": paper_dir,
                "file_path": file_path,
                "sections": structured_content,
                "standardized_text": standardized_text,
            }

        except Exception as e:
            raise ValueError(f"Error processing file input: {str(e)}")

    @staticmethod
    def extract_json_from_text(text: str) -> Optional[Dict]:
        """
        Extract JSON from text that may contain markdown code blocks or other content.

        Args:
            text: Text that may contain JSON

        Returns:
            Optional[Dict]: Extracted JSON as dictionary or None if not found
        """
        import re

        # Try to find JSON in markdown code blocks
        json_pattern = r"```json\s*(\{.*?\})\s*```"
        match = re.search(json_pattern, text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except json.JSONDecodeError:
                pass

        # Try to find standalone JSON
        json_pattern = r"(\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\})"
        matches = re.findall(json_pattern, text, re.DOTALL)
        for match in matches:
            try:
                parsed = json.loads(match)
                if isinstance(parsed, dict) and "paper_path" in parsed:
                    return parsed
            except json.JSONDecodeError:
                continue

        return None



================================================
FILE: utils/llm_utils.py
================================================
"""
LLM utility functions for DeepCode project.

This module provides common LLM-related utilities to avoid circular imports
and reduce code duplication across the project.
"""

import os
import yaml
from typing import Any, Type, Dict, Tuple

# Import LLM classes
from mcp_agent.workflows.llm.augmented_llm_anthropic import AnthropicAugmentedLLM
from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM


def get_preferred_llm_class(config_path: str = "mcp_agent.secrets.yaml") -> Type[Any]:
    """
    Automatically select the LLM class based on API key availability in configuration.

    Reads from YAML config file and returns AnthropicAugmentedLLM if anthropic.api_key
    is available, otherwise returns OpenAIAugmentedLLM.

    Args:
        config_path: Path to the YAML configuration file

    Returns:
        class: The preferred LLM class
    """
    try:
        # Try to read the configuration file
        if os.path.exists(config_path):
            with open(config_path, "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)

            # Check for anthropic API key in config
            anthropic_config = config.get("anthropic", {})
            anthropic_key = anthropic_config.get("api_key", "")

            if anthropic_key and anthropic_key.strip() and not anthropic_key == "":
                # print("🤖 Using AnthropicAugmentedLLM (Anthropic API key found in config)")
                return AnthropicAugmentedLLM
            else:
                # print("🤖 Using OpenAIAugmentedLLM (Anthropic API key not configured)")
                return OpenAIAugmentedLLM
        else:
            print(f"🤖 Config file {config_path} not found, using OpenAIAugmentedLLM")
            return OpenAIAugmentedLLM

    except Exception as e:
        print(f"🤖 Error reading config file {config_path}: {e}")
        print("🤖 Falling back to OpenAIAugmentedLLM")
        return OpenAIAugmentedLLM


def get_default_models(config_path: str = "mcp_agent.config.yaml"):
    """
    Get default models from configuration file.

    Args:
        config_path: Path to the configuration file

    Returns:
        dict: Dictionary with 'anthropic' and 'openai' default models
    """
    try:
        if os.path.exists(config_path):
            with open(config_path, "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)

            # Handle null values in config sections
            anthropic_config = config.get("anthropic") or {}
            openai_config = config.get("openai") or {}

            anthropic_model = anthropic_config.get(
                "default_model", "claude-sonnet-4-20250514"
            )
            openai_model = openai_config.get("default_model", "o3-mini")

            return {"anthropic": anthropic_model, "openai": openai_model}
        else:
            print(f"Config file {config_path} not found, using default models")
            return {"anthropic": "claude-sonnet-4-20250514", "openai": "o3-mini"}

    except Exception as e:
        print(f"❌Error reading config file {config_path}: {e}")
        return {"anthropic": "claude-sonnet-4-20250514", "openai": "o3-mini"}


def get_document_segmentation_config(
    config_path: str = "mcp_agent.config.yaml",
) -> Dict[str, Any]:
    """
    Get document segmentation configuration from config file.

    Args:
        config_path: Path to the main configuration file

    Returns:
        Dict containing segmentation configuration with default values
    """
    try:
        if os.path.exists(config_path):
            with open(config_path, "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)

            # Get document segmentation config with defaults
            seg_config = config.get("document_segmentation", {})
            return {
                "enabled": seg_config.get("enabled", True),
                "size_threshold_chars": seg_config.get("size_threshold_chars", 50000),
            }
        else:
            print(
                f"📄 Config file {config_path} not found, using default segmentation settings"
            )
            return {"enabled": True, "size_threshold_chars": 50000}

    except Exception as e:
        print(f"📄 Error reading segmentation config from {config_path}: {e}")
        print("📄 Using default segmentation settings")
        return {"enabled": True, "size_threshold_chars": 50000}


def should_use_document_segmentation(
    document_content: str, config_path: str = "mcp_agent.config.yaml"
) -> Tuple[bool, str]:
    """
    Determine whether to use document segmentation based on configuration and document size.

    Args:
        document_content: The content of the document to analyze
        config_path: Path to the configuration file

    Returns:
        Tuple of (should_segment, reason) where:
        - should_segment: Boolean indicating whether to use segmentation
        - reason: String explaining the decision
    """
    seg_config = get_document_segmentation_config(config_path)

    if not seg_config["enabled"]:
        return False, "Document segmentation disabled in configuration"

    doc_size = len(document_content)
    threshold = seg_config["size_threshold_chars"]

    if doc_size > threshold:
        return (
            True,
            f"Document size ({doc_size:,} chars) exceeds threshold ({threshold:,} chars)",
        )
    else:
        return (
            False,
            f"Document size ({doc_size:,} chars) below threshold ({threshold:,} chars)",
        )


def get_adaptive_agent_config(
    use_segmentation: bool, search_server_names: list = None
) -> Dict[str, list]:
    """
    Get adaptive agent configuration based on whether to use document segmentation.

    Args:
        use_segmentation: Whether to include document-segmentation server
        search_server_names: Base search server names (from get_search_server_names)

    Returns:
        Dict containing server configurations for different agents
    """
    if search_server_names is None:
        search_server_names = []

    # Base configuration
    config = {
        "concept_analysis": [],
        "algorithm_analysis": search_server_names.copy(),
        "code_planner": search_server_names.copy(),
    }

    # Add document-segmentation server if needed
    if use_segmentation:
        config["concept_analysis"] = ["document-segmentation"]
        if "document-segmentation" not in config["algorithm_analysis"]:
            config["algorithm_analysis"].append("document-segmentation")
        if "document-segmentation" not in config["code_planner"]:
            config["code_planner"].append("document-segmentation")
    else:
        config["concept_analysis"] = ["filesystem"]
        if "filesystem" not in config["algorithm_analysis"]:
            config["algorithm_analysis"].append("filesystem")
        if "filesystem" not in config["code_planner"]:
            config["code_planner"].append("filesystem")

    return config


def get_adaptive_prompts(use_segmentation: bool) -> Dict[str, str]:
    """
    Get appropriate prompt versions based on segmentation usage.

    Args:
        use_segmentation: Whether to use segmented reading prompts

    Returns:
        Dict containing prompt configurations
    """
    # Import here to avoid circular imports
    from prompts.code_prompts import (
        PAPER_CONCEPT_ANALYSIS_PROMPT,
        PAPER_ALGORITHM_ANALYSIS_PROMPT,
        CODE_PLANNING_PROMPT,
        PAPER_CONCEPT_ANALYSIS_PROMPT_TRADITIONAL,
        PAPER_ALGORITHM_ANALYSIS_PROMPT_TRADITIONAL,
        CODE_PLANNING_PROMPT_TRADITIONAL,
    )

    if use_segmentation:
        return {
            "concept_analysis": PAPER_CONCEPT_ANALYSIS_PROMPT,
            "algorithm_analysis": PAPER_ALGORITHM_ANALYSIS_PROMPT,
            "code_planning": CODE_PLANNING_PROMPT,
        }
    else:
        return {
            "concept_analysis": PAPER_CONCEPT_ANALYSIS_PROMPT_TRADITIONAL,
            "algorithm_analysis": PAPER_ALGORITHM_ANALYSIS_PROMPT_TRADITIONAL,
            "code_planning": CODE_PLANNING_PROMPT_TRADITIONAL,
        }



================================================
FILE: utils/simple_llm_logger.py
================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
超简化LLM响应日志记录器
专注于记录LLM回复的核心内容，配置简单易用
"""

import json
import os
import yaml
from datetime import datetime
from pathlib import Path
from typing import Dict, Any


class SimpleLLMLogger:
    """超简化的LLM响应日志记录器"""

    def __init__(self, config_path: str = "mcp_agent.config.yaml"):
        """
        初始化日志记录器

        Args:
            config_path: 配置文件路径
        """
        self.config = self._load_config(config_path)
        self.llm_config = self.config.get("llm_logger", {})

        # 如果禁用则直接返回
        if not self.llm_config.get("enabled", True):
            self.enabled = False
            return

        self.enabled = True
        self._setup_logger()

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """加载配置文件"""
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        except Exception as e:
            print(f"⚠️ 配置文件加载失败: {e}，使用默认配置")
            return self._get_default_config()

    def _get_default_config(self) -> Dict[str, Any]:
        """获取默认配置"""
        return {
            "llm_logger": {
                "enabled": True,
                "output_format": "json",
                "log_level": "basic",
                "log_directory": "logs/llm_responses",
                "filename_pattern": "llm_responses_{timestamp}.jsonl",
                "include_models": ["claude-sonnet-4", "gpt-4", "o3-mini"],
                "min_response_length": 50,
            }
        }

    def _setup_logger(self):
        """设置日志记录器"""
        log_dir = self.llm_config.get("log_directory", "logs/llm_responses")

        # 创建日志目录
        Path(log_dir).mkdir(parents=True, exist_ok=True)

        # 生成日志文件名
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename_pattern = self.llm_config.get(
            "filename_pattern", "llm_responses_{timestamp}.jsonl"
        )
        self.log_file = os.path.join(
            log_dir, filename_pattern.format(timestamp=timestamp)
        )

        print(f"📝 LLM响应日志: {self.log_file}")

    def log_response(self, content: str, model: str = "", agent: str = "", **kwargs):
        """
        记录LLM响应 - 简化版本

        Args:
            content: LLM响应内容
            model: 模型名称
            agent: Agent名称
            **kwargs: 其他可选信息
        """
        if not self.enabled:
            return

        # 检查是否应该记录
        if not self._should_log(content, model):
            return

        # 构建日志记录
        log_entry = self._build_entry(content, model, agent, kwargs)

        # 写入日志
        self._write_log(log_entry)

        # 控制台显示
        self._console_log(content, model, agent)

    def _should_log(self, content: str, model: str) -> bool:
        """检查是否应该记录"""
        # 检查长度
        min_length = self.llm_config.get("min_response_length", 50)
        if len(content) < min_length:
            return False

        # 检查模型
        include_models = self.llm_config.get("include_models", [])
        if include_models and not any(m in model for m in include_models):
            return False

        return True

    def _build_entry(self, content: str, model: str, agent: str, extra: Dict) -> Dict:
        """构建日志条目"""
        log_level = self.llm_config.get("log_level", "basic")

        if log_level == "basic":
            # 基础级别：只记录核心内容
            return {
                "timestamp": datetime.now().isoformat(),
                "content": content,
                "model": model,
            }
        else:
            # 详细级别：包含更多信息
            entry = {
                "timestamp": datetime.now().isoformat(),
                "content": content,
                "model": model,
                "agent": agent,
            }
            # 添加额外信息
            if "token_usage" in extra:
                entry["tokens"] = extra["token_usage"]
            if "session_id" in extra:
                entry["session"] = extra["session_id"]
            return entry

    def _write_log(self, entry: Dict):
        """写入日志文件"""
        output_format = self.llm_config.get("output_format", "json")

        try:
            with open(self.log_file, "a", encoding="utf-8") as f:
                if output_format == "json":
                    f.write(json.dumps(entry, ensure_ascii=False) + "\n")
                elif output_format == "text":
                    timestamp = entry.get("timestamp", "")
                    model = entry.get("model", "")
                    content = entry.get("content", "")
                    f.write(f"[{timestamp}] {model}: {content}\n\n")
                elif output_format == "markdown":
                    timestamp = entry.get("timestamp", "")
                    model = entry.get("model", "")
                    content = entry.get("content", "")
                    f.write(f"**{timestamp}** | {model}\n\n{content}\n\n---\n\n")
        except Exception as e:
            print(f"⚠️ 写入日志失败: {e}")

    def _console_log(self, content: str, model: str, agent: str):
        """控制台简要显示"""
        preview = content[:80] + "..." if len(content) > 80 else content
        print(f"🤖 {model} ({agent}): {preview}")


# 全局实例
_global_logger = None


def get_llm_logger() -> SimpleLLMLogger:
    """获取全局LLM日志记录器实例"""
    global _global_logger
    if _global_logger is None:
        _global_logger = SimpleLLMLogger()
    return _global_logger


def log_llm_response(content: str, model: str = "", agent: str = "", **kwargs):
    """便捷函数：记录LLM响应"""
    logger = get_llm_logger()
    logger.log_response(content, model, agent, **kwargs)


# 示例使用
if __name__ == "__main__":
    # 测试日志记录
    log_llm_response(
        content="这是一个测试的LLM响应内容，用于验证简化日志记录器的功能是否正常工作。",
        model="claude-sonnet-4-20250514",
        agent="TestAgent",
    )

    print("✅ 简化LLM日志测试完成")



================================================
FILE: workflows/__init__.py
================================================
"""
Intelligent Agent Orchestration Workflows for Research-to-Code Automation.

This package provides advanced AI-driven workflow orchestration capabilities
for automated research analysis and code implementation synthesis.
"""

from .agent_orchestration_engine import (
    run_research_analyzer,
    run_resource_processor,
    run_code_analyzer,
    github_repo_download,
    paper_reference_analyzer,
    execute_multi_agent_research_pipeline,
    paper_code_preparation,  # Deprecated, for backward compatibility
)

from .code_implementation_workflow import CodeImplementationWorkflow

__all__ = [
    # Initial workflows
    "run_research_analyzer",
    "run_resource_processor",
    "run_code_analyzer",
    "github_repo_download",
    "paper_reference_analyzer",
    "execute_multi_agent_research_pipeline",  # Main multi-agent pipeline function
    "paper_code_preparation",  # Deprecated, for backward compatibility
    # Code implementation workflows
    "CodeImplementationWorkflow",
]



================================================
FILE: workflows/code_implementation_workflow.py
================================================
"""
Paper Code Implementation Workflow - MCP-compliant Iterative Development

Features:
1. File Tree Creation
2. Code Implementation - Based on aisi-basic-agent iterative development

MCP Architecture:
- MCP Server: tools/code_implementation_server.py
- MCP Client: Called through mcp_agent framework
- Configuration: mcp_agent.config.yaml
"""

import asyncio
import json
import logging
import os
import sys
import time
import yaml
from pathlib import Path
from typing import Dict, Any, Optional, List

# MCP Agent imports
from mcp_agent.agents.agent import Agent

# Local imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from prompts.code_prompts import STRUCTURE_GENERATOR_PROMPT
from prompts.code_prompts import (
    GENERAL_CODE_IMPLEMENTATION_SYSTEM_PROMPT,
)
from workflows.agents import CodeImplementationAgent
from workflows.agents.memory_agent_concise import ConciseMemoryAgent
from config.mcp_tool_definitions import get_mcp_tools
from utils.llm_utils import get_preferred_llm_class, get_default_models
# DialogueLogger removed - no longer needed


class CodeImplementationWorkflow:
    """
    Paper Code Implementation Workflow Manager

    Uses standard MCP architecture:
    1. Connect to code-implementation server via MCP client
    2. Use MCP protocol for tool calls
    3. Support workspace management and operation history tracking
    """

    # ==================== 1. Class Initialization and Configuration (Infrastructure Layer) ====================

    def __init__(self, config_path: str = "mcp_agent.secrets.yaml"):
        """Initialize workflow with configuration"""
        self.config_path = config_path
        self.api_config = self._load_api_config()
        self.default_models = get_default_models("mcp_agent.config.yaml")
        self.logger = self._create_logger()
        self.mcp_agent = None
        self.enable_read_tools = (
            True  # Default value, will be overridden by run_workflow parameter
        )

    def _load_api_config(self) -> Dict[str, Any]:
        """Load API configuration from YAML file"""
        try:
            with open(self.config_path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        except Exception as e:
            raise Exception(f"Failed to load API config: {e}")

    def _create_logger(self) -> logging.Logger:
        """Create and configure logger"""
        logger = logging.getLogger(__name__)
        # Don't add handlers to child loggers - let them propagate to root
        logger.setLevel(logging.INFO)
        return logger

    def _read_plan_file(self, plan_file_path: str) -> str:
        """Read implementation plan file"""
        plan_path = Path(plan_file_path)
        if not plan_path.exists():
            raise FileNotFoundError(
                f"Implementation plan file not found: {plan_file_path}"
            )

        with open(plan_path, "r", encoding="utf-8") as f:
            return f.read()

    def _check_file_tree_exists(self, target_directory: str) -> bool:
        """Check if file tree structure already exists"""
        code_directory = os.path.join(target_directory, "generate_code")
        return os.path.exists(code_directory) and len(os.listdir(code_directory)) > 0

    # ==================== 2. Public Interface Methods (External API Layer) ====================

    async def run_workflow(
        self,
        plan_file_path: str,
        target_directory: Optional[str] = None,
        pure_code_mode: bool = False,
        enable_read_tools: bool = True,
    ):
        """Run complete workflow - Main public interface"""
        # Set the read tools configuration
        self.enable_read_tools = enable_read_tools

        try:
            plan_content = self._read_plan_file(plan_file_path)

            if target_directory is None:
                target_directory = str(Path(plan_file_path).parent)

            # Calculate code directory for workspace alignment
            code_directory = os.path.join(target_directory, "generate_code")

            self.logger.info("=" * 80)
            self.logger.info("🚀 STARTING CODE IMPLEMENTATION WORKFLOW")
            self.logger.info("=" * 80)
            self.logger.info(f"📄 Plan file: {plan_file_path}")
            self.logger.info(f"📂 Plan file parent: {target_directory}")
            self.logger.info(f"🎯 Code directory (MCP workspace): {code_directory}")
            self.logger.info(
                f"⚙️  Read tools: {'ENABLED' if self.enable_read_tools else 'DISABLED'}"
            )
            self.logger.info("=" * 80)

            results = {}

            # Check if file tree exists
            if self._check_file_tree_exists(target_directory):
                self.logger.info("File tree exists, skipping creation")
                results["file_tree"] = "Already exists, skipped creation"
            else:
                self.logger.info("Creating file tree...")
                results["file_tree"] = await self.create_file_structure(
                    plan_content, target_directory
                )

            # Code implementation
            if pure_code_mode:
                self.logger.info("Starting pure code implementation...")
                results["code_implementation"] = await self.implement_code_pure(
                    plan_content, target_directory, code_directory
                )
            else:
                pass

            self.logger.info("Workflow execution successful")

            return {
                "status": "success",
                "plan_file": plan_file_path,
                "target_directory": target_directory,
                "code_directory": os.path.join(target_directory, "generate_code"),
                "results": results,
                "mcp_architecture": "standard",
            }

        except Exception as e:
            self.logger.error(f"Workflow execution failed: {e}")

            return {"status": "error", "message": str(e), "plan_file": plan_file_path}
        finally:
            await self._cleanup_mcp_agent()

    async def create_file_structure(
        self, plan_content: str, target_directory: str
    ) -> str:
        """Create file tree structure based on implementation plan"""
        self.logger.info("Starting file tree creation...")

        structure_agent = Agent(
            name="StructureGeneratorAgent",
            instruction=STRUCTURE_GENERATOR_PROMPT,
            server_names=["command-executor"],
        )

        async with structure_agent:
            creator = await structure_agent.attach_llm(
                get_preferred_llm_class(self.config_path)
            )

            message = f"""Analyze the following implementation plan and generate shell commands to create the file tree structure.

Target Directory: {target_directory}/generate_code

Implementation Plan:
{plan_content}

Tasks:
1. Find the file tree structure in the implementation plan
2. Generate shell commands (mkdir -p, touch) to create that structure
3. Use the execute_commands tool to run the commands and create the file structure

Requirements:
- Use mkdir -p to create directories
- Use touch to create files
- Include __init__.py file for Python packages
- Use relative paths to the target directory
- Execute commands to actually create the file structure"""

            result = await creator.generate_str(message=message)
            self.logger.info("File tree structure creation completed")
            return result

    async def implement_code_pure(
        self, plan_content: str, target_directory: str, code_directory: str = None
    ) -> str:
        """Pure code implementation - focus on code writing without testing"""
        self.logger.info("Starting pure code implementation (no testing)...")

        # Use provided code_directory or calculate it (for backwards compatibility)
        if code_directory is None:
            code_directory = os.path.join(target_directory, "generate_code")

        self.logger.info(f"🎯 Using code directory (MCP workspace): {code_directory}")

        if not os.path.exists(code_directory):
            raise FileNotFoundError(
                "File tree structure not found, please run file tree creation first"
            )

        try:
            client, client_type = await self._initialize_llm_client()
            await self._initialize_mcp_agent(code_directory)

            tools = self._prepare_mcp_tool_definitions()
            system_message = GENERAL_CODE_IMPLEMENTATION_SYSTEM_PROMPT
            messages = []

            #             implementation_message = f"""**TASK: Implement Research Paper Reproduction Code**

            # You are implementing a complete, working codebase that reproduces the core algorithms, experiments, and methods described in a research paper. Your goal is to create functional code that can replicate the paper's key results and contributions.

            # **What you need to do:**
            # - Analyze the paper content and reproduction plan to understand requirements
            # - Implement all core algorithms mentioned in the main body of the paper
            # - Create the necessary components following the planned architecture
            # - Test each component to ensure functionality
            # - Integrate components into a cohesive, executable system
            # - Focus on reproducing main contributions rather than appendix-only experiments

            # **RESOURCES:**
            # - **Paper & Reproduction Plan**: `{target_directory}/` (contains .md paper files and initial_plan.txt with detailed implementation guidance)
            # - **Reference Code Indexes**: `{target_directory}/indexes/` (JSON files with implementation patterns from related codebases)
            # - **Implementation Directory**: `{code_directory}/` (your working directory for all code files)

            # **CURRENT OBJECTIVE:**
            # Start by reading the reproduction plan (`{target_directory}/initial_plan.txt`) to understand the implementation strategy, then examine the paper content to identify the first priority component to implement. Use the search_code tool to find relevant reference implementations from the indexes directory (`{target_directory}/indexes/*.json`) before coding.

            # ---
            # **START:** Review the plan above and begin implementation."""
            implementation_message = f"""**Task: Implement code based on the following reproduction plan**

**Code Reproduction Plan:**
{plan_content}

**Working Directory:** {code_directory}

**Current Objective:** Begin implementation by analyzing the plan structure, examining the current project layout, and implementing the first foundation file according to the plan's priority order."""

            messages.append({"role": "user", "content": implementation_message})

            result = await self._pure_code_implementation_loop(
                client,
                client_type,
                system_message,
                messages,
                tools,
                plan_content,
                target_directory,
            )

            return result

        finally:
            await self._cleanup_mcp_agent()

    # ==================== 3. Core Business Logic (Implementation Layer) ====================

    async def _pure_code_implementation_loop(
        self,
        client,
        client_type,
        system_message,
        messages,
        tools,
        plan_content,
        target_directory,
    ):
        """Pure code implementation loop with memory optimization and phase consistency"""
        max_iterations = 500
        iteration = 0
        start_time = time.time()
        max_time = 2400  # 40 minutes

        # Initialize specialized agents
        code_agent = CodeImplementationAgent(
            self.mcp_agent, self.logger, self.enable_read_tools
        )
        memory_agent = ConciseMemoryAgent(plan_content, self.logger, target_directory)

        # Log read tools configuration
        read_tools_status = "ENABLED" if self.enable_read_tools else "DISABLED"
        self.logger.info(
            f"🔧 Read tools (read_file, read_code_mem): {read_tools_status}"
        )
        if not self.enable_read_tools:
            self.logger.info(
                "🚫 No read mode: read_file and read_code_mem tools will be skipped"
            )

        # Connect code agent with memory agent for summary generation
        # Note: Concise memory agent doesn't need LLM client for summary generation
        code_agent.set_memory_agent(memory_agent, client, client_type)

        # Initialize memory agent with iteration 0
        memory_agent.start_new_round(iteration=0)

        while iteration < max_iterations:
            iteration += 1
            elapsed_time = time.time() - start_time

            if elapsed_time > max_time:
                self.logger.warning(f"Time limit reached: {elapsed_time:.2f}s")
                break

            # # Test simplified memory approach if we have files implemented
            # if iteration == 5 and code_agent.get_files_implemented_count() > 0:
            #     self.logger.info("🧪 Testing simplified memory approach...")
            #     test_results = await memory_agent.test_simplified_memory_approach()
            #     self.logger.info(f"Memory test results: {test_results}")

            # self.logger.info(f"Pure code implementation iteration {iteration}: generating code")

            messages = self._validate_messages(messages)
            current_system_message = code_agent.get_system_prompt()

            # Round logging removed

            # Call LLM
            response = await self._call_llm_with_tools(
                client, client_type, current_system_message, messages, tools
            )

            response_content = response.get("content", "").strip()
            if not response_content:
                response_content = "Continue implementing code files..."

            messages.append({"role": "assistant", "content": response_content})

            # Handle tool calls
            if response.get("tool_calls"):
                tool_results = await code_agent.execute_tool_calls(
                    response["tool_calls"]
                )

                # Record essential tool results in concise memory agent
                for tool_call, tool_result in zip(response["tool_calls"], tool_results):
                    memory_agent.record_tool_result(
                        tool_name=tool_call["name"],
                        tool_input=tool_call["input"],
                        tool_result=tool_result.get("result"),
                    )

                # NEW LOGIC: Check if write_file was called and trigger memory optimization immediately

                # Determine guidance based on results
                has_error = self._check_tool_results_for_errors(tool_results)
                files_count = code_agent.get_files_implemented_count()

                if has_error:
                    guidance = self._generate_error_guidance()
                else:
                    guidance = self._generate_success_guidance(files_count)

                compiled_response = self._compile_user_response(tool_results, guidance)
                messages.append({"role": "user", "content": compiled_response})

                # NEW LOGIC: Apply memory optimization immediately after write_file detection
                if memory_agent.should_trigger_memory_optimization(
                    messages, code_agent.get_files_implemented_count()
                ):
                    # Memory optimization triggered

                    # Apply concise memory optimization
                    files_implemented_count = code_agent.get_files_implemented_count()
                    current_system_message = code_agent.get_system_prompt()
                    messages = memory_agent.apply_memory_optimization(
                        current_system_message, messages, files_implemented_count
                    )

                    # Memory optimization completed

            else:
                files_count = code_agent.get_files_implemented_count()
                no_tools_guidance = self._generate_no_tools_guidance(files_count)
                messages.append({"role": "user", "content": no_tools_guidance})

            # Check for analysis loop and provide corrective guidance
            if code_agent.is_in_analysis_loop():
                analysis_loop_guidance = code_agent.get_analysis_loop_guidance()
                messages.append({"role": "user", "content": analysis_loop_guidance})
                self.logger.warning(
                    "Analysis loop detected and corrective guidance provided"
                )

            # Record file implementations in memory agent (for the current round)
            for file_info in code_agent.get_implementation_summary()["completed_files"]:
                memory_agent.record_file_implementation(file_info["file"])

            # REMOVED: Old memory optimization logic - now happens immediately after write_file
            # Memory optimization is now triggered immediately after write_file detection

            # Start new round for next iteration, sync with workflow iteration
            memory_agent.start_new_round(iteration=iteration)

            # Check completion
            if any(
                keyword in response_content.lower()
                for keyword in [
                    "all files implemented",
                    "all phases completed",
                    "reproduction plan fully implemented",
                    "all code of repo implementation complete",
                ]
            ):
                self.logger.info("Code implementation declared complete")
                break

            # Emergency trim if too long
            if len(messages) > 50:
                self.logger.warning(
                    "Emergency message trim - applying concise memory optimization"
                )

                current_system_message = code_agent.get_system_prompt()
                files_implemented_count = code_agent.get_files_implemented_count()
                messages = memory_agent.apply_memory_optimization(
                    current_system_message, messages, files_implemented_count
                )

        return await self._generate_pure_code_final_report_with_concise_agents(
            iteration, time.time() - start_time, code_agent, memory_agent
        )

    # ==================== 4. MCP Agent and LLM Communication Management (Communication Layer) ====================

    async def _initialize_mcp_agent(self, code_directory: str):
        """Initialize MCP agent and connect to code-implementation server"""
        try:
            self.mcp_agent = Agent(
                name="CodeImplementationAgent",
                instruction="You are a code implementation assistant, using MCP tools to implement paper code replication.",
                server_names=["code-implementation", "code-reference-indexer"],
            )

            await self.mcp_agent.__aenter__()
            llm = await self.mcp_agent.attach_llm(
                get_preferred_llm_class(self.config_path)
            )

            # Set workspace to the target code directory
            workspace_result = await self.mcp_agent.call_tool(
                "set_workspace", {"workspace_path": code_directory}
            )
            self.logger.info(f"Workspace setup result: {workspace_result}")

            return llm

        except Exception as e:
            self.logger.error(f"Failed to initialize MCP agent: {e}")
            if self.mcp_agent:
                try:
                    await self.mcp_agent.__aexit__(None, None, None)
                except Exception:
                    pass
                self.mcp_agent = None
            raise

    async def _cleanup_mcp_agent(self):
        """Clean up MCP agent resources"""
        if self.mcp_agent:
            try:
                await self.mcp_agent.__aexit__(None, None, None)
                self.logger.info("MCP agent connection closed")
            except Exception as e:
                self.logger.warning(f"Error closing MCP agent: {e}")
            finally:
                self.mcp_agent = None

    async def _initialize_llm_client(self):
        """Initialize LLM client (Anthropic or OpenAI) based on API key availability"""
        # Check which API has available key and try that first
        anthropic_key = self.api_config.get("anthropic", {}).get("api_key", "")
        openai_key = self.api_config.get("openai", {}).get("api_key", "")

        # Try Anthropic API first if key is available
        if anthropic_key and anthropic_key.strip():
            try:
                from anthropic import AsyncAnthropic

                client = AsyncAnthropic(api_key=anthropic_key)
                # Test connection with default model from config
                await client.messages.create(
                    model=self.default_models["anthropic"],
                    max_tokens=20,
                    messages=[{"role": "user", "content": "test"}],
                )
                self.logger.info(
                    f"Using Anthropic API with model: {self.default_models['anthropic']}"
                )
                return client, "anthropic"
            except Exception as e:
                self.logger.warning(f"Anthropic API unavailable: {e}")

        # Try OpenAI API if Anthropic failed or key not available
        if openai_key and openai_key.strip():
            try:
                from openai import AsyncOpenAI

                # Handle custom base_url if specified
                openai_config = self.api_config.get("openai", {})
                base_url = openai_config.get("base_url")

                if base_url:
                    client = AsyncOpenAI(api_key=openai_key, base_url=base_url)
                else:
                    client = AsyncOpenAI(api_key=openai_key)

                # Test connection with default model from config
                # Try max_tokens first, fallback to max_completion_tokens if unsupported
                try:
                    await client.chat.completions.create(
                        model=self.default_models["openai"],
                        max_tokens=20,
                        messages=[{"role": "user", "content": "test"}],
                    )
                except Exception as e:
                    if "max_tokens" in str(e) and "max_completion_tokens" in str(e):
                        # Retry with max_completion_tokens for models that require it
                        await client.chat.completions.create(
                            model=self.default_models["openai"],
                            max_completion_tokens=20,
                            messages=[{"role": "user", "content": "test"}],
                        )
                    else:
                        raise
                self.logger.info(
                    f"Using OpenAI API with model: {self.default_models['openai']}"
                )
                if base_url:
                    self.logger.info(f"Using custom base URL: {base_url}")
                return client, "openai"
            except Exception as e:
                self.logger.warning(f"OpenAI API unavailable: {e}")

        raise ValueError(
            "No available LLM API - please check your API keys in configuration"
        )

    async def _call_llm_with_tools(
        self, client, client_type, system_message, messages, tools, max_tokens=8192
    ):
        """Call LLM with tools"""
        try:
            if client_type == "anthropic":
                return await self._call_anthropic_with_tools(
                    client, system_message, messages, tools, max_tokens
                )
            elif client_type == "openai":
                return await self._call_openai_with_tools(
                    client, system_message, messages, tools, max_tokens
                )
            else:
                raise ValueError(f"Unsupported client type: {client_type}")
        except Exception as e:
            self.logger.error(f"LLM call failed: {e}")
            raise

    async def _call_anthropic_with_tools(
        self, client, system_message, messages, tools, max_tokens
    ):
        """Call Anthropic API"""
        validated_messages = self._validate_messages(messages)
        if not validated_messages:
            validated_messages = [
                {"role": "user", "content": "Please continue implementing code"}
            ]

        try:
            response = await client.messages.create(
                model=self.default_models["anthropic"],
                system=system_message,
                messages=validated_messages,
                tools=tools,
                max_tokens=max_tokens,
                temperature=0.2,
            )
        except Exception as e:
            self.logger.error(f"Anthropic API call failed: {e}")
            raise

        content = ""
        tool_calls = []

        for block in response.content:
            if block.type == "text":
                content += block.text
            elif block.type == "tool_use":
                tool_calls.append(
                    {"id": block.id, "name": block.name, "input": block.input}
                )

        return {"content": content, "tool_calls": tool_calls}

    async def _call_openai_with_tools(
        self, client, system_message, messages, tools, max_tokens
    ):
        """Call OpenAI API"""
        openai_tools = []
        for tool in tools:
            openai_tools.append(
                {
                    "type": "function",
                    "function": {
                        "name": tool["name"],
                        "description": tool["description"],
                        "parameters": tool["input_schema"],
                    },
                }
            )

        openai_messages = [{"role": "system", "content": system_message}]
        openai_messages.extend(messages)

        # Try max_tokens first, fallback to max_completion_tokens if unsupported
        try:
            response = await client.chat.completions.create(
                model=self.default_models["openai"],
                messages=openai_messages,
                tools=openai_tools if openai_tools else None,
                max_tokens=max_tokens,
                temperature=0.2,
            )
        except Exception as e:
            if "max_tokens" in str(e) and "max_completion_tokens" in str(e):
                # Retry with max_completion_tokens for models that require it
                response = await client.chat.completions.create(
                    model=self.default_models["openai"],
                    messages=openai_messages,
                    tools=openai_tools if openai_tools else None,
                    max_completion_tokens=max_tokens,
                )
            else:
                raise

        message = response.choices[0].message
        content = message.content or ""

        tool_calls = []
        if message.tool_calls:
            for tool_call in message.tool_calls:
                tool_calls.append(
                    {
                        "id": tool_call.id,
                        "name": tool_call.function.name,
                        "input": json.loads(tool_call.function.arguments),
                    }
                )

        return {"content": content, "tool_calls": tool_calls}

    # ==================== 5. Tools and Utility Methods (Utility Layer) ====================

    def _validate_messages(self, messages: List[Dict]) -> List[Dict]:
        """Validate and clean message list"""
        valid_messages = []
        for msg in messages:
            content = msg.get("content", "").strip()
            if content:
                valid_messages.append(
                    {"role": msg.get("role", "user"), "content": content}
                )
            else:
                self.logger.warning(f"Skipping empty message: {msg}")
        return valid_messages

    def _prepare_mcp_tool_definitions(self) -> List[Dict[str, Any]]:
        """Prepare tool definitions in Anthropic API standard format"""
        return get_mcp_tools("code_implementation")

    def _check_tool_results_for_errors(self, tool_results: List[Dict]) -> bool:
        """Check tool results for errors"""
        for result in tool_results:
            try:
                if hasattr(result["result"], "content") and result["result"].content:
                    content_text = result["result"].content[0].text
                    parsed_result = json.loads(content_text)
                    if parsed_result.get("status") == "error":
                        return True
                elif isinstance(result["result"], str):
                    if "error" in result["result"].lower():
                        return True
            except (json.JSONDecodeError, AttributeError, IndexError):
                result_str = str(result["result"])
                if "error" in result_str.lower():
                    return True
        return False

    # ==================== 6. User Interaction and Feedback (Interaction Layer) ====================

    def _generate_success_guidance(self, files_count: int) -> str:
        """Generate concise success guidance for continuing implementation"""
        return f"""✅ File implementation completed successfully!

📊 **Progress Status:** {files_count} files implemented

🎯 **Next Action:** Check if ALL files from the reproduction plan are implemented.

⚡ **Decision Process:**
1. **If ALL files are implemented:** Use `execute_python` or `execute_bash` to test the complete implementation, then respond "**implementation complete**" to end the conversation
2. **If MORE files need implementation:** Continue with dependency-aware workflow:
   - **Start with `read_code_mem`** to understand existing implementations and dependencies
   - **Then `write_file`** to implement the new component
   - **Finally: Test** if needed

💡 **Key Point:** Always verify completion status before continuing with new file creation."""

    def _generate_error_guidance(self) -> str:
        """Generate error guidance for handling issues"""
        return """❌ Error detected during file implementation.

🔧 **Action Required:**
1. Review the error details above
2. Fix the identified issue
3. **Check if ALL files from the reproduction plan are implemented:**
   - **If YES:** Use `execute_python` or `execute_bash` to test the complete implementation, then respond "**implementation complete**" to end the conversation
   - **If NO:** Continue with proper development cycle for next file:
     - **Start with `read_code_mem`** to understand existing implementations
     - **Then `write_file`** to implement properly
     - **Test** if needed
4. Ensure proper error handling in future implementations

💡 **Remember:** Always verify if all planned files are implemented before continuing with new file creation."""

    def _generate_no_tools_guidance(self, files_count: int) -> str:
        """Generate concise guidance when no tools are called"""
        return f"""⚠️ No tool calls detected in your response.

📊 **Current Progress:** {files_count} files implemented

🚨 **Action Required:** You must use tools. **FIRST check if ALL files from the reproduction plan are implemented:**

⚡ **Decision Process:**
1. **If ALL files are implemented:** Use `execute_python` or `execute_bash` to test the complete implementation, then respond "**implementation complete**" to end the conversation
2. **If MORE files need implementation:** Follow the development cycle:
   - **Start with `read_code_mem`** to understand existing implementations
   - **Then `write_file`** to implement the new component
   - **Finally: Test** if needed

🚨 **Critical:** Always verify completion status first, then use appropriate tools - not just explanations!"""

    def _compile_user_response(self, tool_results: List[Dict], guidance: str) -> str:
        """Compile tool results and guidance into a single user response"""
        response_parts = []

        if tool_results:
            response_parts.append("🔧 **Tool Execution Results:**")
            for tool_result in tool_results:
                tool_name = tool_result["tool_name"]
                result_content = tool_result["result"]
                response_parts.append(
                    f"```\nTool: {tool_name}\nResult: {result_content}\n```"
                )

        if guidance:
            response_parts.append("\n" + guidance)

        return "\n\n".join(response_parts)

    # ==================== 7. Reporting and Output (Output Layer) ====================

    async def _generate_pure_code_final_report_with_concise_agents(
        self,
        iterations: int,
        elapsed_time: float,
        code_agent: CodeImplementationAgent,
        memory_agent: ConciseMemoryAgent,
    ):
        """Generate final report using concise agent statistics"""
        try:
            code_stats = code_agent.get_implementation_statistics()
            memory_stats = memory_agent.get_memory_statistics(
                code_stats["files_implemented_count"]
            )

            if self.mcp_agent:
                history_result = await self.mcp_agent.call_tool(
                    "get_operation_history", {"last_n": 30}
                )
                history_data = (
                    json.loads(history_result)
                    if isinstance(history_result, str)
                    else history_result
                )
            else:
                history_data = {"total_operations": 0, "history": []}

            write_operations = 0
            files_created = []
            if "history" in history_data:
                for item in history_data["history"]:
                    if item.get("action") == "write_file":
                        write_operations += 1
                        file_path = item.get("details", {}).get("file_path", "unknown")
                        files_created.append(file_path)

            report = f"""
# Pure Code Implementation Completion Report (Write-File-Based Memory Mode)

## Execution Summary
- Implementation iterations: {iterations}
- Total elapsed time: {elapsed_time:.2f} seconds
- Files implemented: {code_stats['total_files_implemented']}
- File write operations: {write_operations}
- Total MCP operations: {history_data.get('total_operations', 0)}

## Read Tools Configuration
- Read tools enabled: {code_stats['read_tools_status']['read_tools_enabled']}
- Status: {code_stats['read_tools_status']['status']}
- Tools affected: {', '.join(code_stats['read_tools_status']['tools_affected'])}

## Agent Performance
### Code Implementation Agent
- Files tracked: {code_stats['files_implemented_count']}
- Technical decisions: {code_stats['technical_decisions_count']}
- Constraints tracked: {code_stats['constraints_count']}
- Architecture notes: {code_stats['architecture_notes_count']}
- Dependency analysis performed: {code_stats['dependency_analysis_count']}
- Files read for dependencies: {code_stats['files_read_for_dependencies']}
- Last summary triggered at file count: {code_stats['last_summary_file_count']}

### Concise Memory Agent (Write-File-Based)
- Last write_file detected: {memory_stats['last_write_file_detected']}
- Should clear memory next: {memory_stats['should_clear_memory_next']}
- Files implemented count: {memory_stats['implemented_files_tracked']}
- Current round: {memory_stats['current_round']}
- Concise mode active: {memory_stats['concise_mode_active']}
- Current round tool results: {memory_stats['current_round_tool_results']}
- Essential tools recorded: {memory_stats['essential_tools_recorded']}

## Files Created
"""
            for file_path in files_created[-20:]:
                report += f"- {file_path}\n"

            if len(files_created) > 20:
                report += f"... and {len(files_created) - 20} more files\n"

            report += """
## Architecture Features
✅ WRITE-FILE-BASED Memory Agent - Clear after each file generation
✅ After write_file: Clear history → Keep system prompt + initial plan + tool results
✅ Tool accumulation: read_code_mem, read_file, search_reference_code until next write_file
✅ Clean memory cycle: write_file → clear → accumulate → write_file → clear
✅ Essential tool recording with write_file detection
✅ Specialized agent separation for clean code organization
✅ MCP-compliant tool execution
✅ Production-grade code with comprehensive type hints
✅ Intelligent dependency analysis and file reading
✅ Automated read_file usage for implementation context
✅ Eliminates conversation clutter between file generations
✅ Focused memory for efficient next file generation
"""
            return report

        except Exception as e:
            self.logger.error(f"Failed to generate final report: {e}")
            return f"Failed to generate final report: {str(e)}"


async def main():
    """Main function for running the workflow"""
    # Configure root logger carefully to avoid duplicates
    root_logger = logging.getLogger()
    if not root_logger.handlers:
        handler = logging.StreamHandler()
        formatter = logging.Formatter("%(levelname)s:%(name)s:%(message)s")
        handler.setFormatter(formatter)
        root_logger.addHandler(handler)
        root_logger.setLevel(logging.INFO)

    workflow = CodeImplementationWorkflow()

    print("=" * 60)
    print("Code Implementation Workflow with UNIFIED Reference Indexer")
    print("=" * 60)
    print("Select mode:")
    print("1. Test Code Reference Indexer Integration")
    print("2. Run Full Implementation Workflow")
    print("3. Run Implementation with Pure Code Mode")
    print("4. Test Read Tools Configuration")

    # mode_choice = input("Enter choice (1-4, default: 3): ").strip()

    # For testing purposes, we'll run the test first
    # if mode_choice == "4":
    #     print("Testing Read Tools Configuration...")

    #     # Create a test workflow normally
    #     test_workflow = CodeImplementationWorkflow()

    #     # Create a mock code agent for testing
    #     print("\n🧪 Testing with read tools DISABLED:")
    #     test_agent_disabled = CodeImplementationAgent(None, enable_read_tools=False)
    #     await test_agent_disabled.test_read_tools_configuration()

    #     print("\n🧪 Testing with read tools ENABLED:")
    #     test_agent_enabled = CodeImplementationAgent(None, enable_read_tools=True)
    #     await test_agent_enabled.test_read_tools_configuration()

    #     print("✅ Read tools configuration testing completed!")
    #     return

    # print("Running Code Reference Indexer Integration Test...")

    test_success = True
    if test_success:
        print("\n" + "=" * 60)
        print("🎉 UNIFIED Code Reference Indexer Integration Test PASSED!")
        print("🔧 Three-step process successfully merged into ONE tool")
        print("=" * 60)

        # Ask if user wants to continue with actual workflow
        print("\nContinuing with workflow execution...")

        plan_file = "/Users/lizongwei/Reasearch/DeepCode_Base/DeepCode/deepcode_lab/papers/1/initial_plan.txt"
        # plan_file = "/data2/bjdwhzzh/project-hku/Code-Agent2.0/Code-Agent/deepcode-mcp/agent_folders/papers/1/initial_plan.txt"
        target_directory = (
            "/Users/lizongwei/Reasearch/DeepCode_Base/DeepCode/deepcode_lab/papers/1/"
        )
        print("Implementation Mode Selection:")
        print("1. Pure Code Implementation Mode (Recommended)")
        print("2. Iterative Implementation Mode")

        pure_code_mode = True
        mode_name = "Pure Code Implementation Mode with Memory Agent Architecture + Code Reference Indexer"
        print(f"Using: {mode_name}")

        # Configure read tools - modify this parameter to enable/disable read tools
        enable_read_tools = (
            True  # Set to False to disable read_file and read_code_mem tools
        )
        read_tools_status = "ENABLED" if enable_read_tools else "DISABLED"
        print(f"🔧 Read tools (read_file, read_code_mem): {read_tools_status}")

        # NOTE: To test without read tools, change the line above to:
        # enable_read_tools = False

        result = await workflow.run_workflow(
            plan_file,
            target_directory=target_directory,
            pure_code_mode=pure_code_mode,
            enable_read_tools=enable_read_tools,
        )

        print("=" * 60)
        print("Workflow Execution Results:")
        print(f"Status: {result['status']}")
        print(f"Mode: {mode_name}")

        if result["status"] == "success":
            print(f"Code Directory: {result['code_directory']}")
            print(f"MCP Architecture: {result.get('mcp_architecture', 'unknown')}")
            print("Execution completed!")
        else:
            print(f"Error Message: {result['message']}")

        print("=" * 60)
        print(
            "✅ Using Standard MCP Architecture with Memory Agent + Code Reference Indexer"
        )

    else:
        print("\n" + "=" * 60)
        print("❌ Code Reference Indexer Integration Test FAILED!")
        print("Please check the configuration and try again.")
        print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: workflows/code_implementation_workflow_index.py
================================================
"""
Paper Code Implementation Workflow - MCP-compliant Iterative Development

Features:
1. File Tree Creation
2. Code Implementation - Based on aisi-basic-agent iterative development

MCP Architecture:
- MCP Server: tools/code_implementation_server.py
- MCP Client: Called through mcp_agent framework
- Configuration: mcp_agent.config.yaml
"""

import asyncio
import json
import logging
import os
import sys
import time
import yaml
from pathlib import Path
from typing import Dict, Any, Optional, List

# MCP Agent imports
from mcp_agent.agents.agent import Agent

# Local imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from prompts.code_prompts import STRUCTURE_GENERATOR_PROMPT
from prompts.code_prompts import (
    PURE_CODE_IMPLEMENTATION_SYSTEM_PROMPT_INDEX,
)
from workflows.agents import CodeImplementationAgent
from workflows.agents.memory_agent_concise import ConciseMemoryAgent
from config.mcp_tool_definitions_index import get_mcp_tools
from utils.llm_utils import get_preferred_llm_class, get_default_models
# DialogueLogger removed - no longer needed


class CodeImplementationWorkflowWithIndex:
    """
    Paper Code Implementation Workflow Manager with Code Reference Indexer

    Uses standard MCP architecture with enhanced indexing capabilities:
    1. Connect to code-implementation server via MCP client
    2. Use MCP protocol for tool calls
    3. Support workspace management and operation history tracking
    4. Integrated code reference indexer for enhanced code understanding
    """

    # ==================== 1. Class Initialization and Configuration (Infrastructure Layer) ====================

    def __init__(self, config_path: str = "mcp_agent.secrets.yaml"):
        """Initialize workflow with configuration"""
        self.config_path = config_path
        self.api_config = self._load_api_config()
        self.default_models = get_default_models("mcp_agent.config.yaml")
        self.logger = self._create_logger()
        self.mcp_agent = None
        self.enable_read_tools = (
            True  # Default value, will be overridden by run_workflow parameter
        )

    def _load_api_config(self) -> Dict[str, Any]:
        """Load API configuration from YAML file"""
        try:
            with open(self.config_path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        except Exception as e:
            raise Exception(f"Failed to load API config: {e}")

    def _create_logger(self) -> logging.Logger:
        """Create and configure logger"""
        logger = logging.getLogger(__name__)
        # Don't add handlers to child loggers - let them propagate to root
        logger.setLevel(logging.INFO)
        return logger

    def _read_plan_file(self, plan_file_path: str) -> str:
        """Read implementation plan file"""
        plan_path = Path(plan_file_path)
        if not plan_path.exists():
            raise FileNotFoundError(
                f"Implementation plan file not found: {plan_file_path}"
            )

        with open(plan_path, "r", encoding="utf-8") as f:
            return f.read()

    def _check_file_tree_exists(self, target_directory: str) -> bool:
        """Check if file tree structure already exists"""
        code_directory = os.path.join(target_directory, "generate_code")
        return os.path.exists(code_directory) and len(os.listdir(code_directory)) > 0

    # ==================== 2. Public Interface Methods (External API Layer) ====================

    async def run_workflow(
        self,
        plan_file_path: str,
        target_directory: Optional[str] = None,
        pure_code_mode: bool = False,
        enable_read_tools: bool = True,
    ):
        """Run complete workflow - Main public interface"""
        # Set the read tools configuration
        self.enable_read_tools = enable_read_tools

        try:
            plan_content = self._read_plan_file(plan_file_path)

            if target_directory is None:
                target_directory = str(Path(plan_file_path).parent)

            # Calculate code directory for workspace alignment
            code_directory = os.path.join(target_directory, "generate_code")

            self.logger.info("=" * 80)
            self.logger.info("🚀 STARTING CODE IMPLEMENTATION WORKFLOW")
            self.logger.info("=" * 80)
            self.logger.info(f"📄 Plan file: {plan_file_path}")
            self.logger.info(f"📂 Plan file parent: {target_directory}")
            self.logger.info(f"🎯 Code directory (MCP workspace): {code_directory}")
            self.logger.info(
                f"⚙️  Read tools: {'ENABLED' if self.enable_read_tools else 'DISABLED'}"
            )
            self.logger.info("=" * 80)

            results = {}

            # Check if file tree exists
            if self._check_file_tree_exists(target_directory):
                self.logger.info("File tree exists, skipping creation")
                results["file_tree"] = "Already exists, skipped creation"
            else:
                self.logger.info("Creating file tree...")
                results["file_tree"] = await self.create_file_structure(
                    plan_content, target_directory
                )

            # Code implementation
            if pure_code_mode:
                self.logger.info("Starting pure code implementation...")
                results["code_implementation"] = await self.implement_code_pure(
                    plan_content, target_directory, code_directory
                )
            else:
                pass

            self.logger.info("Workflow execution successful")

            return {
                "status": "success",
                "plan_file": plan_file_path,
                "target_directory": target_directory,
                "code_directory": os.path.join(target_directory, "generate_code"),
                "results": results,
                "mcp_architecture": "standard",
            }

        except Exception as e:
            self.logger.error(f"Workflow execution failed: {e}")

            return {"status": "error", "message": str(e), "plan_file": plan_file_path}
        finally:
            await self._cleanup_mcp_agent()

    async def create_file_structure(
        self, plan_content: str, target_directory: str
    ) -> str:
        """Create file tree structure based on implementation plan"""
        self.logger.info("Starting file tree creation...")

        structure_agent = Agent(
            name="StructureGeneratorAgent",
            instruction=STRUCTURE_GENERATOR_PROMPT,
            server_names=["command-executor"],
        )

        async with structure_agent:
            creator = await structure_agent.attach_llm(
                get_preferred_llm_class(self.config_path)
            )

            message = f"""Analyze the following implementation plan and generate shell commands to create the file tree structure.

Target Directory: {target_directory}/generate_code

Implementation Plan:
{plan_content}

Tasks:
1. Find the file tree structure in the implementation plan
2. Generate shell commands (mkdir -p, touch) to create that structure
3. Use the execute_commands tool to run the commands and create the file structure

Requirements:
- Use mkdir -p to create directories
- Use touch to create files
- Include __init__.py file for Python packages
- Use relative paths to the target directory
- Execute commands to actually create the file structure"""

            result = await creator.generate_str(message=message)
            self.logger.info("File tree structure creation completed")
            return result

    async def implement_code_pure(
        self, plan_content: str, target_directory: str, code_directory: str = None
    ) -> str:
        """Pure code implementation - focus on code writing without testing"""
        self.logger.info("Starting pure code implementation (no testing)...")

        # Use provided code_directory or calculate it (for backwards compatibility)
        if code_directory is None:
            code_directory = os.path.join(target_directory, "generate_code")

        self.logger.info(f"🎯 Using code directory (MCP workspace): {code_directory}")

        if not os.path.exists(code_directory):
            raise FileNotFoundError(
                "File tree structure not found, please run file tree creation first"
            )

        try:
            client, client_type = await self._initialize_llm_client()
            await self._initialize_mcp_agent(code_directory)

            tools = self._prepare_mcp_tool_definitions()
            system_message = PURE_CODE_IMPLEMENTATION_SYSTEM_PROMPT_INDEX
            messages = []

            #             implementation_message = f"""**TASK: Implement Research Paper Reproduction Code**

            # You are implementing a complete, working codebase that reproduces the core algorithms, experiments, and methods described in a research paper. Your goal is to create functional code that can replicate the paper's key results and contributions.

            # **What you need to do:**
            # - Analyze the paper content and reproduction plan to understand requirements
            # - Implement all core algorithms mentioned in the main body of the paper
            # - Create the necessary components following the planned architecture
            # - Test each component to ensure functionality
            # - Integrate components into a cohesive, executable system
            # - Focus on reproducing main contributions rather than appendix-only experiments

            # **RESOURCES:**
            # - **Paper & Reproduction Plan**: `{target_directory}/` (contains .md paper files and initial_plan.txt with detailed implementation guidance)
            # - **Reference Code Indexes**: `{target_directory}/indexes/` (JSON files with implementation patterns from related codebases)
            # - **Implementation Directory**: `{code_directory}/` (your working directory for all code files)

            # **CURRENT OBJECTIVE:**
            # Start by reading the reproduction plan (`{target_directory}/initial_plan.txt`) to understand the implementation strategy, then examine the paper content to identify the first priority component to implement. Use the search_code tool to find relevant reference implementations from the indexes directory (`{target_directory}/indexes/*.json`) before coding.

            # ---
            # **START:** Review the plan above and begin implementation."""
            implementation_message = f"""**Task: Implement code based on the following reproduction plan**

**Code Reproduction Plan:**
{plan_content}

**Working Directory:** {code_directory}

**Current Objective:** Begin implementation by analyzing the plan structure, examining the current project layout, and implementing the first foundation file according to the plan's priority order."""

            messages.append({"role": "user", "content": implementation_message})

            result = await self._pure_code_implementation_loop(
                client,
                client_type,
                system_message,
                messages,
                tools,
                plan_content,
                target_directory,
            )

            return result

        finally:
            await self._cleanup_mcp_agent()

    # ==================== 3. Core Business Logic (Implementation Layer) ====================

    async def _pure_code_implementation_loop(
        self,
        client,
        client_type,
        system_message,
        messages,
        tools,
        plan_content,
        target_directory,
    ):
        """Pure code implementation loop with memory optimization and phase consistency"""
        max_iterations = 500
        iteration = 0
        start_time = time.time()
        max_time = 2400  # 40 minutes

        # Initialize specialized agents
        code_agent = CodeImplementationAgent(
            self.mcp_agent, self.logger, self.enable_read_tools
        )
        memory_agent = ConciseMemoryAgent(plan_content, self.logger, target_directory)

        # Log read tools configuration
        read_tools_status = "ENABLED" if self.enable_read_tools else "DISABLED"
        self.logger.info(
            f"🔧 Read tools (read_file, read_code_mem): {read_tools_status}"
        )
        if not self.enable_read_tools:
            self.logger.info(
                "🚫 No read mode: read_file and read_code_mem tools will be skipped"
            )

        # Connect code agent with memory agent for summary generation
        # Note: Concise memory agent doesn't need LLM client for summary generation
        code_agent.set_memory_agent(memory_agent, client, client_type)

        # Initialize memory agent with iteration 0
        memory_agent.start_new_round(iteration=0)

        while iteration < max_iterations:
            iteration += 1
            elapsed_time = time.time() - start_time

            if elapsed_time > max_time:
                self.logger.warning(f"Time limit reached: {elapsed_time:.2f}s")
                break

            # # Test simplified memory approach if we have files implemented
            # if iteration == 5 and code_agent.get_files_implemented_count() > 0:
            #     self.logger.info("🧪 Testing simplified memory approach...")
            #     test_results = await memory_agent.test_simplified_memory_approach()
            #     self.logger.info(f"Memory test results: {test_results}")

            # self.logger.info(f"Pure code implementation iteration {iteration}: generating code")

            messages = self._validate_messages(messages)
            current_system_message = code_agent.get_system_prompt()

            # Round logging removed

            # Call LLM
            response = await self._call_llm_with_tools(
                client, client_type, current_system_message, messages, tools
            )

            response_content = response.get("content", "").strip()
            if not response_content:
                response_content = "Continue implementing code files..."

            messages.append({"role": "assistant", "content": response_content})

            # Handle tool calls
            if response.get("tool_calls"):
                tool_results = await code_agent.execute_tool_calls(
                    response["tool_calls"]
                )

                # Record essential tool results in concise memory agent
                for tool_call, tool_result in zip(response["tool_calls"], tool_results):
                    memory_agent.record_tool_result(
                        tool_name=tool_call["name"],
                        tool_input=tool_call["input"],
                        tool_result=tool_result.get("result"),
                    )

                # NEW LOGIC: Check if write_file was called and trigger memory optimization immediately

                # Determine guidance based on results
                has_error = self._check_tool_results_for_errors(tool_results)
                files_count = code_agent.get_files_implemented_count()

                if has_error:
                    guidance = self._generate_error_guidance()
                else:
                    guidance = self._generate_success_guidance(files_count)

                compiled_response = self._compile_user_response(tool_results, guidance)
                messages.append({"role": "user", "content": compiled_response})

                # NEW LOGIC: Apply memory optimization immediately after write_file detection
                if memory_agent.should_trigger_memory_optimization(
                    messages, code_agent.get_files_implemented_count()
                ):
                    # Memory optimization triggered

                    # Apply concise memory optimization
                    files_implemented_count = code_agent.get_files_implemented_count()
                    current_system_message = code_agent.get_system_prompt()
                    messages = memory_agent.apply_memory_optimization(
                        current_system_message, messages, files_implemented_count
                    )

                    # Memory optimization completed

            else:
                files_count = code_agent.get_files_implemented_count()
                no_tools_guidance = self._generate_no_tools_guidance(files_count)
                messages.append({"role": "user", "content": no_tools_guidance})

            # Check for analysis loop and provide corrective guidance
            if code_agent.is_in_analysis_loop():
                analysis_loop_guidance = code_agent.get_analysis_loop_guidance()
                messages.append({"role": "user", "content": analysis_loop_guidance})
                self.logger.warning(
                    "Analysis loop detected and corrective guidance provided"
                )

            # Record file implementations in memory agent (for the current round)
            for file_info in code_agent.get_implementation_summary()["completed_files"]:
                memory_agent.record_file_implementation(file_info["file"])

            # REMOVED: Old memory optimization logic - now happens immediately after write_file
            # Memory optimization is now triggered immediately after write_file detection

            # Start new round for next iteration, sync with workflow iteration
            memory_agent.start_new_round(iteration=iteration)

            # Check completion
            if any(
                keyword in response_content.lower()
                for keyword in [
                    "all files implemented",
                    "all phases completed",
                    "reproduction plan fully implemented",
                    "all code of repo implementation complete",
                ]
            ):
                self.logger.info("Code implementation declared complete")
                break

            # Emergency trim if too long
            if len(messages) > 50:
                self.logger.warning(
                    "Emergency message trim - applying concise memory optimization"
                )

                current_system_message = code_agent.get_system_prompt()
                files_implemented_count = code_agent.get_files_implemented_count()
                messages = memory_agent.apply_memory_optimization(
                    current_system_message, messages, files_implemented_count
                )

        return await self._generate_pure_code_final_report_with_concise_agents(
            iteration, time.time() - start_time, code_agent, memory_agent
        )

    # ==================== 4. MCP Agent and LLM Communication Management (Communication Layer) ====================

    async def _initialize_mcp_agent(self, code_directory: str):
        """Initialize MCP agent and connect to code-implementation server"""
        try:
            self.mcp_agent = Agent(
                name="CodeImplementationAgent",
                instruction="You are a code implementation assistant, using MCP tools to implement paper code replication.",
                server_names=["code-implementation", "code-reference-indexer"],
            )

            await self.mcp_agent.__aenter__()
            llm = await self.mcp_agent.attach_llm(
                get_preferred_llm_class(self.config_path)
            )

            # Set workspace to the target code directory
            workspace_result = await self.mcp_agent.call_tool(
                "set_workspace", {"workspace_path": code_directory}
            )
            self.logger.info(f"Workspace setup result: {workspace_result}")

            return llm

        except Exception as e:
            self.logger.error(f"Failed to initialize MCP agent: {e}")
            if self.mcp_agent:
                try:
                    await self.mcp_agent.__aexit__(None, None, None)
                except Exception:
                    pass
                self.mcp_agent = None
            raise

    async def _cleanup_mcp_agent(self):
        """Clean up MCP agent resources"""
        if self.mcp_agent:
            try:
                await self.mcp_agent.__aexit__(None, None, None)
                self.logger.info("MCP agent connection closed")
            except Exception as e:
                self.logger.warning(f"Error closing MCP agent: {e}")
            finally:
                self.mcp_agent = None

    async def _initialize_llm_client(self):
        """Initialize LLM client (Anthropic or OpenAI) based on API key availability"""
        # Check which API has available key and try that first
        anthropic_key = self.api_config.get("anthropic", {}).get("api_key", "")
        openai_key = self.api_config.get("openai", {}).get("api_key", "")

        # Try Anthropic API first if key is available
        if anthropic_key and anthropic_key.strip():
            try:
                from anthropic import AsyncAnthropic

                client = AsyncAnthropic(api_key=anthropic_key)
                # Test connection with default model from config
                await client.messages.create(
                    model=self.default_models["anthropic"],
                    max_tokens=20,
                    messages=[{"role": "user", "content": "test"}],
                )
                self.logger.info(
                    f"Using Anthropic API with model: {self.default_models['anthropic']}"
                )
                return client, "anthropic"
            except Exception as e:
                self.logger.warning(f"Anthropic API unavailable: {e}")

        # Try OpenAI API if Anthropic failed or key not available
        if openai_key and openai_key.strip():
            try:
                from openai import AsyncOpenAI

                # Handle custom base_url if specified
                openai_config = self.api_config.get("openai", {})
                base_url = openai_config.get("base_url")

                if base_url:
                    client = AsyncOpenAI(api_key=openai_key, base_url=base_url)
                else:
                    client = AsyncOpenAI(api_key=openai_key)

                # Test connection with default model from config
                # Try max_tokens first, fallback to max_completion_tokens if unsupported
                try:
                    await client.chat.completions.create(
                        model=self.default_models["openai"],
                        max_tokens=20,
                        messages=[{"role": "user", "content": "test"}],
                    )
                except Exception as e:
                    if "max_tokens" in str(e) and "max_completion_tokens" in str(e):
                        # Retry with max_completion_tokens for models that require it
                        await client.chat.completions.create(
                            model=self.default_models["openai"],
                            max_completion_tokens=20,
                            messages=[{"role": "user", "content": "test"}],
                        )
                    else:
                        raise
                self.logger.info(
                    f"Using OpenAI API with model: {self.default_models['openai']}"
                )
                if base_url:
                    self.logger.info(f"Using custom base URL: {base_url}")
                return client, "openai"
            except Exception as e:
                self.logger.warning(f"OpenAI API unavailable: {e}")

        raise ValueError(
            "No available LLM API - please check your API keys in configuration"
        )

    async def _call_llm_with_tools(
        self, client, client_type, system_message, messages, tools, max_tokens=8192
    ):
        """Call LLM with tools"""
        try:
            if client_type == "anthropic":
                return await self._call_anthropic_with_tools(
                    client, system_message, messages, tools, max_tokens
                )
            elif client_type == "openai":
                return await self._call_openai_with_tools(
                    client, system_message, messages, tools, max_tokens
                )
            else:
                raise ValueError(f"Unsupported client type: {client_type}")
        except Exception as e:
            self.logger.error(f"LLM call failed: {e}")
            raise

    async def _call_anthropic_with_tools(
        self, client, system_message, messages, tools, max_tokens
    ):
        """Call Anthropic API"""
        validated_messages = self._validate_messages(messages)
        if not validated_messages:
            validated_messages = [
                {"role": "user", "content": "Please continue implementing code"}
            ]

        try:
            response = await client.messages.create(
                model=self.default_models["anthropic"],
                system=system_message,
                messages=validated_messages,
                tools=tools,
                max_tokens=max_tokens,
                temperature=0.2,
            )
        except Exception as e:
            self.logger.error(f"Anthropic API call failed: {e}")
            raise

        content = ""
        tool_calls = []

        for block in response.content:
            if block.type == "text":
                content += block.text
            elif block.type == "tool_use":
                tool_calls.append(
                    {"id": block.id, "name": block.name, "input": block.input}
                )

        return {"content": content, "tool_calls": tool_calls}

    async def _call_openai_with_tools(
        self, client, system_message, messages, tools, max_tokens
    ):
        """Call OpenAI API"""
        openai_tools = []
        for tool in tools:
            openai_tools.append(
                {
                    "type": "function",
                    "function": {
                        "name": tool["name"],
                        "description": tool["description"],
                        "parameters": tool["input_schema"],
                    },
                }
            )

        openai_messages = [{"role": "system", "content": system_message}]
        openai_messages.extend(messages)

        # Try max_tokens first, fallback to max_completion_tokens if unsupported
        try:
            response = await client.chat.completions.create(
                model=self.default_models["openai"],
                messages=openai_messages,
                tools=openai_tools if openai_tools else None,
                max_tokens=max_tokens,
                temperature=0.2,
            )
        except Exception as e:
            if "max_tokens" in str(e) and "max_completion_tokens" in str(e):
                # Retry with max_completion_tokens for models that require it
                response = await client.chat.completions.create(
                    model=self.default_models["openai"],
                    messages=openai_messages,
                    tools=openai_tools if openai_tools else None,
                    max_completion_tokens=max_tokens,
                )
            else:
                raise

        message = response.choices[0].message
        content = message.content or ""

        tool_calls = []
        if message.tool_calls:
            for tool_call in message.tool_calls:
                tool_calls.append(
                    {
                        "id": tool_call.id,
                        "name": tool_call.function.name,
                        "input": json.loads(tool_call.function.arguments),
                    }
                )

        return {"content": content, "tool_calls": tool_calls}

    # ==================== 5. Tools and Utility Methods (Utility Layer) ====================

    def _validate_messages(self, messages: List[Dict]) -> List[Dict]:
        """Validate and clean message list"""
        valid_messages = []
        for msg in messages:
            content = msg.get("content", "").strip()
            if content:
                valid_messages.append(
                    {"role": msg.get("role", "user"), "content": content}
                )
            else:
                self.logger.warning(f"Skipping empty message: {msg}")
        return valid_messages

    def _prepare_mcp_tool_definitions(self) -> List[Dict[str, Any]]:
        """Prepare tool definitions in Anthropic API standard format"""
        return get_mcp_tools("code_implementation")

    def _check_tool_results_for_errors(self, tool_results: List[Dict]) -> bool:
        """Check tool results for errors"""
        for result in tool_results:
            try:
                if hasattr(result["result"], "content") and result["result"].content:
                    content_text = result["result"].content[0].text
                    parsed_result = json.loads(content_text)
                    if parsed_result.get("status") == "error":
                        return True
                elif isinstance(result["result"], str):
                    if "error" in result["result"].lower():
                        return True
            except (json.JSONDecodeError, AttributeError, IndexError):
                result_str = str(result["result"])
                if "error" in result_str.lower():
                    return True
        return False

    # ==================== 6. User Interaction and Feedback (Interaction Layer) ====================

    def _generate_success_guidance(self, files_count: int) -> str:
        """Generate concise success guidance for continuing implementation"""
        return f"""✅ File implementation completed successfully!

📊 **Progress Status:** {files_count} files implemented

🎯 **Next Action:** Check if ALL files from the reproduction plan are implemented.

⚡ **Decision Process:**
1. **If ALL files are implemented:** Use `execute_python` or `execute_bash` to test the complete implementation, then respond "**implementation complete**" to end the conversation
2. **If MORE files need implementation:** Continue with dependency-aware workflow:
   - **Start with `read_code_mem`** to understand existing implementations and dependencies
   - **Optionally use `search_code_references`** for reference patterns (OPTIONAL - use for inspiration only, original paper specs take priority)
   - **Then `write_file`** to implement the new component
   - **Finally: Test** if needed

💡 **Key Point:** Always verify completion status before continuing with new file creation."""

    def _generate_error_guidance(self) -> str:
        """Generate error guidance for handling issues"""
        return """❌ Error detected during file implementation.

🔧 **Action Required:**
1. Review the error details above
2. Fix the identified issue
3. **Check if ALL files from the reproduction plan are implemented:**
   - **If YES:** Use `execute_python` or `execute_bash` to test the complete implementation, then respond "**implementation complete**" to end the conversation
   - **If NO:** Continue with proper development cycle for next file:
     - **Start with `read_code_mem`** to understand existing implementations
     - **Optionally use `search_code_references`** for reference patterns (OPTIONAL - for inspiration only)
     - **Then `write_file`** to implement properly
     - **Test** if needed
4. Ensure proper error handling in future implementations

💡 **Remember:** Always verify if all planned files are implemented before continuing with new file creation."""

    def _generate_no_tools_guidance(self, files_count: int) -> str:
        """Generate concise guidance when no tools are called"""
        return f"""⚠️ No tool calls detected in your response.

📊 **Current Progress:** {files_count} files implemented

🚨 **Action Required:** You must use tools. **FIRST check if ALL files from the reproduction plan are implemented:**

⚡ **Decision Process:**
1. **If ALL files are implemented:** Use `execute_python` or `execute_bash` to test the complete implementation, then respond "**implementation complete**" to end the conversation
2. **If MORE files need implementation:** Follow the development cycle:
   - **Start with `read_code_mem`** to understand existing implementations
   - **Optionally use `search_code_references`** for reference patterns (OPTIONAL - for inspiration only)
   - **Then `write_file`** to implement the new component
   - **Finally: Test** if needed

🚨 **Critical:** Always verify completion status first, then use appropriate tools - not just explanations!"""

    def _compile_user_response(self, tool_results: List[Dict], guidance: str) -> str:
        """Compile tool results and guidance into a single user response"""
        response_parts = []

        if tool_results:
            response_parts.append("🔧 **Tool Execution Results:**")
            for tool_result in tool_results:
                tool_name = tool_result["tool_name"]
                result_content = tool_result["result"]
                response_parts.append(
                    f"```\nTool: {tool_name}\nResult: {result_content}\n```"
                )

        if guidance:
            response_parts.append("\n" + guidance)

        return "\n\n".join(response_parts)

    # ==================== 7. Reporting and Output (Output Layer) ====================

    async def _generate_pure_code_final_report_with_concise_agents(
        self,
        iterations: int,
        elapsed_time: float,
        code_agent: CodeImplementationAgent,
        memory_agent: ConciseMemoryAgent,
    ):
        """Generate final report using concise agent statistics"""
        try:
            code_stats = code_agent.get_implementation_statistics()
            memory_stats = memory_agent.get_memory_statistics(
                code_stats["files_implemented_count"]
            )

            if self.mcp_agent:
                history_result = await self.mcp_agent.call_tool(
                    "get_operation_history", {"last_n": 30}
                )
                history_data = (
                    json.loads(history_result)
                    if isinstance(history_result, str)
                    else history_result
                )
            else:
                history_data = {"total_operations": 0, "history": []}

            write_operations = 0
            files_created = []
            if "history" in history_data:
                for item in history_data["history"]:
                    if item.get("action") == "write_file":
                        write_operations += 1
                        file_path = item.get("details", {}).get("file_path", "unknown")
                        files_created.append(file_path)

            report = f"""
# Pure Code Implementation Completion Report (Write-File-Based Memory Mode)

## Execution Summary
- Implementation iterations: {iterations}
- Total elapsed time: {elapsed_time:.2f} seconds
- Files implemented: {code_stats['total_files_implemented']}
- File write operations: {write_operations}
- Total MCP operations: {history_data.get('total_operations', 0)}

## Read Tools Configuration
- Read tools enabled: {code_stats['read_tools_status']['read_tools_enabled']}
- Status: {code_stats['read_tools_status']['status']}
- Tools affected: {', '.join(code_stats['read_tools_status']['tools_affected'])}

## Agent Performance
### Code Implementation Agent
- Files tracked: {code_stats['files_implemented_count']}
- Technical decisions: {code_stats['technical_decisions_count']}
- Constraints tracked: {code_stats['constraints_count']}
- Architecture notes: {code_stats['architecture_notes_count']}
- Dependency analysis performed: {code_stats['dependency_analysis_count']}
- Files read for dependencies: {code_stats['files_read_for_dependencies']}
- Last summary triggered at file count: {code_stats['last_summary_file_count']}

### Concise Memory Agent (Write-File-Based)
- Last write_file detected: {memory_stats['last_write_file_detected']}
- Should clear memory next: {memory_stats['should_clear_memory_next']}
- Files implemented count: {memory_stats['implemented_files_tracked']}
- Current round: {memory_stats['current_round']}
- Concise mode active: {memory_stats['concise_mode_active']}
- Current round tool results: {memory_stats['current_round_tool_results']}
- Essential tools recorded: {memory_stats['essential_tools_recorded']}

## Files Created
"""
            for file_path in files_created[-20:]:
                report += f"- {file_path}\n"

            if len(files_created) > 20:
                report += f"... and {len(files_created) - 20} more files\n"

            report += """
## Architecture Features
✅ WRITE-FILE-BASED Memory Agent - Clear after each file generation
✅ After write_file: Clear history → Keep system prompt + initial plan + tool results
✅ Tool accumulation: read_code_mem, read_file, search_reference_code until next write_file
✅ Clean memory cycle: write_file → clear → accumulate → write_file → clear
✅ Essential tool recording with write_file detection
✅ Specialized agent separation for clean code organization
✅ MCP-compliant tool execution
✅ Production-grade code with comprehensive type hints
✅ Intelligent dependency analysis and file reading
✅ Automated read_file usage for implementation context
✅ Eliminates conversation clutter between file generations
✅ Focused memory for efficient next file generation
"""
            return report

        except Exception as e:
            self.logger.error(f"Failed to generate final report: {e}")
            return f"Failed to generate final report: {str(e)}"


async def main():
    """Main function for running the workflow"""
    # Configure root logger carefully to avoid duplicates
    root_logger = logging.getLogger()
    if not root_logger.handlers:
        handler = logging.StreamHandler()
        formatter = logging.Formatter("%(levelname)s:%(name)s:%(message)s")
        handler.setFormatter(formatter)
        root_logger.addHandler(handler)
        root_logger.setLevel(logging.INFO)

    workflow = CodeImplementationWorkflowWithIndex()

    print("=" * 60)
    print("Code Implementation Workflow with UNIFIED Reference Indexer")
    print("=" * 60)
    print("Select mode:")
    print("1. Test Code Reference Indexer Integration")
    print("2. Run Full Implementation Workflow")
    print("3. Run Implementation with Pure Code Mode")
    print("4. Test Read Tools Configuration")

    # mode_choice = input("Enter choice (1-4, default: 3): ").strip()

    # For testing purposes, we'll run the test first
    # if mode_choice == "4":
    #     print("Testing Read Tools Configuration...")

    #     # Create a test workflow normally
    #     test_workflow = CodeImplementationWorkflow()

    #     # Create a mock code agent for testing
    #     print("\n🧪 Testing with read tools DISABLED:")
    #     test_agent_disabled = CodeImplementationAgent(None, enable_read_tools=False)
    #     await test_agent_disabled.test_read_tools_configuration()

    #     print("\n🧪 Testing with read tools ENABLED:")
    #     test_agent_enabled = CodeImplementationAgent(None, enable_read_tools=True)
    #     await test_agent_enabled.test_read_tools_configuration()

    #     print("✅ Read tools configuration testing completed!")
    #     return

    # print("Running Code Reference Indexer Integration Test...")

    test_success = True
    if test_success:
        print("\n" + "=" * 60)
        print("🎉 UNIFIED Code Reference Indexer Integration Test PASSED!")
        print("🔧 Three-step process successfully merged into ONE tool")
        print("=" * 60)

        # Ask if user wants to continue with actual workflow
        print("\nContinuing with workflow execution...")

        plan_file = "/Users/lizongwei/Reasearch/DeepCode_Base/DeepCode/deepcode_lab/papers/1/initial_plan.txt"
        # plan_file = "/data2/bjdwhzzh/project-hku/Code-Agent2.0/Code-Agent/deepcode-mcp/agent_folders/papers/1/initial_plan.txt"
        target_directory = (
            "/Users/lizongwei/Reasearch/DeepCode_Base/DeepCode/deepcode_lab/papers/1/"
        )
        print("Implementation Mode Selection:")
        print("1. Pure Code Implementation Mode (Recommended)")
        print("2. Iterative Implementation Mode")

        pure_code_mode = True
        mode_name = "Pure Code Implementation Mode with Memory Agent Architecture + Code Reference Indexer"
        print(f"Using: {mode_name}")

        # Configure read tools - modify this parameter to enable/disable read tools
        enable_read_tools = (
            True  # Set to False to disable read_file and read_code_mem tools
        )
        read_tools_status = "ENABLED" if enable_read_tools else "DISABLED"
        print(f"🔧 Read tools (read_file, read_code_mem): {read_tools_status}")

        # NOTE: To test without read tools, change the line above to:
        # enable_read_tools = False

        result = await workflow.run_workflow(
            plan_file,
            target_directory=target_directory,
            pure_code_mode=pure_code_mode,
            enable_read_tools=enable_read_tools,
        )

        print("=" * 60)
        print("Workflow Execution Results:")
        print(f"Status: {result['status']}")
        print(f"Mode: {mode_name}")

        if result["status"] == "success":
            print(f"Code Directory: {result['code_directory']}")
            print(f"MCP Architecture: {result.get('mcp_architecture', 'unknown')}")
            print("Execution completed!")
        else:
            print(f"Error Message: {result['message']}")

        print("=" * 60)
        print(
            "✅ Using Standard MCP Architecture with Memory Agent + Code Reference Indexer"
        )

    else:
        print("\n" + "=" * 60)
        print("❌ Code Reference Indexer Integration Test FAILED!")
        print("Please check the configuration and try again.")
        print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: workflows/codebase_index_workflow.py
================================================
"""
Codebase Index Workflow

This workflow integrates the functionality of run_indexer.py and code_indexer.py
to build intelligent relationships between existing codebase and target structure.

Features:
- Extract target file structure from initial_plan.txt
- Analyze codebase and build indexes
- Generate relationship mappings and statistical reports
- Provide reference basis for code reproduction
"""

import asyncio
import json
import logging
import os
import re
import sys
from pathlib import Path
from typing import Dict, Any, Optional
import yaml

# Add tools directory to path
sys.path.append(str(Path(__file__).parent.parent / "tools"))

from tools.code_indexer import CodeIndexer


class CodebaseIndexWorkflow:
    """Codebase Index Workflow Class"""

    def __init__(self, logger=None):
        """
        Initialize workflow

        Args:
            logger: Logger instance
        """
        self.logger = logger or self._setup_default_logger()
        self.indexer = None

    def _setup_default_logger(self) -> logging.Logger:
        """Setup default logger"""
        logger = logging.getLogger("CodebaseIndexWorkflow")
        logger.setLevel(logging.INFO)

        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)

        return logger

    def extract_file_tree_from_plan(self, plan_content: str) -> Optional[str]:
        """
        Extract file tree structure from initial_plan.txt content

        Args:
            plan_content: Content of the initial_plan.txt file

        Returns:
            Extracted file tree structure as string
        """
        # Look for file structure section, specifically "## File Structure" format
        file_structure_pattern = r"## File Structure[^\n]*\n```[^\n]*\n(.*?)\n```"

        match = re.search(file_structure_pattern, plan_content, re.DOTALL)
        if match:
            file_tree = match.group(1).strip()
            lines = file_tree.split("\n")

            # Clean tree structure - remove empty lines and comments not part of structure
            cleaned_lines = []
            for line in lines:
                # Keep tree structure lines
                if line.strip() and (
                    any(char in line for char in ["├──", "└──", "│"])
                    or line.strip().endswith("/")
                    or "." in line.split("/")[-1]  # has file extension
                    or line.strip().endswith(".py")
                    or line.strip().endswith(".txt")
                    or line.strip().endswith(".md")
                    or line.strip().endswith(".yaml")
                ):
                    cleaned_lines.append(line)

            if len(cleaned_lines) >= 5:
                file_tree = "\n".join(cleaned_lines)
                self.logger.info(
                    f"📊 Extracted file tree structure from ## File Structure section ({len(cleaned_lines)} lines)"
                )
                return file_tree

        # Fallback: look for any code block containing project structure
        code_block_patterns = [
            r"```[^\n]*\n(project/.*?(?:├──|└──).*?)\n```",
            r"```[^\n]*\n(src/.*?(?:├──|└──).*?)\n```",
            r"```[^\n]*\n(core/.*?(?:├──|└──).*?)\n```",
            r"```[^\n]*\n(.*?(?:├──|└──).*?(?:\.py|\.txt|\.md|\.yaml).*?)\n```",
        ]

        for pattern in code_block_patterns:
            match = re.search(pattern, plan_content, re.DOTALL)
            if match:
                file_tree = match.group(1).strip()
                lines = [line for line in file_tree.split("\n") if line.strip()]
                if len(lines) >= 5:
                    self.logger.info(
                        f"📊 Extracted file tree structure from code block ({len(lines)} lines)"
                    )
                    return file_tree

        # Final fallback: extract file paths from file mentions and create basic structure
        self.logger.warning(
            "⚠️ No standard file tree found, trying to extract from file mentions..."
        )

        # Search for file paths in backticks throughout the document
        file_mentions = re.findall(
            r"`([^`]*(?:\.py|\.txt|\.md|\.yaml|\.yml)[^`]*)`", plan_content
        )

        if file_mentions:
            # Organize files into directory structure
            dirs = set()
            files_by_dir = {}

            for file_path in file_mentions:
                file_path = file_path.strip()
                if "/" in file_path:
                    dir_path = "/".join(file_path.split("/")[:-1])
                    filename = file_path.split("/")[-1]
                    dirs.add(dir_path)
                    if dir_path not in files_by_dir:
                        files_by_dir[dir_path] = []
                    files_by_dir[dir_path].append(filename)
                else:
                    if "root" not in files_by_dir:
                        files_by_dir["root"] = []
                    files_by_dir["root"].append(file_path)

            # Create tree structure
            structure_lines = []

            # Determine root directory name from common patterns
            if any("src/" in f for f in file_mentions):
                root_name = "src"
            elif any("core/" in f for f in file_mentions):
                root_name = "core"
            elif any("lib/" in f for f in file_mentions):
                root_name = "lib"
            else:
                root_name = "project"
            structure_lines.append(f"{root_name}/")

            # Add directories and files
            sorted_dirs = sorted(dirs) if dirs else []
            for i, dir_path in enumerate(sorted_dirs):
                is_last_dir = i == len(sorted_dirs) - 1
                prefix = "└──" if is_last_dir else "├──"
                structure_lines.append(f"{prefix} {dir_path}/")

                if dir_path in files_by_dir:
                    files = sorted(files_by_dir[dir_path])
                    for j, filename in enumerate(files):
                        is_last_file = j == len(files) - 1
                        if is_last_dir:
                            file_prefix = "    └──" if is_last_file else "    ├──"
                        else:
                            file_prefix = "│   └──" if is_last_file else "│   ├──"
                        structure_lines.append(f"{file_prefix} {filename}")

            # Add root files (if any)
            if "root" in files_by_dir:
                root_files = sorted(files_by_dir["root"])
                for i, filename in enumerate(root_files):
                    is_last = (i == len(root_files) - 1) and not sorted_dirs
                    prefix = "└──" if is_last else "├──"
                    structure_lines.append(f"{prefix} {filename}")

            if len(structure_lines) >= 3:
                file_tree = "\n".join(structure_lines)
                self.logger.info(
                    f"📊 Generated file tree from file mentions ({len(structure_lines)} lines)"
                )
                return file_tree

        # If no file tree found, return None
        self.logger.warning("⚠️ No file tree structure found in initial plan")
        return None

    def load_target_structure_from_plan(self, plan_path: str) -> str:
        """
        Load target structure from initial_plan.txt and extract file tree

        Args:
            plan_path: Path to initial_plan.txt file

        Returns:
            Extracted file tree structure
        """
        try:
            # Load complete plan content
            with open(plan_path, "r", encoding="utf-8") as f:
                plan_content = f.read()

            self.logger.info(f"📄 Loaded initial plan ({len(plan_content)} characters)")

            # Extract file tree structure
            file_tree = self.extract_file_tree_from_plan(plan_content)

            if file_tree:
                self.logger.info(
                    "✅ Successfully extracted file tree from initial plan"
                )
                self.logger.info("📋 Extracted structure preview:")
                # Show first few lines of extracted tree
                preview_lines = file_tree.split("\n")[:8]
                for line in preview_lines:
                    self.logger.info(f"   {line}")
                if len(file_tree.split("\n")) > 8:
                    self.logger.info(
                        f"   ... {len(file_tree.split('\n')) - 8} more lines"
                    )
                return file_tree
            else:
                self.logger.warning("⚠️ Unable to extract file tree from initial plan")
                self.logger.info("🔄 Falling back to default target structure")
                return self.get_default_target_structure()

        except Exception as e:
            self.logger.error(f"❌ Failed to load initial plan file {plan_path}: {e}")
            self.logger.info("🔄 Falling back to default target structure")
            return self.get_default_target_structure()

    def get_default_target_structure(self) -> str:
        """Get default target structure"""
        return """
project/
├── src/
│   ├── core/
│   │   ├── gcn.py        # GCN encoder
│   │   ├── diffusion.py  # forward/reverse processes
│   │   ├── denoiser.py   # denoising MLP
│   │   └── fusion.py     # fusion combiner
│   ├── models/           # model wrapper classes
│   │   └── recdiff.py
│   ├── utils/
│   │   ├── data.py       # loading & preprocessing
│   │   ├── predictor.py  # scoring functions
│   │   ├── loss.py       # loss functions
│   │   ├── metrics.py    # NDCG, Recall etc.
│   │   └── sched.py      # beta/alpha schedule utils
│   └── configs/
│       └── default.yaml  # hyperparameters, paths
├── tests/
│   ├── test_gcn.py
│   ├── test_diffusion.py
│   ├── test_denoiser.py
│   ├── test_loss.py
│   └── test_pipeline.py
├── docs/
│   ├── architecture.md
│   ├── api_reference.md
│   └── README.md
├── experiments/
│   ├── run_experiment.py
│   └── notebooks/
│       └── analysis.ipynb
├── requirements.txt
└── setup.py
"""

    def load_or_create_indexer_config(self, paper_dir: str) -> Dict[str, Any]:
        """
        Load or create indexer configuration

        Args:
            paper_dir: Paper directory path

        Returns:
            Configuration dictionary
        """
        # Try to load existing configuration file
        config_path = Path(__file__).parent.parent / "tools" / "indexer_config.yaml"

        try:
            if config_path.exists():
                with open(config_path, "r", encoding="utf-8") as f:
                    config = yaml.safe_load(f)

                # Update path configuration to current paper directory
                if "paths" not in config:
                    config["paths"] = {}
                config["paths"]["code_base_path"] = os.path.join(paper_dir, "code_base")
                config["paths"]["output_dir"] = os.path.join(paper_dir, "indexes")

                # Adjust performance settings for workflow
                if "performance" in config:
                    config["performance"]["enable_concurrent_analysis"] = (
                        False  # Disable concurrency to avoid API limits
                    )
                if "debug" in config:
                    config["debug"]["verbose_output"] = True  # Enable verbose output
                if "llm" in config:
                    config["llm"]["request_delay"] = 0.5  # Increase request delay

                self.logger.info(f"Loaded configuration file: {config_path}")
                return config

        except Exception as e:
            self.logger.warning(f"Failed to load configuration file: {e}")

        # If loading fails, use default configuration
        self.logger.info("Using default configuration")
        default_config = {
            "paths": {
                "code_base_path": os.path.join(paper_dir, "code_base"),
                "output_dir": os.path.join(paper_dir, "indexes"),
            },
            "llm": {
                "model_provider": "anthropic",
                "max_tokens": 4000,
                "temperature": 0.3,
                "request_delay": 0.5,  # Increase request delay
                "max_retries": 3,
                "retry_delay": 1.0,
            },
            "file_analysis": {
                "max_file_size": 1048576,  # 1MB
                "max_content_length": 3000,
                "supported_extensions": [
                    ".py",
                    ".js",
                    ".ts",
                    ".java",
                    ".cpp",
                    ".c",
                    ".h",
                    ".hpp",
                    ".cs",
                    ".php",
                    ".rb",
                    ".go",
                    ".rs",
                    ".scala",
                    ".kt",
                    ".yaml",
                    ".yml",
                    ".json",
                    ".xml",
                    ".toml",
                    ".md",
                    ".txt",
                ],
                "skip_directories": [
                    "__pycache__",
                    "node_modules",
                    "target",
                    "build",
                    "dist",
                    "venv",
                    "env",
                    ".git",
                    ".svn",
                    "data",
                    "datasets",
                ],
            },
            "relationships": {
                "min_confidence_score": 0.3,
                "high_confidence_threshold": 0.7,
                "relationship_types": {
                    "direct_match": 1.0,
                    "partial_match": 0.8,
                    "reference": 0.6,
                    "utility": 0.4,
                },
            },
            "performance": {
                "enable_concurrent_analysis": False,  # Disable concurrency to avoid API limits
                "max_concurrent_files": 3,
                "enable_content_caching": True,
                "max_cache_size": 100,
            },
            "debug": {
                "verbose_output": True,
                "save_raw_responses": False,
                "mock_llm_responses": False,
            },
            "output": {
                "generate_summary": True,
                "generate_statistics": True,
                "include_metadata": True,
                "json_indent": 2,
            },
            "logging": {"level": "INFO", "log_to_file": False},
        }

        return default_config

    async def run_indexing_workflow(
        self,
        paper_dir: str,
        initial_plan_path: Optional[str] = None,
        config_path: str = "mcp_agent.secrets.yaml",
    ) -> Dict[str, Any]:
        """
        Run the complete code indexing workflow

        Args:
            paper_dir: Paper directory path
            initial_plan_path: Initial plan file path (optional)
            config_path: API configuration file path

        Returns:
            Index result dictionary
        """
        try:
            self.logger.info("🚀 Starting codebase index workflow...")

            # Step 1: Determine initial plan file path
            if not initial_plan_path:
                initial_plan_path = os.path.join(paper_dir, "initial_plan.txt")

            # Step 2: Load target structure
            if os.path.exists(initial_plan_path):
                self.logger.info(
                    f"📐 Loading target structure from {initial_plan_path}"
                )
                target_structure = self.load_target_structure_from_plan(
                    initial_plan_path
                )
            else:
                self.logger.warning(
                    f"⚠️ Initial plan file does not exist: {initial_plan_path}"
                )
                self.logger.info("📐 Using default target structure")
                target_structure = self.get_default_target_structure()

            # Step 3: Check codebase path
            code_base_path = os.path.join(paper_dir, "code_base")
            if not os.path.exists(code_base_path):
                self.logger.error(f"❌ Codebase path does not exist: {code_base_path}")
                return {
                    "status": "error",
                    "message": f"Code base path does not exist: {code_base_path}",
                    "output_files": {},
                }

            # Step 4: Create output directory
            output_dir = os.path.join(paper_dir, "indexes")
            os.makedirs(output_dir, exist_ok=True)

            # Step 5: Load configuration
            indexer_config = self.load_or_create_indexer_config(paper_dir)

            self.logger.info(f"📁 Codebase path: {code_base_path}")
            self.logger.info(f"📤 Output directory: {output_dir}")

            # Step 6: Create code indexer
            self.indexer = CodeIndexer(
                code_base_path=code_base_path,
                target_structure=target_structure,
                output_dir=output_dir,
                config_path=config_path,
                enable_pre_filtering=True,
            )

            # Apply configuration settings
            self.indexer.indexer_config = indexer_config

            # Directly set configuration attributes to indexer
            if "file_analysis" in indexer_config:
                file_config = indexer_config["file_analysis"]
                self.indexer.supported_extensions = set(
                    file_config.get(
                        "supported_extensions", self.indexer.supported_extensions
                    )
                )
                self.indexer.skip_directories = set(
                    file_config.get("skip_directories", self.indexer.skip_directories)
                )
                self.indexer.max_file_size = file_config.get(
                    "max_file_size", self.indexer.max_file_size
                )
                self.indexer.max_content_length = file_config.get(
                    "max_content_length", self.indexer.max_content_length
                )

            if "llm" in indexer_config:
                llm_config = indexer_config["llm"]
                self.indexer.model_provider = llm_config.get(
                    "model_provider", self.indexer.model_provider
                )
                self.indexer.llm_max_tokens = llm_config.get(
                    "max_tokens", self.indexer.llm_max_tokens
                )
                self.indexer.llm_temperature = llm_config.get(
                    "temperature", self.indexer.llm_temperature
                )
                self.indexer.request_delay = llm_config.get(
                    "request_delay", self.indexer.request_delay
                )
                self.indexer.max_retries = llm_config.get(
                    "max_retries", self.indexer.max_retries
                )
                self.indexer.retry_delay = llm_config.get(
                    "retry_delay", self.indexer.retry_delay
                )

            if "relationships" in indexer_config:
                rel_config = indexer_config["relationships"]
                self.indexer.min_confidence_score = rel_config.get(
                    "min_confidence_score", self.indexer.min_confidence_score
                )
                self.indexer.high_confidence_threshold = rel_config.get(
                    "high_confidence_threshold", self.indexer.high_confidence_threshold
                )
                self.indexer.relationship_types = rel_config.get(
                    "relationship_types", self.indexer.relationship_types
                )

            if "performance" in indexer_config:
                perf_config = indexer_config["performance"]
                self.indexer.enable_concurrent_analysis = perf_config.get(
                    "enable_concurrent_analysis",
                    self.indexer.enable_concurrent_analysis,
                )
                self.indexer.max_concurrent_files = perf_config.get(
                    "max_concurrent_files", self.indexer.max_concurrent_files
                )
                self.indexer.enable_content_caching = perf_config.get(
                    "enable_content_caching", self.indexer.enable_content_caching
                )
                self.indexer.max_cache_size = perf_config.get(
                    "max_cache_size", self.indexer.max_cache_size
                )

            if "debug" in indexer_config:
                debug_config = indexer_config["debug"]
                self.indexer.verbose_output = debug_config.get(
                    "verbose_output", self.indexer.verbose_output
                )
                self.indexer.save_raw_responses = debug_config.get(
                    "save_raw_responses", self.indexer.save_raw_responses
                )
                self.indexer.mock_llm_responses = debug_config.get(
                    "mock_llm_responses", self.indexer.mock_llm_responses
                )

            if "output" in indexer_config:
                output_config = indexer_config["output"]
                self.indexer.generate_summary = output_config.get(
                    "generate_summary", self.indexer.generate_summary
                )
                self.indexer.generate_statistics = output_config.get(
                    "generate_statistics", self.indexer.generate_statistics
                )
                self.indexer.include_metadata = output_config.get(
                    "include_metadata", self.indexer.include_metadata
                )

            self.logger.info("🔧 Indexer configuration completed")
            self.logger.info(f"🤖 Model provider: {self.indexer.model_provider}")
            self.logger.info(
                f"⚡ Concurrent analysis: {'Enabled' if self.indexer.enable_concurrent_analysis else 'Disabled'}"
            )
            self.logger.info(
                f"🗄️ Content caching: {'Enabled' if self.indexer.enable_content_caching else 'Disabled'}"
            )
            self.logger.info(
                f"🔍 Pre-filtering: {'Enabled' if self.indexer.enable_pre_filtering else 'Disabled'}"
            )

            self.logger.info("=" * 60)
            self.logger.info("🚀 Starting code indexing process...")

            # Step 7: Build all indexes
            output_files = await self.indexer.build_all_indexes()

            # Step 8: Generate summary report
            if output_files:
                summary_report = self.indexer.generate_summary_report(output_files)

                self.logger.info("=" * 60)
                self.logger.info("✅ Indexing completed successfully!")
                self.logger.info(f"📊 Processed {len(output_files)} repositories")
                self.logger.info("📁 Generated index files:")
                for repo_name, file_path in output_files.items():
                    self.logger.info(f"   📄 {repo_name}: {file_path}")
                self.logger.info(f"📋 Summary report: {summary_report}")

                # Statistics (if enabled)
                if self.indexer.generate_statistics:
                    self.logger.info("\n📈 Processing statistics:")
                    total_relationships = 0
                    high_confidence_relationships = 0

                    for file_path in output_files.values():
                        try:
                            with open(file_path, "r", encoding="utf-8") as f:
                                index_data = json.load(f)
                                relationships = index_data.get("relationships", [])
                                total_relationships += len(relationships)
                                high_confidence_relationships += len(
                                    [
                                        r
                                        for r in relationships
                                        if r.get("confidence_score", 0)
                                        > self.indexer.high_confidence_threshold
                                    ]
                                )
                        except Exception as e:
                            self.logger.warning(
                                f"   ⚠️ Unable to load statistics from {file_path}: {e}"
                            )

                    self.logger.info(
                        f"   🔗 Total relationships found: {total_relationships}"
                    )
                    self.logger.info(
                        f"   ⭐ High confidence relationships: {high_confidence_relationships}"
                    )
                    self.logger.info(
                        f"   📊 Average relationships per repository: {total_relationships / len(output_files) if output_files else 0:.1f}"
                    )

                self.logger.info("\n🎉 Code indexing process completed successfully!")

                return {
                    "status": "success",
                    "message": f"Successfully indexed {len(output_files)} repositories",
                    "output_files": output_files,
                    "summary_report": summary_report,
                    "statistics": {
                        "total_repositories": len(output_files),
                        "total_relationships": total_relationships,
                        "high_confidence_relationships": high_confidence_relationships,
                    }
                    if self.indexer.generate_statistics
                    else None,
                }
            else:
                self.logger.warning("⚠️ No index files generated")
                return {
                    "status": "warning",
                    "message": "No index files were generated",
                    "output_files": {},
                }

        except Exception as e:
            self.logger.error(f"❌ Index workflow failed: {e}")
            # If there are detailed error messages, log them
            import traceback

            self.logger.error(f"Detailed error information: {traceback.format_exc()}")
            return {"status": "error", "message": str(e), "output_files": {}}

    def print_banner(self):
        """Print application banner"""
        banner = """
╔═══════════════════════════════════════════════════════════════════════╗
║                    🔍 Codebase Index Workflow v1.0                   ║
║              Intelligent Code Relationship Analysis Tool              ║
╠═══════════════════════════════════════════════════════════════════════╣
║  📁 Analyzes existing codebases                                      ║
║  🔗 Builds intelligent relationships with target structure           ║
║  🤖 Powered by LLM analysis                                          ║
║  📊 Generates detailed JSON indexes                                   ║
║  🎯 Provides reference for code reproduction                          ║
╚═══════════════════════════════════════════════════════════════════════╝
        """
        print(banner)


# Convenience function for direct workflow invocation
async def run_codebase_indexing(
    paper_dir: str,
    initial_plan_path: Optional[str] = None,
    config_path: str = "mcp_agent.secrets.yaml",
    logger=None,
) -> Dict[str, Any]:
    """
    Convenience function to run codebase indexing

    Args:
        paper_dir: Paper directory path
        initial_plan_path: Initial plan file path (optional)
        config_path: API configuration file path
        logger: Logger instance (optional)

    Returns:
        Index result dictionary
    """
    workflow = CodebaseIndexWorkflow(logger=logger)
    workflow.print_banner()

    return await workflow.run_indexing_workflow(
        paper_dir=paper_dir,
        initial_plan_path=initial_plan_path,
        config_path=config_path,
    )


# Main function for testing
async def main():
    """Main function for testing workflow"""
    import logging

    # Setup logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    # Test parameters
    paper_dir = "./deepcode_lab/papers/1"
    initial_plan_path = os.path.join(paper_dir, "initial_plan.txt")

    # Run workflow
    result = await run_codebase_indexing(
        paper_dir=paper_dir, initial_plan_path=initial_plan_path, logger=logger
    )

    logger.info(f"Index result: {result}")


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: workflows/agents/__init__.py
================================================
"""
Agents Package for Code Implementation Workflow

This package contains specialized agents for different aspects of code implementation:
- CodeImplementationAgent: Handles file-by-file code generation
- ConciseMemoryAgent: Manages memory optimization and consistency across phases
"""

from .code_implementation_agent import CodeImplementationAgent
from .memory_agent_concise import ConciseMemoryAgent as MemoryAgent

__all__ = ["CodeImplementationAgent", "MemoryAgent"]



================================================
FILE: workflows/agents/code_implementation_agent.py
================================================
"""
Code Implementation Agent for File-by-File Development

Handles systematic code implementation with progress tracking and
memory optimization for long-running development sessions.
"""

import json
import time
import logging
from typing import Dict, Any, List, Optional

# Import tiktoken for token calculation
try:
    import tiktoken

    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False

# Import prompts from code_prompts
import sys
import os

sys.path.insert(
    0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
)
from prompts.code_prompts import (
    GENERAL_CODE_IMPLEMENTATION_SYSTEM_PROMPT,
)


class CodeImplementationAgent:
    """
    Code Implementation Agent for systematic file-by-file development

    Responsibilities:
    - Track file implementation progress
    - Execute MCP tool calls for code generation
    - Monitor implementation status
    - Coordinate with Summary Agent for memory optimization
    - Calculate token usage for context management
    """

    def __init__(
        self,
        mcp_agent,
        logger: Optional[logging.Logger] = None,
        enable_read_tools: bool = True,
    ):
        """
        Initialize Code Implementation Agent

        Args:
            mcp_agent: MCP agent instance for tool calls
            logger: Logger instance for tracking operations
            enable_read_tools: Whether to enable read_file and read_code_mem tools (default: True)
        """
        self.mcp_agent = mcp_agent
        self.logger = logger or self._create_default_logger()
        self.enable_read_tools = enable_read_tools  # Control read tools execution

        self.implementation_summary = {
            "completed_files": [],
            "technical_decisions": [],
            "important_constraints": [],
            "architecture_notes": [],
            "dependency_analysis": [],  # Track dependency analysis and file reads
        }
        self.files_implemented_count = 0
        self.implemented_files_set = (
            set()
        )  # Track unique file paths to avoid duplicate counting
        self.files_read_for_dependencies = (
            set()
        )  # Track files read for dependency analysis
        self.last_summary_file_count = (
            0  # Track the file count when last summary was triggered
        )

        # Token calculation settings
        self.max_context_tokens = (
            200000  # Default max context tokens for Claude-3.5-Sonnet
        )
        self.token_buffer = 10000  # Safety buffer before reaching max
        self.summary_trigger_tokens = (
            self.max_context_tokens - self.token_buffer
        )  # Trigger summary when approaching limit
        self.last_summary_token_count = (
            0  # Track token count when last summary was triggered
        )

        # Initialize tokenizer
        if TIKTOKEN_AVAILABLE:
            try:
                # Use Claude-3 tokenizer (approximation with OpenAI's o200k_base)
                self.tokenizer = tiktoken.get_encoding("o200k_base")
                self.logger.info("Token calculation enabled with o200k_base encoding")
            except Exception as e:
                self.tokenizer = None
                self.logger.warning(f"Failed to initialize tokenizer: {e}")
        else:
            self.tokenizer = None
            self.logger.warning(
                "tiktoken not available, token-based summary triggering disabled"
            )

        # Analysis loop detection
        self.recent_tool_calls = []  # Track recent tool calls to detect analysis loops
        self.max_read_without_write = 5  # Max read_file calls without write_file

        # Memory agent integration
        self.memory_agent = None  # Will be set externally
        self.llm_client = None  # Will be set externally
        self.llm_client_type = None  # Will be set externally

        # Log read tools configuration
        read_tools_status = "ENABLED" if self.enable_read_tools else "DISABLED"
        self.logger.info(
            f"🔧 Code Implementation Agent initialized - Read tools: {read_tools_status}"
        )
        if not self.enable_read_tools:
            self.logger.info(
                "🚫 Testing mode: read_file and read_code_mem will be skipped when called"
            )

    def _create_default_logger(self) -> logging.Logger:
        """Create default logger if none provided"""
        logger = logging.getLogger(f"{__name__}.CodeImplementationAgent")
        # Don't add handlers to child loggers - let them propagate to root
        logger.setLevel(logging.INFO)
        return logger

    def get_system_prompt(self) -> str:
        """
        Get the system prompt for code implementation
        """
        return GENERAL_CODE_IMPLEMENTATION_SYSTEM_PROMPT

    def set_memory_agent(self, memory_agent, llm_client=None, llm_client_type=None):
        """
        Set memory agent for code summary generation

        Args:
            memory_agent: Memory agent instance
            llm_client: LLM client for summary generation
            llm_client_type: Type of LLM client ("anthropic" or "openai")
        """
        self.memory_agent = memory_agent
        self.llm_client = llm_client
        self.llm_client_type = llm_client_type
        self.logger.info("Memory agent integration configured")

    async def execute_tool_calls(self, tool_calls: List[Dict]) -> List[Dict]:
        """
        Execute MCP tool calls and track implementation progress

        Args:
            tool_calls: List of tool calls to execute

        Returns:
            List of tool execution results
        """
        results = []

        for tool_call in tool_calls:
            tool_name = tool_call["name"]
            tool_input = tool_call["input"]

            self.logger.info(f"Executing MCP tool: {tool_name}")

            try:
                # Check if read tools are disabled
                if not self.enable_read_tools and tool_name in [
                    "read_file",
                    "read_code_mem",
                ]:
                    # self.logger.info(f"🚫 SKIPPING {tool_name} - Read tools disabled for testing")
                    # Return a mock result indicating the tool was skipped
                    mock_result = json.dumps(
                        {
                            "status": "skipped",
                            "message": f"{tool_name} tool disabled for testing",
                            "tool_disabled": True,
                            "original_input": tool_input,
                        },
                        ensure_ascii=False,
                    )

                    results.append(
                        {
                            "tool_id": tool_call["id"],
                            "tool_name": tool_name,
                            "result": mock_result,
                        }
                    )
                    continue

                # read_code_mem is now a proper MCP tool, no special handling needed

                # INTERCEPT read_file calls - redirect to read_code_mem first if memory agent is available
                if tool_name == "read_file":
                    file_path = tool_call["input"].get("file_path", "unknown")
                    self.logger.info(f"🔍 READ_FILE CALL DETECTED: {file_path}")
                    self.logger.info(
                        f"📊 Files implemented count: {self.files_implemented_count}"
                    )
                    self.logger.info(
                        f"🧠 Memory agent available: {self.memory_agent is not None}"
                    )

                    # Enable optimization if memory agent is available (more aggressive approach)
                    if self.memory_agent is not None:
                        self.logger.info(
                            f"🔄 INTERCEPTING read_file call for {file_path} (memory agent available)"
                        )
                        result = await self._handle_read_file_with_memory_optimization(
                            tool_call
                        )
                        results.append(result)
                        continue
                    else:
                        self.logger.info(
                            "📁 NO INTERCEPTION: no memory agent available"
                        )

                if self.mcp_agent:
                    # Execute tool call through MCP protocol
                    result = await self.mcp_agent.call_tool(tool_name, tool_input)

                    # Track file implementation progress
                    if tool_name == "write_file":
                        await self._track_file_implementation_with_summary(
                            tool_call, result
                        )
                    elif tool_name == "read_file":
                        self._track_dependency_analysis(tool_call, result)

                    # Track tool calls for analysis loop detection
                    self._track_tool_call_for_loop_detection(tool_name)

                    results.append(
                        {
                            "tool_id": tool_call["id"],
                            "tool_name": tool_name,
                            "result": result,
                        }
                    )
                else:
                    results.append(
                        {
                            "tool_id": tool_call["id"],
                            "tool_name": tool_name,
                            "result": json.dumps(
                                {
                                    "status": "error",
                                    "message": "MCP agent not initialized",
                                },
                                ensure_ascii=False,
                            ),
                        }
                    )

            except Exception as e:
                self.logger.error(f"MCP tool execution failed: {e}")
                results.append(
                    {
                        "tool_id": tool_call["id"],
                        "tool_name": tool_name,
                        "result": json.dumps(
                            {"status": "error", "message": str(e)}, ensure_ascii=False
                        ),
                    }
                )

        return results

    # _handle_read_code_mem method removed - read_code_mem is now a proper MCP tool

    async def _handle_read_file_with_memory_optimization(self, tool_call: Dict) -> Dict:
        """
        Intercept read_file calls and redirect to read_code_mem if a summary exists.
        This prevents unnecessary file reads if the summary is already available.
        """
        file_path = tool_call["input"].get("file_path")
        if not file_path:
            return {
                "tool_id": tool_call["id"],
                "tool_name": "read_file",
                "result": json.dumps(
                    {"status": "error", "message": "file_path parameter is required"},
                    ensure_ascii=False,
                ),
            }

        # Check if a summary exists for this file using read_code_mem MCP tool
        should_use_summary = False
        if self.memory_agent and self.mcp_agent:
            try:
                # Use read_code_mem MCP tool to check if summary exists (pass file path as list)
                read_code_mem_result = await self.mcp_agent.call_tool(
                    "read_code_mem", {"file_paths": [file_path]}
                )

                # Parse the result to check if summary was found
                import json

                if isinstance(read_code_mem_result, str):
                    try:
                        result_data = json.loads(read_code_mem_result)
                        # Check if any summaries were found in the results
                        should_use_summary = (
                            result_data.get("status")
                            in ["all_summaries_found", "partial_summaries_found"]
                            and result_data.get("summaries_found", 0) > 0
                        )
                    except json.JSONDecodeError:
                        should_use_summary = False
            except Exception as e:
                self.logger.debug(f"read_code_mem check failed for {file_path}: {e}")
                should_use_summary = False

        if should_use_summary:
            self.logger.info(f"🔄 READ_FILE INTERCEPTED: Using summary for {file_path}")

            # Use the MCP agent to call read_code_mem tool
            if self.mcp_agent:
                result = await self.mcp_agent.call_tool(
                    "read_code_mem", {"file_paths": [file_path]}
                )

                # Modify the result to indicate it was originally a read_file call
                import json

                try:
                    result_data = (
                        json.loads(result) if isinstance(result, str) else result
                    )
                    if isinstance(result_data, dict):
                        # Extract the specific file result for the single file we requested
                        file_results = result_data.get("results", [])
                        if file_results and len(file_results) > 0:
                            specific_result = file_results[
                                0
                            ]  # Get the first (and only) result
                            # Transform to match the old single-file format for backward compatibility
                            transformed_result = {
                                "status": specific_result.get("status", "no_summary"),
                                "file_path": specific_result.get(
                                    "file_path", file_path
                                ),
                                "summary_content": specific_result.get(
                                    "summary_content"
                                ),
                                "message": specific_result.get("message", ""),
                                "original_tool": "read_file",
                                "optimization": "redirected_to_read_code_mem",
                            }
                            final_result = json.dumps(
                                transformed_result, ensure_ascii=False
                            )
                        else:
                            # Fallback if no results
                            result_data["original_tool"] = "read_file"
                            result_data["optimization"] = "redirected_to_read_code_mem"
                            final_result = json.dumps(result_data, ensure_ascii=False)
                    else:
                        final_result = result
                except (json.JSONDecodeError, TypeError):
                    final_result = result

                return {
                    "tool_id": tool_call["id"],
                    "tool_name": "read_file",  # Keep original tool name for tracking
                    "result": final_result,
                }
            else:
                self.logger.warning(
                    "MCP agent not available for read_code_mem optimization"
                )
        else:
            self.logger.info(
                f"📁 READ_FILE: No summary for {file_path}, using actual file"
            )

            # Execute the original read_file call
            if self.mcp_agent:
                result = await self.mcp_agent.call_tool("read_file", tool_call["input"])

                # Track dependency analysis for the actual file read
                self._track_dependency_analysis(tool_call, result)

                # Track tool calls for analysis loop detection
                self._track_tool_call_for_loop_detection("read_file")

                return {
                    "tool_id": tool_call["id"],
                    "tool_name": "read_file",
                    "result": result,
                }
            else:
                return {
                    "tool_id": tool_call["id"],
                    "tool_name": "read_file",
                    "result": json.dumps(
                        {"status": "error", "message": "MCP agent not initialized"},
                        ensure_ascii=False,
                    ),
                }

    async def _track_file_implementation_with_summary(
        self, tool_call: Dict, result: Any
    ):
        """
        Track file implementation and create code summary

        Args:
            tool_call: The write_file tool call
            result: Result of the tool execution
        """
        # First do the regular tracking
        self._track_file_implementation(tool_call, result)

        # Then create and save code summary if memory agent is available
        if self.memory_agent and self.llm_client and self.llm_client_type:
            try:
                file_path = tool_call["input"].get("file_path")
                file_content = tool_call["input"].get("content", "")

                if file_path and file_content:
                    # Create code implementation summary
                    summary = await self.memory_agent.create_code_implementation_summary(
                        self.llm_client,
                        self.llm_client_type,
                        file_path,
                        file_content,
                        self.get_files_implemented_count(),  # Pass the current file count
                    )

                    self.logger.info(
                        f"Created code summary for implemented file: {file_path}, summary: {summary[:100]}..."
                    )
                else:
                    self.logger.warning(
                        "Missing file path or content for summary generation"
                    )

            except Exception as e:
                self.logger.error(f"Failed to create code summary: {e}")

    def _track_file_implementation(self, tool_call: Dict, result: Any):
        """
        Track file implementation progress
        """
        try:
            # Handle different result types from MCP
            result_data = None

            # Check if result is a CallToolResult object
            if hasattr(result, "content"):
                # Extract content from CallToolResult
                if hasattr(result.content, "text"):
                    result_content = result.content.text
                else:
                    result_content = str(result.content)

                # Try to parse as JSON
                try:
                    result_data = json.loads(result_content)
                except json.JSONDecodeError:
                    # If not JSON, create a structure
                    result_data = {
                        "status": "success",
                        "file_path": tool_call["input"].get("file_path", "unknown"),
                    }
            elif isinstance(result, str):
                # Try to parse string result
                try:
                    result_data = json.loads(result)
                except json.JSONDecodeError:
                    result_data = {
                        "status": "success",
                        "file_path": tool_call["input"].get("file_path", "unknown"),
                    }
            elif isinstance(result, dict):
                # Direct dictionary result
                result_data = result
            else:
                # Fallback: assume success and extract file path from input
                result_data = {
                    "status": "success",
                    "file_path": tool_call["input"].get("file_path", "unknown"),
                }

            # Extract file path for tracking
            file_path = None
            if result_data and result_data.get("status") == "success":
                file_path = result_data.get(
                    "file_path", tool_call["input"].get("file_path", "unknown")
                )
            else:
                file_path = tool_call["input"].get("file_path")

            # Only count unique files, not repeated tool calls on same file
            if file_path and file_path not in self.implemented_files_set:
                # This is a new file implementation
                self.implemented_files_set.add(file_path)
                self.files_implemented_count += 1
                # self.logger.info(f"New file implementation tracked: count={self.files_implemented_count}, file={file_path}")
                # print(f"New file implementation tracked: count={self.files_implemented_count}, file={file_path}")

                # Add to completed files list
                self.implementation_summary["completed_files"].append(
                    {
                        "file": file_path,
                        "iteration": self.files_implemented_count,
                        "timestamp": time.time(),
                        "size": result_data.get("size", 0) if result_data else 0,
                    }
                )

                # self.logger.info(
                #     f"New file implementation tracked: count={self.files_implemented_count}, file={file_path}"
                # )
                # print(f"📝 NEW FILE IMPLEMENTED: count={self.files_implemented_count}, file={file_path}")
                # print(f"🔧 OPTIMIZATION NOW ENABLED: files_implemented_count > 0 = {self.files_implemented_count > 0}")
            elif file_path and file_path in self.implemented_files_set:
                # This file was already implemented (duplicate tool call)
                self.logger.debug(
                    f"File already tracked, skipping duplicate count: {file_path}"
                )
            else:
                # No valid file path found
                self.logger.warning("No valid file path found for tracking")

        except Exception as e:
            self.logger.warning(f"Failed to track file implementation: {e}")
            # Even if tracking fails, try to count based on tool input (but check for duplicates)

            file_path = tool_call["input"].get("file_path")
            if file_path and file_path not in self.implemented_files_set:
                self.implemented_files_set.add(file_path)
                self.files_implemented_count += 1
                self.logger.info(
                    f"File implementation counted (emergency fallback): count={self.files_implemented_count}, file={file_path}"
                )

    def _track_dependency_analysis(self, tool_call: Dict, result: Any):
        """
        Track dependency analysis through read_file calls
        """
        try:
            file_path = tool_call["input"].get("file_path")
            if file_path:
                # Track unique files read for dependency analysis
                if file_path not in self.files_read_for_dependencies:
                    self.files_read_for_dependencies.add(file_path)

                    # Add to dependency analysis summary
                    self.implementation_summary["dependency_analysis"].append(
                        {
                            "file_read": file_path,
                            "timestamp": time.time(),
                            "purpose": "dependency_analysis",
                        }
                    )

                    self.logger.info(
                        f"Dependency analysis tracked: file_read={file_path}"
                    )

        except Exception as e:
            self.logger.warning(f"Failed to track dependency analysis: {e}")

    def calculate_messages_token_count(self, messages: List[Dict]) -> int:
        """
        Calculate total token count for a list of messages

        Args:
            messages: List of chat messages with 'role' and 'content' keys

        Returns:
            Total token count
        """
        if not self.tokenizer:
            # Fallback: rough estimation based on character count
            total_chars = sum(len(str(msg.get("content", ""))) for msg in messages)
            # Rough approximation: 1 token ≈ 4 characters
            return total_chars // 4

        try:
            total_tokens = 0
            for message in messages:
                content = str(message.get("content", ""))
                role = message.get("role", "")

                # Count tokens for content
                if content:
                    content_tokens = len(
                        self.tokenizer.encode(content, disallowed_special=())
                    )
                    total_tokens += content_tokens

                # Add tokens for role and message structure
                role_tokens = len(self.tokenizer.encode(role, disallowed_special=()))
                total_tokens += role_tokens + 4  # Extra tokens for message formatting

            return total_tokens

        except Exception as e:
            self.logger.warning(f"Token calculation failed: {e}")
            # Fallback estimation
            total_chars = sum(len(str(msg.get("content", ""))) for msg in messages)
            return total_chars // 4

    def should_trigger_summary_by_tokens(self, messages: List[Dict]) -> bool:
        """
        Check if summary should be triggered based on token count

        Args:
            messages: Current conversation messages

        Returns:
            True if summary should be triggered based on token count
        """
        if not messages:
            return False

        # Calculate current token count / 计算当前token数
        current_token_count = self.calculate_messages_token_count(messages)

        # Check if we should trigger summary / 检查是否应触发总结
        should_trigger = (
            current_token_count > self.summary_trigger_tokens
            and current_token_count
            > self.last_summary_token_count
            + 10000  # Minimum 10k tokens between summaries / 总结间最少10k tokens
        )

        if should_trigger:
            self.logger.info(
                f"Token-based summary trigger: current={current_token_count:,}, "
                f"threshold={self.summary_trigger_tokens:,}, "
                f"last_summary={self.last_summary_token_count:,}"
            )

        return should_trigger

    def should_trigger_summary(
        self, summary_trigger: int = 5, messages: List[Dict] = None
    ) -> bool:
        """
        Check if summary should be triggered based on token count (preferred) or file count (fallback)
        根据token数（首选）或文件数（回退）检查是否应触发总结

        Args:
            summary_trigger: Number of files after which to trigger summary (fallback)
            messages: Current conversation messages for token calculation

        Returns:
            True if summary should be triggered
        """
        # Primary: Token-based triggering / 主要：基于token的触发
        if messages and self.tokenizer:
            return self.should_trigger_summary_by_tokens(messages)

        # Fallback: File-based triggering (original logic) / 回退：基于文件的触发（原始逻辑）
        self.logger.info("Using fallback file-based summary triggering")
        should_trigger = (
            self.files_implemented_count > 0
            and self.files_implemented_count % summary_trigger == 0
            and self.files_implemented_count > self.last_summary_file_count
        )

        return should_trigger

    def mark_summary_triggered(self, messages: List[Dict] = None):
        """
        Mark that summary has been triggered for current state
        标记当前状态的总结已被触发

        Args:
            messages: Current conversation messages for token tracking
        """
        # Update file-based tracking / 更新基于文件的跟踪
        self.last_summary_file_count = self.files_implemented_count

        # Update token-based tracking / 更新基于token的跟踪
        if messages and self.tokenizer:
            self.last_summary_token_count = self.calculate_messages_token_count(
                messages
            )
            self.logger.info(
                f"Summary marked as triggered - file_count: {self.files_implemented_count}, "
                f"token_count: {self.last_summary_token_count:,}"
            )
        else:
            self.logger.info(
                f"Summary marked as triggered for file count: {self.files_implemented_count}"
            )

    def get_implementation_summary(self) -> Dict[str, Any]:
        """
        Get current implementation summary
        获取当前实现总结
        """
        return self.implementation_summary.copy()

    def get_files_implemented_count(self) -> int:
        """
        Get the number of files implemented so far
        获取到目前为止实现的文件数量
        """
        return self.files_implemented_count

    def get_read_tools_status(self) -> Dict[str, Any]:
        """
        Get read tools configuration status
        获取读取工具配置状态

        Returns:
            Dictionary with read tools status information
        """
        return {
            "read_tools_enabled": self.enable_read_tools,
            "status": "ENABLED" if self.enable_read_tools else "DISABLED",
            "tools_affected": ["read_file", "read_code_mem"],
            "description": "Read tools configuration for testing purposes",
        }

    def add_technical_decision(self, decision: str, context: str = ""):
        """
        Add a technical decision to the implementation summary
        向实现总结添加技术决策

        Args:
            decision: Description of the technical decision
            context: Additional context for the decision
        """
        self.implementation_summary["technical_decisions"].append(
            {"decision": decision, "context": context, "timestamp": time.time()}
        )
        self.logger.info(f"Technical decision recorded: {decision}")

    def add_constraint(self, constraint: str, impact: str = ""):
        """
        Add an important constraint to the implementation summary
        向实现总结添加重要约束

        Args:
            constraint: Description of the constraint
            impact: Impact of the constraint on implementation
        """
        self.implementation_summary["important_constraints"].append(
            {"constraint": constraint, "impact": impact, "timestamp": time.time()}
        )
        self.logger.info(f"Constraint recorded: {constraint}")

    def add_architecture_note(self, note: str, component: str = ""):
        """
        Add an architecture note to the implementation summary
        向实现总结添加架构注释

        Args:
            note: Architecture note description
            component: Related component or module
        """
        self.implementation_summary["architecture_notes"].append(
            {"note": note, "component": component, "timestamp": time.time()}
        )
        self.logger.info(f"Architecture note recorded: {note}")

    def get_implementation_statistics(self) -> Dict[str, Any]:
        """
        Get comprehensive implementation statistics
        获取全面的实现统计信息
        """
        return {
            "total_files_implemented": self.files_implemented_count,
            "files_implemented_count": self.files_implemented_count,
            "technical_decisions_count": len(
                self.implementation_summary["technical_decisions"]
            ),
            "constraints_count": len(
                self.implementation_summary["important_constraints"]
            ),
            "architecture_notes_count": len(
                self.implementation_summary["architecture_notes"]
            ),
            "dependency_analysis_count": len(
                self.implementation_summary["dependency_analysis"]
            ),
            "files_read_for_dependencies": len(self.files_read_for_dependencies),
            "unique_files_implemented": len(self.implemented_files_set),
            "completed_files_list": [
                f["file"] for f in self.implementation_summary["completed_files"]
            ],
            "dependency_files_read": list(self.files_read_for_dependencies),
            "last_summary_file_count": self.last_summary_file_count,
            "read_tools_status": self.get_read_tools_status(),  # Include read tools configuration
        }

    def force_enable_optimization(self):
        """
        Force enable optimization for testing purposes
        强制启用优化用于测试目的
        """
        self.files_implemented_count = 1
        self.logger.info(
            f"🔧 OPTIMIZATION FORCE ENABLED: files_implemented_count set to {self.files_implemented_count}"
        )
        print(
            f"🔧 OPTIMIZATION FORCE ENABLED: files_implemented_count set to {self.files_implemented_count}"
        )

    def reset_implementation_tracking(self):
        """
        Reset implementation tracking (useful for new sessions)
        重置实现跟踪（对新会话有用）
        """
        self.implementation_summary = {
            "completed_files": [],
            "technical_decisions": [],
            "important_constraints": [],
            "architecture_notes": [],
            "dependency_analysis": [],  # Reset dependency analysis and file reads
        }
        self.files_implemented_count = 0
        self.implemented_files_set = (
            set()
        )  # Reset the unique files set / 重置唯一文件集合
        self.files_read_for_dependencies = (
            set()
        )  # Reset files read for dependency analysis / 重置为依赖分析而读取的文件
        self.last_summary_file_count = 0  # Reset the file count when last summary was triggered / 重置上次触发总结时的文件数
        self.last_summary_token_count = 0  # Reset token count when last summary was triggered / 重置上次触发总结时的token数
        self.logger.info("Implementation tracking reset")

        # Reset analysis loop detection / 重置分析循环检测
        self.recent_tool_calls = []
        self.logger.info("Analysis loop detection reset")

    def _track_tool_call_for_loop_detection(self, tool_name: str):
        """
        Track tool calls for analysis loop detection
        跟踪工具调用以检测分析循环

        Args:
            tool_name: Name of the tool called
        """
        self.recent_tool_calls.append(tool_name)
        if len(self.recent_tool_calls) > self.max_read_without_write:
            self.recent_tool_calls.pop(0)

        if len(set(self.recent_tool_calls)) == 1:
            self.logger.warning("Analysis loop detected")

    def is_in_analysis_loop(self) -> bool:
        """
        Check if the agent is in an analysis loop (only reading files, not writing)
        检查代理是否在分析循环中（只读文件，不写文件）

        Returns:
            True if in analysis loop
        """
        if len(self.recent_tool_calls) < self.max_read_without_write:
            return False

        # Check if recent calls are all read_file or search_reference_code / 检查最近的调用是否都是read_file或search_reference_code
        analysis_tools = {
            "read_file",
            "search_reference_code",
            "get_all_available_references",
        }
        recent_calls_set = set(self.recent_tool_calls)

        # If all recent calls are analysis tools, we're in an analysis loop / 如果最近的调用都是分析工具，我们在分析循环中
        in_loop = (
            recent_calls_set.issubset(analysis_tools) and len(recent_calls_set) >= 1
        )

        if in_loop:
            self.logger.warning(
                f"Analysis loop detected! Recent calls: {self.recent_tool_calls}"
            )

        return in_loop

    def get_analysis_loop_guidance(self) -> str:
        """
        Get guidance to break out of analysis loop
        获取跳出分析循环的指导

        Returns:
            Guidance message to encourage implementation
        """
        return f"""🚨 **ANALYSIS LOOP DETECTED - IMMEDIATE ACTION REQUIRED**

**Problem**: You've been reading/analyzing files for {len(self.recent_tool_calls)} consecutive calls without writing code.
**Recent tool calls**: {' → '.join(self.recent_tool_calls)}

**SOLUTION - IMPLEMENT CODE NOW**:
1. **STOP ANALYZING** - You have enough information
2. **Use write_file** to create the next code file according to the implementation plan
3. **Choose ANY file** from the plan that hasn't been implemented yet
4. **Write complete, working code** - don't ask for permission or clarification

**Files implemented so far**: {self.files_implemented_count}
**Your goal**: Implement MORE files, not analyze existing ones!

**CRITICAL**: Your next response MUST use write_file to create a new code file!"""

    async def test_summary_functionality(self, test_file_path: str = None):
        """
        Test if the code summary functionality is working correctly
        测试代码总结功能是否正常工作

        Args:
            test_file_path: Specific file to test, if None will test all implemented files
        """
        if not self.memory_agent:
            self.logger.warning("No memory agent available for testing")
            return

        if test_file_path:
            files_to_test = [test_file_path]
        else:
            # Use implemented files from tracking
            files_to_test = list(self.implemented_files_set)[
                :3
            ]  # Limit to first 3 files

        if not files_to_test:
            self.logger.warning("No implemented files to test")
            return

        # Test each file silently
        summary_files_found = 0

        for file_path in files_to_test:
            if self.mcp_agent:
                try:
                    result = await self.mcp_agent.call_tool(
                        "read_code_mem", {"file_paths": [file_path]}
                    )

                    # Parse the result to check if summary was found
                    import json

                    result_data = (
                        json.loads(result) if isinstance(result, str) else result
                    )

                    if (
                        result_data.get("status")
                        in ["all_summaries_found", "partial_summaries_found"]
                        and result_data.get("summaries_found", 0) > 0
                    ):
                        summary_files_found += 1
                except Exception as e:
                    self.logger.warning(
                        f"Failed to test read_code_mem for {file_path}: {e}"
                    )
            else:
                self.logger.warning("MCP agent not available for testing")

        self.logger.info(
            f"📋 Summary testing: {summary_files_found}/{len(files_to_test)} files have summaries"
        )

    async def test_automatic_read_file_optimization(self):
        """
        Test the automatic read_file optimization that redirects to read_code_mem
        测试自动read_file优化，重定向到read_code_mem
        """
        print("=" * 80)
        print("🔄 TESTING AUTOMATIC READ_FILE OPTIMIZATION")
        print("=" * 80)

        # Simulate that at least one file has been implemented (to trigger optimization)
        self.files_implemented_count = 1

        # Test with a generic config file that should have a summary
        test_file = "config.py"

        print(f"📁 Testing automatic optimization for: {test_file}")
        print(f"📊 Files implemented count: {self.files_implemented_count}")
        print(
            f"🔧 Optimization should be: {'ENABLED' if self.files_implemented_count > 0 else 'DISABLED'}"
        )

        # Create a simulated read_file tool call
        simulated_read_file_call = {
            "id": "test_read_file_optimization",
            "name": "read_file",
            "input": {"file_path": test_file},
        }

        print("\n🔄 Simulating read_file call:")
        print(f"   Tool: {simulated_read_file_call['name']}")
        print(f"   File: {simulated_read_file_call['input']['file_path']}")

        # Execute the tool call (this should trigger automatic optimization)
        results = await self.execute_tool_calls([simulated_read_file_call])

        if results:
            result = results[0]
            print("\n✅ Tool execution completed:")
            print(f"   Tool name: {result.get('tool_name', 'N/A')}")
            print(f"   Tool ID: {result.get('tool_id', 'N/A')}")

            # Parse the result to check if optimization occurred
            import json

            try:
                result_data = json.loads(result.get("result", "{}"))
                if result_data.get("optimization") == "redirected_to_read_code_mem":
                    print("🎉 SUCCESS: read_file was automatically optimized!")
                    print(
                        f"   Original tool: {result_data.get('original_tool', 'N/A')}"
                    )
                    print(f"   Status: {result_data.get('status', 'N/A')}")
                elif result_data.get("status") == "summary_found":
                    print("🎉 SUCCESS: Summary was found and returned!")
                else:
                    print("ℹ️  INFO: No optimization occurred (no summary available)")
            except json.JSONDecodeError:
                print("⚠️  WARNING: Could not parse result as JSON")
        else:
            print("❌ ERROR: No results returned from tool execution")

        print("\n" + "=" * 80)
        print("🔄 AUTOMATIC READ_FILE OPTIMIZATION TEST COMPLETE")
        print("=" * 80)

    async def test_summary_optimization(self, test_file_path: str = "config.py"):
        """
        Test the summary optimization functionality with a specific file
        测试特定文件的总结优化功能

        Args:
            test_file_path: File path to test (default: config.py which should be in summary)
        """
        if not self.mcp_agent:
            return False

        try:
            # Use MCP agent to call read_code_mem tool
            result = await self.mcp_agent.call_tool(
                "read_code_mem", {"file_paths": [test_file_path]}
            )

            # Parse the result to check if summary was found
            import json

            result_data = json.loads(result) if isinstance(result, str) else result

            return (
                result_data.get("status")
                in ["all_summaries_found", "partial_summaries_found"]
                and result_data.get("summaries_found", 0) > 0
            )
        except Exception as e:
            self.logger.warning(f"Failed to test read_code_mem optimization: {e}")
            return False

    async def test_read_tools_configuration(self):
        """
        Test the read tools configuration to verify enabling/disabling works correctly
        测试读取工具配置以验证启用/禁用是否正常工作
        """
        print("=" * 60)
        print("🧪 TESTING READ TOOLS CONFIGURATION")
        print("=" * 60)

        status = self.get_read_tools_status()
        print(f"Read tools enabled: {status['read_tools_enabled']}")
        print(f"Status: {status['status']}")
        print(f"Tools affected: {status['tools_affected']}")

        # Test with mock tool calls
        test_tools = [
            {
                "id": "test_read_file",
                "name": "read_file",
                "input": {"file_path": "test.py"},
            },
            {
                "id": "test_read_code_mem",
                "name": "read_code_mem",
                "input": {"file_path": "test.py"},
            },
            {
                "id": "test_write_file",
                "name": "write_file",
                "input": {"file_path": "test.py", "content": "# test"},
            },
        ]

        print(
            f"\n🔄 Testing tool execution with read_tools_enabled={self.enable_read_tools}"
        )

        for tool_call in test_tools:
            tool_name = tool_call["name"]
            if not self.enable_read_tools and tool_name in [
                "read_file",
                "read_code_mem",
            ]:
                print(f"🚫 {tool_name}: Would be SKIPPED (disabled)")
            else:
                print(f"✅ {tool_name}: Would be EXECUTED")

        print("=" * 60)
        print("🧪 READ TOOLS CONFIGURATION TEST COMPLETE")
        print("=" * 60)

        return status



================================================
FILE: workflows/agents/document_segmentation_agent.py
================================================
"""
Document Segmentation Agent

A lightweight agent that coordinates with the document segmentation MCP server
to analyze document structure and prepare segments for other agents.
"""

import os
import logging
from typing import Dict, Any, Optional

from mcp_agent.agents.agent import Agent
from utils.llm_utils import get_preferred_llm_class


class DocumentSegmentationAgent:
    """
    Intelligent document segmentation agent with semantic analysis capabilities.

    This enhanced agent provides:
    1. **Semantic Document Classification**: Content-based document type identification
    2. **Adaptive Segmentation Strategy**: Algorithm integrity and semantic coherence preservation
    3. **Planning Agent Optimization**: Segment preparation specifically optimized for downstream agents
    4. **Quality Intelligence Validation**: Advanced metrics for completeness and technical accuracy
    5. **Algorithm Completeness Protection**: Ensures critical algorithms and formulas remain intact

    Key improvements over traditional segmentation:
    - Semantic content analysis vs mechanical structure splitting
    - Dynamic character limits based on content complexity
    - Enhanced relevance scoring for planning agents
    - Algorithm and formula integrity preservation
    - Content type-aware segmentation strategies
    """

    def __init__(self, logger: Optional[logging.Logger] = None):
        self.logger = logger or self._create_default_logger()
        self.mcp_agent = None

    def _create_default_logger(self) -> logging.Logger:
        """Create default logger if none provided"""
        logger = logging.getLogger(f"{__name__}.DocumentSegmentationAgent")
        logger.setLevel(logging.INFO)
        return logger

    async def __aenter__(self):
        """Async context manager entry"""
        await self.initialize()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        await self.cleanup()

    async def initialize(self):
        """Initialize the MCP agent connection"""
        try:
            self.mcp_agent = Agent(
                name="DocumentSegmentationCoordinator",
                instruction="""You are an intelligent document segmentation coordinator that leverages advanced semantic analysis for optimal document processing.

Your enhanced capabilities include:
1. **Semantic Content Analysis**: Coordinate intelligent document type classification based on content semantics rather than structural patterns
2. **Algorithm Integrity Protection**: Ensure algorithm blocks, formulas, and related content maintain logical coherence
3. **Adaptive Segmentation Strategy**: Select optimal segmentation approaches (semantic_research_focused, algorithm_preserve_integrity, concept_implementation_hybrid, etc.)
4. **Quality Intelligence Validation**: Assess segmentation quality using enhanced metrics for completeness, relevance, and technical accuracy
5. **Planning Agent Optimization**: Ensure segments are specifically optimized for ConceptAnalysisAgent, AlgorithmAnalysisAgent, and CodePlannerAgent needs

**Key Principles**:
- Prioritize content semantics over mechanical structure
- Preserve algorithm and formula completeness
- Optimize for downstream agent token efficiency
- Ensure technical content integrity
- Provide actionable quality assessments

Use the enhanced document-segmentation tools to deliver superior segmentation results that significantly improve planning agent performance.""",
                server_names=["document-segmentation", "filesystem"],
            )

            # Initialize the agent context
            await self.mcp_agent.__aenter__()

            # Attach LLM
            self.llm = await self.mcp_agent.attach_llm(get_preferred_llm_class())

            self.logger.info("DocumentSegmentationAgent initialized successfully")

        except Exception as e:
            self.logger.error(f"Failed to initialize DocumentSegmentationAgent: {e}")
            raise

    async def cleanup(self):
        """Cleanup resources"""
        if self.mcp_agent:
            try:
                await self.mcp_agent.__aexit__(None, None, None)
            except Exception as e:
                self.logger.warning(f"Error during cleanup: {e}")

    async def analyze_and_prepare_document(
        self, paper_dir: str, force_refresh: bool = False
    ) -> Dict[str, Any]:
        """
        Perform intelligent semantic analysis and create optimized document segments.

        This method coordinates with the enhanced document segmentation server to:
        - Classify document type using semantic content analysis
        - Select optimal segmentation strategy (semantic_research_focused, algorithm_preserve_integrity, etc.)
        - Preserve algorithm and formula integrity
        - Optimize segments for downstream planning agents

        Args:
            paper_dir: Path to the paper directory
            force_refresh: Whether to force re-analysis with latest algorithms

        Returns:
            Dict containing enhanced analysis results and intelligent segment information
        """
        try:
            self.logger.info(f"Starting document analysis for: {paper_dir}")

            # Check if markdown file exists
            md_files = [f for f in os.listdir(paper_dir) if f.endswith(".md")]
            if not md_files:
                raise ValueError(f"No markdown file found in {paper_dir}")

            # Use the enhanced document segmentation tool
            message = f"""Please perform intelligent semantic analysis and segmentation for the document in directory: {paper_dir}

Use the analyze_and_segment_document tool with these parameters:
- paper_dir: {paper_dir}
- force_refresh: {force_refresh}

**Focus on these enhanced objectives**:
1. **Semantic Document Classification**: Identify document type using content semantics (research_paper, algorithm_focused, technical_doc, etc.)
2. **Intelligent Segmentation Strategy**: Select the optimal strategy based on content analysis:
   - `semantic_research_focused` for research papers with high algorithm density
   - `algorithm_preserve_integrity` for algorithm-heavy documents
   - `concept_implementation_hybrid` for mixed concept/implementation content
3. **Algorithm Completeness**: Ensure algorithm blocks, formulas, and related descriptions remain logically connected
4. **Planning Agent Optimization**: Create segments that maximize effectiveness for ConceptAnalysisAgent, AlgorithmAnalysisAgent, and CodePlannerAgent

After segmentation, get a document overview and provide:
- Quality assessment of semantic segmentation approach
- Algorithm/formula integrity verification
- Recommendations for planning agent optimization
- Technical content completeness evaluation"""

            result = await self.llm.generate_str(message=message)

            self.logger.info("Document analysis completed successfully")

            # Parse the result and return structured information
            return {
                "status": "success",
                "paper_dir": paper_dir,
                "analysis_result": result,
                "segments_available": True,
            }

        except Exception as e:
            self.logger.error(f"Error in document analysis: {e}")
            return {
                "status": "error",
                "paper_dir": paper_dir,
                "error_message": str(e),
                "segments_available": False,
            }

    async def get_document_overview(self, paper_dir: str) -> Dict[str, Any]:
        """
        Get overview of document structure and segments.

        Args:
            paper_dir: Path to the paper directory

        Returns:
            Dict containing document overview information
        """
        try:
            message = f"""Please provide an intelligent overview of the enhanced document segmentation for: {paper_dir}

Use the get_document_overview tool to retrieve:
- **Semantic Document Classification**: Document type and confidence score
- **Adaptive Segmentation Strategy**: Strategy used and reasoning
- **Segment Intelligence**: Total segments with enhanced metadata
- **Content Type Distribution**: Breakdown by algorithm, concept, formula, implementation content
- **Quality Intelligence Assessment**: Completeness, coherence, and planning agent optimization

Provide a comprehensive analysis focusing on:
1. Semantic vs structural segmentation quality
2. Algorithm and formula integrity preservation
3. Segment relevance for downstream planning agents
4. Technical content distribution and completeness"""

            result = await self.llm.generate_str(message=message)

            return {
                "status": "success",
                "paper_dir": paper_dir,
                "overview_result": result,
            }

        except Exception as e:
            self.logger.error(f"Error getting document overview: {e}")
            return {"status": "error", "paper_dir": paper_dir, "error_message": str(e)}

    async def validate_segmentation_quality(self, paper_dir: str) -> Dict[str, Any]:
        """
        Validate the quality of document segmentation.

        Args:
            paper_dir: Path to the paper directory

        Returns:
            Dict containing validation results
        """
        try:
            # Get overview first
            overview_result = await self.get_document_overview(paper_dir)

            if overview_result["status"] != "success":
                return overview_result

            # Analyze enhanced segmentation quality
            message = f"""Based on the intelligent document overview for {paper_dir}, please evaluate the enhanced segmentation quality using advanced criteria.

**Enhanced Quality Assessment Factors**:
1. **Semantic Coherence**: Do segments maintain logical content boundaries vs mechanical structural splits?
2. **Algorithm Integrity**: Are algorithm blocks, formulas, and related explanations kept together?
3. **Content Type Optimization**: Are different content types (algorithm, concept, formula, implementation) properly identified and scored?
4. **Planning Agent Effectiveness**: Will ConceptAnalysisAgent, AlgorithmAnalysisAgent, and CodePlannerAgent receive optimal information?
5. **Dynamic Sizing**: Are segments adaptively sized based on content complexity rather than fixed limits?
6. **Technical Completeness**: Are critical technical details preserved without fragmentation?

**Provide specific recommendations for**:
- Semantic segmentation improvements
- Algorithm/formula integrity enhancements
- Planning agent optimization opportunities
- Content distribution balance adjustments"""

            validation_result = await self.llm.generate_str(message=message)

            return {
                "status": "success",
                "paper_dir": paper_dir,
                "validation_result": validation_result,
                "overview_data": overview_result,
            }

        except Exception as e:
            self.logger.error(f"Error validating segmentation quality: {e}")
            return {"status": "error", "paper_dir": paper_dir, "error_message": str(e)}


async def run_document_segmentation_analysis(
    paper_dir: str, logger: Optional[logging.Logger] = None, force_refresh: bool = False
) -> Dict[str, Any]:
    """
    Convenience function to run document segmentation analysis.

    Args:
        paper_dir: Path to the paper directory
        logger: Optional logger instance
        force_refresh: Whether to force re-analysis

    Returns:
        Dict containing analysis results
    """
    async with DocumentSegmentationAgent(logger=logger) as agent:
        # Analyze and prepare document
        analysis_result = await agent.analyze_and_prepare_document(
            paper_dir, force_refresh=force_refresh
        )

        if analysis_result["status"] == "success":
            # Validate segmentation quality
            validation_result = await agent.validate_segmentation_quality(paper_dir)
            analysis_result["validation"] = validation_result

        return analysis_result


# Utility function for integration with existing workflow
async def prepare_document_segments(
    paper_dir: str, logger: Optional[logging.Logger] = None
) -> Dict[str, Any]:
    """
    Prepare intelligent document segments optimized for planning agents.

    This enhanced function leverages semantic analysis to create segments that:
    - Preserve algorithm and formula integrity
    - Optimize for ConceptAnalysisAgent, AlgorithmAnalysisAgent, and CodePlannerAgent
    - Use adaptive character limits based on content complexity
    - Maintain technical content completeness

    Called from the orchestration engine (Phase 3.5) to prepare documents
    before the planning phase with superior segmentation quality.

    Args:
        paper_dir: Path to the paper directory containing markdown file
        logger: Optional logger instance for tracking

    Returns:
        Dict containing enhanced preparation results and intelligent metadata
    """
    try:
        logger = logger or logging.getLogger(__name__)
        logger.info(f"Preparing document segments for: {paper_dir}")

        # Run analysis
        result = await run_document_segmentation_analysis(
            paper_dir=paper_dir,
            logger=logger,
            force_refresh=False,  # Use cached analysis if available
        )

        if result["status"] == "success":
            logger.info("Document segments prepared successfully")

            # Create metadata for downstream agents
            segments_dir = os.path.join(paper_dir, "document_segments")

            return {
                "status": "success",
                "paper_dir": paper_dir,
                "segments_dir": segments_dir,
                "segments_ready": True,
                "analysis_summary": result.get("analysis_result", ""),
                "validation_summary": result.get("validation", {}).get(
                    "validation_result", ""
                ),
            }
        else:
            logger.error(
                f"Document segmentation failed: {result.get('error_message', 'Unknown error')}"
            )
            return {
                "status": "error",
                "paper_dir": paper_dir,
                "segments_ready": False,
                "error_message": result.get(
                    "error_message", "Document segmentation failed"
                ),
            }

    except Exception as e:
        logger.error(f"Error preparing document segments: {e}")
        return {
            "status": "error",
            "paper_dir": paper_dir,
            "segments_ready": False,
            "error_message": str(e),
        }



================================================
FILE: .github/dependabot.yml
================================================
# To get started with Dependabot version updates, you'll need to specify which
# package ecosystems to update and where the package manifests are located.
# Please see the documentation for all configuration options:
# https://docs.github.com/code-security/dependabot/dependabot-version-updates/configuration-options-for-the-dependabot.yml-file

version: 2
updates:
  - package-ecosystem: "pip" # See documentation for possible values
    directory: "/" # Location of package manifests
    schedule:
      interval: "weekly"



================================================
FILE: .github/pull_request_template.md
================================================
<!--
Thanks for contributing to DeepCode!

Please ensure your pull request is ready for review before submitting.

About this template

This template helps contributors provide a clear and concise description of their changes. Feel free to adjust it as needed.
-->

## Description

[Briefly describe the changes made in this pull request.]

## Related Issues

[Reference any related issues or tasks addressed by this pull request.]

## Changes Made

[List the specific changes made in this pull request.]

## Checklist

- [ ] Changes tested locally
- [ ] Code reviewed
- [ ] Documentation updated (if necessary)
- [ ] Unit tests added (if applicable)

## Additional Notes

[Add any additional notes or context for the reviewer(s).]



================================================
FILE: .github/ISSUE_TEMPLATE/bug_report.yml
================================================
name: Bug Report
description: File a bug report
title: "[Bug]:"
labels: ["bug", "triage"]

body:
  - type: checkboxes
    id: existingcheck
    attributes:
      label: Do you need to file an issue?
      description: Please help us manage our time by avoiding duplicates and common bugs with the steps below.
      options:
        - label: I have searched the existing issues and this bug is not already filed.
        - label: I believe this is a legitimate bug, not just a question or feature request.
  - type: textarea
    id: description
    attributes:
      label: Describe the bug
      description: A clear and concise description of what the bug is.
      placeholder: What went wrong?
  - type: textarea
    id: reproduce
    attributes:
      label: Steps to reproduce
      description: Steps to reproduce the behavior.
      placeholder: How can we replicate the issue?
  - type: textarea
    id: expected_behavior
    attributes:
      label: Expected Behavior
      description: A clear and concise description of what you expected to happen.
      placeholder: What should have happened?
  - type: textarea
    id: configused
    attributes:
      label: DeepCode Config Used
      description: The DeepCode configuration used for the run.
      placeholder: The settings content or DeepCode configuration
      value: |
        # Paste your config here
  - type: textarea
    id: screenshotslogs
    attributes:
      label: Logs and screenshots
      description: If applicable, add screenshots and logs to help explain your problem.
      placeholder: Add logs and screenshots here
  - type: textarea
    id: additional_information
    attributes:
      label: Additional Information
      description: |
        - DeepCode Version: e.g., v0.1.1
        - Operating System: e.g., Windows 10, Ubuntu 20.04
        - Python Version: e.g., 3.8
        - Related Issues: e.g., #1
        - Any other relevant information.
      value: |
        - DeepCode Version:
        - Operating System:
        - Python Version:
        - Related Issues:



================================================
FILE: .github/ISSUE_TEMPLATE/config.yml
================================================
blank_issues_enabled: false



================================================
FILE: .github/ISSUE_TEMPLATE/feature_request.yml
================================================
name: Feature Request
description: File a feature request
labels: ["enhancement"]
title: "[Feature Request]:"

body:
  - type: checkboxes
    id: existingcheck
    attributes:
      label: Do you need to file a feature request?
      description: Please help us manage our time by avoiding duplicates and common feature request with the steps below.
      options:
        - label: I have searched the existing feature request and this feature request is not already filed.
        - label: I believe this is a legitimate feature request, not just a question or bug.
  - type: textarea
    id: feature_request_description
    attributes:
      label: Feature Request Description
      description: A clear and concise description of the feature request you would like.
      placeholder: What this feature request add more or improve?
  - type: textarea
    id: additional_context
    attributes:
      label: Additional Context
      description: Add any other context or screenshots about the feature request here.
      placeholder: Any additional information



================================================
FILE: .github/ISSUE_TEMPLATE/question.yml
================================================
name: Question
description: Ask a general question
labels: ["question"]
title: "[Question]:"

body:
  - type: checkboxes
    id: existingcheck
    attributes:
      label: Do you need to ask a question?
      description: Please help us manage our time by avoiding duplicates and common questions with the steps below.
      options:
        - label: I have searched the existing question and discussions and this question is not already answered.
        - label: I believe this is a legitimate question, not just a bug or feature request.
  - type: textarea
    id: question
    attributes:
      label: Your Question
      description: A clear and concise description of your question.
      placeholder: What is your question?
  - type: textarea
    id: context
    attributes:
      label: Additional Context
      description: Provide any additional context or details that might help us understand your question better.
      placeholder: Add any relevant information here



================================================
FILE: .github/workflows/linting.yaml
================================================
name: Linting and Formatting

on:
    push:
        branches:
            - main
    pull_request:
        branches:
            - main

jobs:
    lint-and-format:
        runs-on: ubuntu-latest

        steps:
            - name: Checkout code
              uses: actions/checkout@v2

            - name: Set up Python
              uses: actions/setup-python@v2
              with:
                python-version: '3.x'

            - name: Install dependencies
              run: |
                python -m pip install --upgrade pip
                pip install pre-commit

            - name: Run pre-commit
              run: pre-commit run --all-files --show-diff-on-failure



================================================
FILE: .github/workflows/pypi-publish.yml
================================================
name: Upload DeepCode Package

on:
  release:
    types: [published]

permissions:
  contents: read

jobs:
  release-build:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.x"

      - name: Build release distributions
        run: |
          python -m pip install build
          python -m build

      - name: Upload distributions
        uses: actions/upload-artifact@v4
        with:
          name: release-dists
          path: dist/

  pypi-publish:
    runs-on: ubuntu-latest
    needs:
      - release-build
    permissions:
      id-token: write

    environment:
      name: pypi

    steps:
      - name: Retrieve release distributions
        uses: actions/download-artifact@v4
        with:
          name: release-dists
          path: dist/

      - name: Publish release distributions to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          packages-dir: dist/


